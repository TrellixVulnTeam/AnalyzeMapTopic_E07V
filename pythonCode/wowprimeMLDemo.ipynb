{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試資料 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boston-側連續數值演算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "boston=datasets.load_boston()\n",
    "X=boston.data\n",
    "Y=boston.target\n",
    "#將資料分成訓練、測試集 其中測試集佔三成\n",
    "XX_train, XX_test, YY_train, YY_test =train_test_split(X,Y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris-測分類演算法用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "xx=iris.data\n",
    "typeY=iris.target\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#將資料分成訓練、測試集 其中測試集佔三成\n",
    "xx_train, xx_test, Y_train, Y_test =train_test_split(xx,typeY,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MRTinfo', 'Watsons', 'pxmart', 'busData', 'HRdata104', 'info3Store', 'wowprimediendata', 'info591', 'departmentStore', 'websites591', 'carrefour', 'taiwanInfo', 'smallStyleCount', 'addressCoordinate', 'Nhuman', 'taiwanInfoStoneTwo', 'ipeenWebsite', 'taiwanInfoHot7', 'bigStyleCount', 'ipeenInfo', 'conStore', 'CostPower', 'infoClinic']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "client=pymongo.MongoClient('192.168.1.113',27017,username=\"j122085\",password=\"850605\")\n",
    "db=client.rawData\n",
    "print(db.collection_names())\n",
    "collection=db.wowprimediendata\n",
    "# wowDatas=list(collection.find({\"ADGC_weekday\":{\"$gte\":0},\"avgDailyNet\":{\"$gte\":0},\"costPower_Analyze\":{\"$gte\":0}}))\n",
    "wowDatas=list(collection.find({\"ADGC_weekday\":{\"$gte\":0},\n",
    "                               \"avgDailyNet\":{\"$gte\":0},\n",
    "                               \"costPower_Analyze\":{\"$gte\":0},\n",
    "                               \"areaRadius_Analyze\":500}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "wowDatas=[i for i in wowDatas if i['NcostData_Analyze']>1 \n",
    "          and \"家樂\" not in i['StoreName'] \n",
    "          and \"大潤\" not in i['StoreName']\n",
    "          and \"巨城\" not in i['StoreName']\n",
    "          and \"大魯閣\" not in i['StoreName']\n",
    "          and \"新光\" not in i['StoreName']\n",
    "          and \"愛買\" not in i['StoreName']\n",
    "          and \"大買家\" not in i['StoreName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wowDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADGC_holiday': 334,\n",
       " 'ADGC_weekday': 189,\n",
       " 'Address': '高雄市楠梓區慶昌里後昌路622號',\n",
       " 'AreaManager': '陳紀妍',\n",
       " 'Called': '石二鍋',\n",
       " 'CareerName': '石二鍋事業處',\n",
       " 'CareerNo': '112',\n",
       " 'Chef': '陳鈺宸',\n",
       " 'CloseDate': 'None',\n",
       " 'CodeId': 'E',\n",
       " 'CorporationId': '53013861',\n",
       " 'Corporation_ch': '石二鍋高雄後昌分公司',\n",
       " 'CreateDate': '2018-05-02 16:06:18.610000',\n",
       " 'ItemName': '百貨點',\n",
       " 'Manager': '王益珊',\n",
       " 'NMRT_Analyze': 0,\n",
       " 'NbusStation_Analyze': 4,\n",
       " 'Ncarrefour_Analyze': 0,\n",
       " 'Nclinic_Analyze': 9,\n",
       " 'NconStore_Analyze': 2,\n",
       " 'NcostData_Analyze': 2,\n",
       " 'Ndabu_Analyze': 0,\n",
       " 'Ndien_Analyze': 21,\n",
       " 'Nhuman_Analyze': 6496,\n",
       " 'Njob_Analyze': 2,\n",
       " 'Nken_Analyze': 0,\n",
       " 'Nmc_Analyze': 0,\n",
       " 'Npxmart_Analyze': 0,\n",
       " 'NsimCostDien': 4,\n",
       " 'Nstar_Analyze': 0,\n",
       " 'NtStore_Analyze': 0,\n",
       " 'Nwa_Analyze': 0,\n",
       " 'Nwatson_Analyze': 1,\n",
       " 'Phone': '07-3625799',\n",
       " 'PlaceNo': '10',\n",
       " 'StoreName': '高雄後昌',\n",
       " 'StoreNo': '11211',\n",
       " '_id': '11211',\n",
       " 'areaRadius_Analyze': 500,\n",
       " 'avgCost_Analyze': 62.0,\n",
       " 'avgDailyCustomer': 234,\n",
       " 'avgDailyMeal': 212,\n",
       " 'avgDailyNet': 55292,\n",
       " 'avgSalary_Analyze': 25373.0,\n",
       " 'bigadd': '高雄市',\n",
       " 'costPower_Analyze': 60.0,\n",
       " 'lastYearRevenue': 20144562,\n",
       " 'lat': 22.70791585228124,\n",
       " 'lng': 120.29982956405114,\n",
       " 'mostStyle_Analyze': '中式料理',\n",
       " 'smalladd': '楠梓區',\n",
       " 'storeType': '街邊'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wowDatas[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(wowDatas)\n",
    "df.salary = df.avgDailyCustomer.astype(float)                   #traform into float type\n",
    "df.working = df.avgDailyNet.astype(float)                 #traform into float type\n",
    "X = df[['costPower_Analyze','Nhuman_Analyze',\"NsimCostDien\",\n",
    "        'NbusStation_Analyze','NconStore_Analyze','Nstar_Analyze',\n",
    "        'Nmc_Analyze', 'Nken_Analyze','Nwa_Analyze',\n",
    "        'Nwatson_Analyze','Npxmart_Analyze','Ncarrefour_Analyze','Nstar_Analyze','Nclinic_Analyze']].values                   #tranform DataFrame to ndarray Matrix  為了predict輸入的方式\n",
    "# xx=X\n",
    "#將每個欄位的數值都變成0-1(除以最大的數做正規化、並只留下該數值List) \n",
    "# x=[]\n",
    "# for i in range(len(X.T)):\n",
    "#     x.append(X.T[i]/max(X.T[i]))\n",
    "\n",
    "#用zscore正規化\n",
    "x=[]\n",
    "def zscore(x, axis = None):\n",
    "    x=np.array(x)\n",
    "    xmean = x.mean(axis=axis, keepdims=True)\n",
    "    xstd  = np.std(x, axis=axis, keepdims=True)\n",
    "    zscore = (x-xmean)/xstd\n",
    "    return zscore    \n",
    "\n",
    "for i in range(len(X.T)):\n",
    "    x.append(zscore(X.T[i]))\n",
    "\n",
    "x=np.array(x)\n",
    "xx=x.T\n",
    "\n",
    "Y=df['ADGC_weekday'].values\n",
    "\n",
    "\n",
    "#分類\n",
    "Calls=set(i[\"Called\"] for i in wowDatas)\n",
    "for Call in Calls:\n",
    "    mean=np.mean([i[\"ADGC_weekday\"] for i in wowDatas if i[\"Called\"]==Call and 'ADGC_weekday' in i])\n",
    "    if not np.isnan(mean):\n",
    "        for j in wowDatas:\n",
    "            if j[\"Called\"]==Call and 'ADGC_weekday' in j:\n",
    "                if j[\"ADGC_weekday\"]>mean*1.15:\n",
    "                    j['type']=0#\"good\"\n",
    "                elif j[\"ADGC_weekday\"]<mean*0.85:\n",
    "                    j['type']=2#\"bad\"\n",
    "                else:\n",
    "                    j['type']=1#\"normal\"\n",
    "            if 'avgDailyCustomer' in j:\n",
    "                aC=j[\"avgDailyNet\"]/j[\"avgDailyCustomer\"]\n",
    "                if aC<400:\n",
    "                    j['typeP']=\"低\"\n",
    "                elif aC<800:\n",
    "                    j['typeP']=\"中高\"\n",
    "                else:\n",
    "                    j['typeP']=\"高\"\n",
    "                    \n",
    "typeY= np.array([i['type'] for i in wowDatas if 'NbusStation_Analyze' in i and 'ADGC_weekday' in i and 'costPower_Analyze' in i\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xx為zscore正規化資料 \n",
    "# X為原始資料\n",
    "\n",
    "#### 都為262筆8維度\n",
    "\n",
    "# Y為平均來客數>>(訓練預測數值)\n",
    "# typeY為店家來客數表現>>(訓練預測類型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 14), (30, 14))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30,), array([162, 180, 146, 201, 164, 189, 225, 280, 269, 204, 232, 298, 159,\n",
       "        307, 253, 252, 246, 143, 212, 199, 289, 389, 238, 251, 219, 335,\n",
       "        385, 152, 112, 124], dtype=int64))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30,),\n",
       " array([2, 2, 2, 2, 2, 2, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 2, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeY.shape,typeY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 表現好的店家資料(GoodData)\n",
    "#### newDataXG為原始資料 newDataxxG為正規化後的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bestData=\"\"\"74\t247984\t9\t89\t8\t6\t1\t2\n",
    "# 52\t137707\t10\t78\t0\t2\t0\t0\n",
    "# 76\t355419\t10\t287\t31\t15\t5\t1\n",
    "# 54\t382333\t16\t203\t9\t4\t2\t3\n",
    "# 54\t286210\t14\t136\t0\t2\t2\t0\n",
    "# 77\t238522\t10\t96\t8\t7\t1\t2\n",
    "# 57\t229368\t28\t122\t2\t4\t4\t0\n",
    "# 52\t99568\t0\t45\t3\t5\t2\t0\n",
    "# 56\t365344\t8\t184\t9\t6\t3\t3\n",
    "# 63\t282141\t22\t187\t16\t15\t2\t1\"\"\"\n",
    "\n",
    "# newDataXG=np.array([[int(j)for j in i.split(\"\\t\")] for i in bestData.split('\\n')])\n",
    "\n",
    "# newDataxxG=[]\n",
    "# for i in range(len(newDataXG.T)):\n",
    "#     newDataxxG.append(zscore(newDataXG.T[i]))\n",
    "# newDataxxG=np.array(newDataxxG).T\n",
    "\n",
    "# YG=np.array([int(s) for s in \"\"\"389\n",
    "# 393\n",
    "# 394\n",
    "# 397\n",
    "# 399\n",
    "# 414\n",
    "# 424\n",
    "# 430\n",
    "# 460\n",
    "# 512\"\"\".split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 表現差的店家資料(BadData)\n",
    "#### newDataXB為原始資料 newDataxxB為正規化後的資料\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# badData=\"\"\"56\t178949\t12\t125\t5\t8\t5\t3\n",
    "# 70\t353804\t11\t252\t33\t14\t4\t2\n",
    "# 64\t233683\t13\t154\t11\t7\t3\t1\n",
    "# 67\t373797\t11\t229\t34\t16\t2\t3\n",
    "# 60\t185182\t42\t134\t4\t7\t4\t1\n",
    "# 52\t102316\t0\t49\t3\t5\t2\t0\n",
    "# 51\t106274\t7\t39\t1\t2\t2\t0\n",
    "# 65\t342102\t20\t171\t13\t12\t3\t2\n",
    "# 52\t292044\t8\t124\t8\t6\t1\t0\n",
    "# 58\t185640\t39\t132\t4\t6\t3\t1\"\"\"\n",
    "\n",
    "\n",
    "# newDataXB=np.array([[int(j)for j in i.split(\"\\t\")] for i in badData.split('\\n')])\n",
    "\n",
    "# newDataxxB=[]\n",
    "# for i in range(len(newDataXB.T)):\n",
    "#     newDataxxB.append(zscore(newDataXB.T[i]))\n",
    "# newDataxxB=np.array(newDataxxB).T\n",
    "# YB=np.array([int(s) for s in \"\"\"81\n",
    "# 87\n",
    "# 93\n",
    "# 95\n",
    "# 96\n",
    "# 97\n",
    "# 98\n",
    "# 98\n",
    "# 101\n",
    "# 104\"\"\".split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================預測數值===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "XX_train, XX_test, YY_train, YY_test =train_test_split(X,Y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#畫圖用\n",
    "import matplotlib.pyplot as plt\n",
    "def plotPaint(predict,Y,R=0,title=\"\"):\n",
    "    plt.scatter(predict,Y,s=2)\n",
    "    if R==1:\n",
    "        plt.plot(predict, predict, 'ro')\n",
    "#         plt.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Measured')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入sklearn模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試跑LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFvhJREFUeJzt3Xu0JWV95vHvMy0CDkTa0CoBpFGb\neFvaQAeZMWGMMmrIKDoxGXBU1M4iMWpgEo23zASTcaJxqaudeAkORjFewEuWjEOcSLwwJorpxpaL\nLdAqLa0ttAqIoETwN3/Ue2TT1Dlnd/fZl3PO97PWXqf2W7X3/p3q6vPseqvqrVQVkiTt6l9NugBJ\n0nQyICRJvQwISVIvA0KS1MuAkCT1MiAkSb0MCGkMkrwjyX+ddB3S7ojXQUjzS3It8NtVddGka5HG\nxT0IaS8ludeka5BGwYCQ5pHkvcCDgP+d5IdJ/ihJJVmf5JvAp9pyH0rynSQ3J7k4ySMH3uPdSf57\nm358ku1J/jDJDUl2JHn+RH45aQ4GhDSPqnoO8E3gqVV1AHB+m/XvgIcDT27P/w5YA9wfuBR43xxv\n+0DgvsChwHrgrUlWLnz10p4zIKQ9d1ZV3VpVPwKoqndV1S1VdTtwFvCYJPed5bU/Af60qn5SVRcC\nPwR+cSxVS0MyIKQ9d93MRJIVSV6X5GtJfgBc22YdPMtrv1dVdww8vw04YDRlSnvGgJCG03e632Db\ns4CTgRPpuo5Wt/aMtixpdAwIaTjXAw+eY/6BwO3A94D7AP9jHEVJo2RASMP5c+CPk9wEPLNn/rnA\nNuBbwFeAL4yxNmkkvFBOktTLPQhJUi8DQpLUy4CQJPUyICRJvRb1IGMHH3xwrV69etJlSNKismnT\npu9W1ar5llvUAbF69Wo2btw46TIkaVFJsm2Y5exikiT1MiAkSb0MCElSLwNCktTLgJAk9TIgJEm9\nDAhJUi8DQpKGsGnbjTz3nEvYtO3GSZcyNgaEJA1hw0VXc/E132XDRVdPupSxWdRXUkvSuJxx4lF3\n+7kcGBCSNIRjj1jJuesfO+kyxsouJklSLwNCktTLgJAk9TIgJEm9DAhJUi8DQpLUy4CQJPUyICRJ\nvQwISVIvA0KS1MuAkCT1MiAkSb0MCElSLwNCktTLgJAk9TIgJEm9DAhJUi8DQpLUy4CQJPUyICRJ\nvQwISVIvA0LSgtm07Uaee84lbNp246RL0QIwICQtmA0XXc3F13yXDRddPelStADuNekCJC0dZ5x4\n1N1+anEzICQtmGOPWMm56x876TK0QEbWxZRkvyRfTPLlJFcmeU1rPzLJJUmuSXJeknu39n3b861t\n/upR1SZJmt8oj0HcDjyhqh4DrAWekuR44PXAm6tqDXAjsL4tvx64saoeCry5LSdJmpCRBUR1ftie\n7tMeBTwB+HBrfw/w9DZ9cntOm//EJBlVfZKkuY30LKYkK5JsBm4APgl8Dbipqu5oi2wHDm3ThwLX\nAbT5NwM/3/OepyfZmGTjzp07R1m+JC1rIw2IqrqzqtYChwHHAQ/vW6z97NtbqHs0VJ1dVeuqat2q\nVasWrlhpEfK6A43SWK6DqKqbgM8AxwMHJZk5e+ow4NttejtwOECbf1/g++OoT1qsvO5AozTKs5hW\nJTmoTe8PnAhsAT4NPLMtdhrwsTZ9QXtOm/+pqrrHHoSku5xx4lGcsOZgrzvQSGRUf4OTPJruoPMK\nuiA6v6r+NMmDgQ8C9wO+BDy7qm5Psh/wXuBouj2HU6rq63N9xrp162rjxo0jqV+Slqokm6pq3XzL\njexCuaq6jO6P/a7tX6c7HrFr+4+B3xxVPZKk3eNYTJKkXgaEJKmXASFJ6mVASJJ6GRCSpF4GhCSp\nlwEhSeplQEiSehkQkqReBoQkqZcBIUnqZUBIU857PmhSDAhpynnPB03KyEZzlbQwZu714D0fNG4G\nhDTljj1iJeeuf+yky9AyZBeTJKmXASFJ6mVASJJ6GRCSpF4GhCSplwEhSeplQEjSIjOuq+sNCEla\nZMZ1db0XyknSIjOuq+sNCElaZMZ1db1dTJKkXgaEJKmXASFJ6mVASJJ6GRCSpF4GhCSplwEhSepl\nQEiSehkQkqReBoQkqdfIAiLJ4Uk+nWRLkiuTnNHaz0ryrSSb2+Okgde8MsnWJFclefKoapMkzW+U\nexB3AH9YVQ8HjgdelOQRbd6bq2pte1wI0OadAjwSeArwtiQrRlifNDXGNXyztDtGFhBVtaOqLm3T\ntwBbgEPneMnJwAer6vaq+gawFThuVPVJ02RcwzePi4G3NIzlGESS1cDRwCWt6cVJLkvyriQrW9uh\nwHUDL9vO3IEiLRlnnHgUJ6w5eOTDN4/LUgu85WrkAZHkAOAjwJlV9QPg7cBDgLXADuCNM4v2vLx6\n3u/0JBuTbNy5c+eIqpbGa2b45mOPWDn/wovAUgu85WrO+0EkOWau+TNdSHO8fh+6cHhfVX20veb6\ngfnvBD7enm4HDh94+WHAt3s+82zgbIB169bdI0AkTd647leg0ZrvhkEz3+73A9YBX6b7pv9ouu6i\nX57thUkCnANsqao3DbQfUlU72tNnAFe06QuA9yd5E/ALwBrgi7v120i7YdO2G9lw0dWcceJRS+ab\nu7SQ5gyIqvpVgCQfBE6vqsvb80cBL53nvR8HPAe4PMnm1vYq4NQka+m6j64Ffqd91pVJzge+QncG\n1Iuq6s49+aWkYcz0kwN+25V6DHvL0YfNhANAVV3R/sjPqqo+R/9xhQvneM1rgdcOWZO0V8Z1X19p\nsRo2ILYk+V/A39B983823Wmr0qJlP7k0t2ED4vnAC4Ez2vOL6c5GkiQtUUMFRFX9OMk7gAur6qoR\n1yRJmgJDXQeR5GnAZuAT7fnaJBeMsjBJ0mQNe6Hcn9ANe3ETQFVtBlaPqCZp0XOoCS0FwwbEHVV1\n80grkZYQh5rQUjDsQeorkjwLWJFkDfD7wD+NrixpcfMUWi0Fw+5BvIRuGO7bgfcDNwNnjqooabFb\namMraXmadw+i3ZPhNVX1MuDVoy9JkjQN5t2DaMNdHDuGWiRJU2TYYxBfaqe1fgi4daZxZoRWSdLS\nM2xA3A/4HvCEgbYCDAhJWqKGvZL6+aMuRJI0XYYKiCR/Tc/d3arqBQtekSRpKgzbxfTxgen96G70\nc4+7vUmSlo5hu5g+Mvg8yQeAi0ZSkSRpKgx7odyu1gAPWshCpKXEsZi0FAx7DOIW7n4M4jvAy0dS\nkbQEeDtTLQXDdjEdOOpCpKXEsZi0FAy7B/E4YHNV3Zrk2cAxwIaq2jbS6qRFytuZaikY9hjE24Hb\nkjwG+CNgG3DuyKqSJE3c7twPooCT6fYcNgB2O0nSEjbsdRC3JHkl8GzghDbC6z6jK0uSNGnD7kH8\nJ7p7Qayvqu8AhwJvGFlVkqSJG/Yspu8Abxp4/k08BiFJS9pQexBJjk/yz0l+mORfktyZxHtUS9IS\nNmwX018CpwLXAPsDvw28dVRFSZImb+ihNqpqK7Ciqu6sqr8GHj+yqiQtCw5JMt2GPYvptiT3BjYn\n+QtgB/CvR1eWpOXAIUmm27AB8Ry6vY0XA/8FOBz4jVEVJWl5cEiS6Zbu+rchFkz2Bx5UVVeNtqTh\nrVu3rjZu3DjpMiRpUUmyqarWzbfcsGcxPRXYDHyiPV+b5IK9K1GSNM2GPUh9FnAccBNAVW0GVo+m\nJEnSNNidsZi87kGSlpFhA+KKJM8CViRZk+R/Av80wrokTRlPSV1+hg2IlwCPpBuP6QPAD4AzR1WU\npOkzc0rqhouunnQpGpNhx2K6DXh1ewwlyeF04zU9EPgpcHZVbUhyP+A8umMY1wK/VVU3JgmwATgJ\nuA14XlVdOvyvImmUPCV1+ZnzNNf5zlSqqqfN8dpDgEOq6tIkBwKbgKcDzwO+X1WvS/IKYGVVvTzJ\nSXR7KicBj6W778ScV854mqsk7b5hT3Odbw/i3wDX0XUrXQJk2AKqagfdFddU1S1JttANE34ydw3T\n8R7gM8DLW/u57cZEX0hyUJJD2vtIksZsvmMQDwReBTyKrvvn3wPfrarPVtVnh/2QJKuBo+lC5gEz\nf/Tbz/u3xQ6lC6MZ21vbru91epKNSTbu3Llz2BIkSbtpzoBoA/N9oqpOA44HtgKfSfKSYT8gyQHA\nR4Azq+oHcy3aV0JPTWdX1bqqWrdq1aphy5Ak7aZ5D1In2Rf4dbrhvlcDbwE+OsybJ9mHLhzeV1Uz\nr7l+puuoHae4obVvpxvjacZhwLeH+RxJ0sKbcw8iyXvornc4BnhNVf1SVf1ZVX1rvjduZyWdA2yp\nqjcNzLoAOK1NnwZ8bKD9uekcD9zs8QdJmpz59iCeA9wKHAX8fvc3H+i6g6qqfm6O1z6uvf7yJJtb\n26uA1wHnJ1kPfBP4zTbvQrozmLbSneb6/N37VSRJC2nOgKiqoW8o1PPazzH7WU9P7Fm+gBft6edJ\nkhbWHgeAJGlpMyAkSb0MCElSLwNCe8URPqWly4DQXnGET2npGmo0V2k2jvApLV0GhPbKsUes5Nz1\ncw66K2mRsotJktTLgJAk9TIgJEm9DAhJUi8DQiPldRLS4mVAaKS8TkJavDzNVSPldRLS4mVAaKS8\nTkJavOxikiT1MiAkSb0MCElSLwNCktTLgJAk9TIgJEm9DAhJUi8DQpLUy4CQJPUyICRJvQwITS1H\ngpUmy4DQ1HIkWGmyDAiN3bB7BmeceBQnrDnYkWClCXE0V43dzJ4BMOdIr44EK02WexAaO/cMppPH\nfLQr9yA0du4ZTKdh9+y0fBgQkgDv/qd7MiAkAe7Z6Z48BiFJ6mVASJJ6jSwgkrwryQ1JrhhoOyvJ\nt5Jsbo+TBua9MsnWJFclefKo6ppGnj0iaRqNcg/i3cBTetrfXFVr2+NCgCSPAE4BHtle87YkK0ZY\n21TximFJ02hkB6mr6uIkq4dc/GTgg1V1O/CNJFuB44DPj6i8qeLZI5Km0SSOQbw4yWWtC2plazsU\nuG5gme2t7R6SnJ5kY5KNO3fuHHWtYzFz9sixR6ycf2FJGpNxB8TbgYcAa4EdwBtbe3qWrb43qKqz\nq2pdVa1btWrVaKqUJI03IKrq+qq6s6p+CryTrhsJuj2GwwcWPQz49jhrkyTd3VgDIskhA0+fAcyc\n4XQBcEqSfZMcCawBvjjO2jQcz7iSlo+RHaRO8gHg8cDBSbYDfwI8Pslauu6ja4HfAaiqK5OcD3wF\nuAN4UVXdOaratOccr0daPkZ5FtOpPc3nzLH8a4HXjqoeLQzPuJKWD8di0m5xvB5p+XCoDUlSLwNC\nktTLgJAk9TIgJEm9DAhJUi8DQpLUy4CQJPUyICRJvQwISVIvA0KS1MuAkCT1MiAkSb0MCElSLwNC\nktTLgJAk9TIgljBvDyppbxgQS9jM7UE3XHT1pEuRtAh5R7klzNuDStobBsQS5u1BJe0Nu5gkSb0M\nCElSLwNCktTLgJAk9TIgJEm9DAhJUi8DQpLUa1kGhENQSNL8lmVAOASFJM1vWV5J7RAUkjS/ZRkQ\nDkEhSfNbll1MkqT5GRCSpF4GhCSplwEhSeplQEiSehkQkqReBoQkqVeqatI17LEkO4FtC/y2BwPf\nXeD3XEjWt3esb+9Y396ZlvqOqKpV8y20qANiFJJsrKp1k65jNta3d6xv71jf3pn2+nZlF5MkqZcB\nIUnqZUDc09mTLmAe1rd3rG/vWN/emfb67sZjEJKkXu5BSJJ6GRCSpF7LLiCSvCvJDUmuGGh7Q5Kv\nJrksyd8mOai1r07yoySb2+MdE6rvrCTfGqjjpIF5r0yyNclVSZ48ofrOG6jt2iSbW/sk1t/hST6d\nZEuSK5Oc0drvl+STSa5pP1e29iR5S1uHlyU5ZgK1TdP2N1uNU7ENzlHfVGyDSfZL8sUkX271vaa1\nH5nkkrb9nZfk3q193/Z8a5u/epT17baqWlYP4ATgGOCKgbYnAfdq068HXt+mVw8uN8H6zgJe2rPs\nI4AvA/sCRwJfA1aMu75d5r8R+G8TXH+HAMe06QOBq9t6+gvgFa39FQP/xicBfwcEOB64ZAK1TdP2\nN1uNU7ENzlbftGyDbTs6oE3vA1zStqvzgVNa+zuAF7bp3wPe0aZPAc4b57/3fI9ltwdRVRcD39+l\n7e+r6o729AvAYWMv7K5a7lHfHE4GPlhVt1fVN4CtwHEjK46560sS4LeAD4yyhrlU1Y6qurRN3wJs\nAQ6lW1fvaYu9B3h6mz4ZOLc6XwAOSnLIOGubsu1vtvU3m7Fug/PVN+ltsG1HP2xP92mPAp4AfLi1\n77r9zWyXHwae2H6HqbDsAmIIL6D7RjnjyCRfSvLZJL8yqaKAF7cuiHfNdI/Q/ce4bmCZ7cz9n3nU\nfgW4vqquGWib2Ppru+tH032Le0BV7YDujwxw/7bYRNbhLrUNmprtr6fGqdoGZ1mHE98Gk6xoXVw3\nAJ+k26u6aeBLwOA6+tn6a/NvBn5+1DUOy4AYkOTVwB3A+1rTDuBBVXU08AfA+5P83ARKezvwEGBt\nq+mNrb3vm8Ykz1s+lbt/c5vY+ktyAPAR4Myq+sFci/a0jXQdzlbbNG1/PTVO1TY4x7/vxLfBqrqz\nqtbS7QkeBzy8b7H2c9r+D9+NAdEkOQ34D8B/rtYh2Habv9emN9F9Ezhq3LVV1fVto/sp8E7u2oXf\nDhw+sOhhwLfHXR9AknsB/xE4b6ZtUusvyT50fzzeV1Ufbc3Xz3QdtZ83tPaxrsNZapuq7a+vxmna\nBudYh1OzDbbPuwn4DN0xiINafXD3dfSz9dfm35fhu5hHzoAAkjwFeDnwtKq6baB9VZIVbfrBwBrg\n6xOob7BP/BnAzBlEFwCntDMhjmz1fXHc9TUnAl+tqu0zDZNYf63/9hxgS1W9aWDWBcBpbfo04GMD\n7c9N53jg5pmuqHHVNk3b3xw1TsU2OMe/L0zBNtg+b+YstP1bTVuATwPPbIvtuv3NbJfPBD418wVh\nKkz6KPm4H3S7nzuAn9Cl93q6A2vXAZvbY+asgt8ArqQ7S+NS4KkTqu+9wOXAZXQb1CEDy7+a7lvR\nVcCvTaK+1v5u4Hd3WXYS6++X6XbRLxv49zyJrl/3H4Br2s/7teUDvLWtw8uBdROobZq2v9lqnIpt\ncLb6pmUbBB4NfKnVdwV3nU31YLrg3Ap8CNi3te/Xnm9t8x886n/j3Xk41IYkqZddTJKkXgaEJKmX\nASFJ6mVASJJ6GRCSpF4GhJa1JHe2UT6vSPKhJPfZi/d6fJKPt+mnJXnFHMselOT39uAzzkry0j2t\nUdodBoSWux9V1dqqehTwL8DvDs5sF9Dt9v+Tqrqgql43xyIH0Y3kKU0tA0K6y/8DHpruHgJbkryN\n7uKqw5M8Kcnnk1za9jQOgO4q6HT3cvgc3TAPtPbnJfnLNv2AdPd5+HJ7/FvgdcBD2t7LG9pyL0vy\nz21AvNcMvNer091r4SLgF8e2NrTsGRASPxsH59forhaG7g/xudUN8nYr8MfAiVV1DLAR+IMk+9GN\nS/RUulFEHzjL278F+GxVPYbuXhpX0t2T4mtt7+VlSZ5ENwzEcXQD4h2b5IQkx9LdJ+BougD6pQX+\n1aVZ3Wv+RaQlbf82NDN0exDnAL8AbKvu/hDQDbb2COAfu6GAuDfweeBhwDeqDS2d5G+A03s+4wnA\nc6Eb6RO4eWC47BlPao8vtecH0AXGgcDfVhujKckFe/XbSrvBgNBy96Pqhmb+mRYCtw42AZ+sqlN3\nWW4tCzc0c4A/r6q/2uUzzlzAz5B2i11M0vy+ADwuyUMBktwnyVHAV+luRvOQttyps7z+H4AXtteu\naPcjuIVu72DG/wVeMHBs49Ak9wcuBp6RZP8kB9J1Z0ljYUBI86iqncDzgA8kuYwuMB5WVT+m61L6\nP+0g9bZZ3uIM4FeTXA5sAh5Z3T0K/rGdXvuGqvp74P3A59tyHwYOrO72mufRjVr6EbpuMGksHM1V\nktTLPQhJUi8DQpLUy4CQJPUyICRJvQwISVIvA0KS1MuAkCT1+v+NIdktb8D+eAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32c65f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqZcBIUnqZUBIknoZEJKkXgaEJKnX/wdsBRasC1SoVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32c48320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(XX_train, YY_train)\n",
    "# features=\"消費力\t人口數\t公車站數\t四大超商數\t星巴克數\t麥當勞數\t肯德基數\t瓦城數\".split(\"\\t\")\n",
    "\n",
    "# print(\"參數\")\n",
    "# for i,j in zip(features,lm.coef_):\n",
    "#     print(i,j)\n",
    "\n",
    "    \n",
    "    \n",
    "predict=lm.predict(XX_train)\n",
    "plotPaint(predict,YY_train,title=\"train\")\n",
    "predict=lm.predict(XX_test)\n",
    "plotPaint(predict,YY_test,title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.85057717390966"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=lm.predict(XX_train).reshape([len(XX_train)])-np.array(YY_train)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.09475969879868"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=lm.predict(XX_test).reshape([len(XX_test)])-np.array(YY_test)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted_sales = lm.predict(newDataXG)\n",
    "# print(\"好店家預測\")\n",
    "# print(predicted_sales)\n",
    "\n",
    "# predicted_sales = lm.predict(newDataXB)\n",
    "# print(\"差店家預測\")\n",
    "# print(predicted_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試跑SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32a21668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndN8+HPflAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32b07e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "clf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "clf.fit(XX_train, YY_train)\n",
    "# predicted_sales = clf.predict(newDataXG)\n",
    "# print(\"好店家預測\")\n",
    "# print(predicted_sales)\n",
    "\n",
    "# predicted_sales = clf.predict(newDataXB)\n",
    "# print(\"差店家預測\")\n",
    "# print(predicted_sales)\n",
    "\n",
    "predict=clf.predict(XX_train)\n",
    "plotPaint(predict,YY_train,title=\"train\")\n",
    "predict=clf.predict(XX_test)\n",
    "plotPaint(predict,YY_test,title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10002200345189746"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=clf.predict(XX_train).reshape([len(XX_train)])-np.array(YY_train)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.68622564415779"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=clf.predict(XX_test).reshape([len(XX_test)])-np.array(YY_test)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict=clf.predict(newDataXB)\n",
    "# plotPaint(predict,YB,R=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict=clf.predict(newDataXG)\n",
    "# plotPaint(predict,YG,R=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TryData=\"\"\"63\t333451\t13\t148\t10\t8\t2\t2\n",
    "# 62\t205551\t12\t127\t2\t5\t3\t1\n",
    "# 58\t174562\t26\t128\t4\t6\t3\t1\n",
    "# 72\t137555\t12\t100\t4\t9\t1\t1\n",
    "# 79\t223146\t12\t128\t11\t12\t2\t2\n",
    "# 63\t282141\t22\t187\t16\t15\t2\t1\n",
    "# 52\t157180\t4\t83\t5\t4\t2\t1\n",
    "# 71\t128373\t8\t52\t1\t3\t3\t0\"\"\"\n",
    "\n",
    "# #\"消費力\t人口數\t公車站數\t四大超商數\t星巴克數\t麥當勞數\t肯德基數\t瓦城數\"\n",
    "# newDataXTry=np.array([[int(j) for j in i.split(\"\\t\")] for i in TryData.split('\\n')])\n",
    "\n",
    "\n",
    "# newDataxxTry=[]\n",
    "# for i in range(len(newDataXTry.T)):\n",
    "#     newDataxxTry.append(zscore(newDataXTry.T[i]))\n",
    "# newDataxxTry=np.array(newDataxxTry).T\n",
    "\n",
    "\n",
    "# clf.predict(newDataXTry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引入keras模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "from keras.models import Sequential # 序慣模型(可一層一層加入)\n",
    "from keras.layers.core import Dense,Activation # 緊密層、啟動函數\n",
    "from keras.layers import Dropout #減少overfitting的方法\n",
    "from keras.utils import np_utils #one-hot 僅分類時使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 淺層神經網路(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 5 samples\n",
      "Epoch 1/500\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 33079808.0000 - mean_absolute_error: 5078.9404 - val_loss: 24222096.0000 - val_mean_absolute_error: 4554.0288\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 31189296.0000 - mean_absolute_error: 4932.2285 - val_loss: 22808684.0000 - val_mean_absolute_error: 4419.8169\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 29356584.0000 - mean_absolute_error: 4785.6929 - val_loss: 21440224.0000 - val_mean_absolute_error: 4285.8481\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 27582550.0000 - mean_absolute_error: 4639.4233 - val_loss: 20117344.0000 - val_mean_absolute_error: 4152.2119\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 25868014.0000 - mean_absolute_error: 4493.5161 - val_loss: 18840554.0000 - val_mean_absolute_error: 4018.9954\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 24213632.0000 - mean_absolute_error: 4348.0684 - val_loss: 17610282.0000 - val_mean_absolute_error: 3886.2898\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 22619952.0000 - mean_absolute_error: 4203.1768 - val_loss: 16426869.0000 - val_mean_absolute_error: 3754.1870\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 21087416.0000 - mean_absolute_error: 4058.9446 - val_loss: 15290560.0000 - val_mean_absolute_error: 3622.7839\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 19616332.0000 - mean_absolute_error: 3915.4753 - val_loss: 14201506.0000 - val_mean_absolute_error: 3492.1763\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 18206884.0000 - mean_absolute_error: 3772.8760 - val_loss: 13159750.0000 - val_mean_absolute_error: 3362.4648\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 16859124.0000 - mean_absolute_error: 3631.2539 - val_loss: 12165232.0000 - val_mean_absolute_error: 3233.7490\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 15572956.0000 - mean_absolute_error: 3490.7190 - val_loss: 11217787.0000 - val_mean_absolute_error: 3106.1316\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 14348163.0000 - mean_absolute_error: 3351.3838 - val_loss: 10317151.0000 - val_mean_absolute_error: 2979.7168\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 13184381.0000 - mean_absolute_error: 3213.3608 - val_loss: 9462947.0000 - val_mean_absolute_error: 2854.6089\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 12081111.0000 - mean_absolute_error: 3076.7651 - val_loss: 8654686.0000 - val_mean_absolute_error: 2730.9131\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 11037710.0000 - mean_absolute_error: 2941.7109 - val_loss: 7891781.5000 - val_mean_absolute_error: 2608.7358\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 10053397.0000 - mean_absolute_error: 2808.3149 - val_loss: 7173545.5000 - val_mean_absolute_error: 2488.1841\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 9127261.0000 - mean_absolute_error: 2676.6934 - val_loss: 6499177.0000 - val_mean_absolute_error: 2369.3638\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 8258249.5000 - mean_absolute_error: 2546.9631 - val_loss: 5867787.5000 - val_mean_absolute_error: 2252.3833\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7445182.5000 - mean_absolute_error: 2419.2405 - val_loss: 5278374.0000 - val_mean_absolute_error: 2137.3472\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6686750.0000 - mean_absolute_error: 2293.6406 - val_loss: 4729856.0000 - val_mean_absolute_error: 2024.3611\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 5981524.0000 - mean_absolute_error: 2170.2793 - val_loss: 4221057.0000 - val_mean_absolute_error: 1913.5293\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 5327959.0000 - mean_absolute_error: 2049.2698 - val_loss: 3750716.5000 - val_mean_absolute_error: 1804.9539\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 4724400.0000 - mean_absolute_error: 1930.7239 - val_loss: 3317504.2500 - val_mean_absolute_error: 1698.7369\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 4169097.5000 - mean_absolute_error: 1814.7526 - val_loss: 2920006.5000 - val_mean_absolute_error: 1594.9753\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 3660198.5000 - mean_absolute_error: 1701.4624 - val_loss: 2556755.5000 - val_mean_absolute_error: 1493.7659\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 3195771.0000 - mean_absolute_error: 1590.9580 - val_loss: 2226221.2500 - val_mean_absolute_error: 1395.2006\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2773816.0000 - mean_absolute_error: 1483.3416 - val_loss: 1926830.3750 - val_mean_absolute_error: 1299.3696\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2392260.5000 - mean_absolute_error: 1378.7090 - val_loss: 1656962.1250 - val_mean_absolute_error: 1206.3568\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2048987.0000 - mean_absolute_error: 1277.1541 - val_loss: 1414971.3750 - val_mean_absolute_error: 1116.2434\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1741834.3750 - mean_absolute_error: 1178.7646 - val_loss: 1199187.2500 - val_mean_absolute_error: 1029.1045\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1468612.5000 - mean_absolute_error: 1083.6222 - val_loss: 1007926.6250 - val_mean_absolute_error: 945.0099\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1227120.6250 - mean_absolute_error: 991.8043 - val_loss: 839504.1250 - val_mean_absolute_error: 864.0233\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1015148.1875 - mean_absolute_error: 903.3795 - val_loss: 692241.4375 - val_mean_absolute_error: 786.2018\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 830497.0000 - mean_absolute_error: 818.4103 - val_loss: 564476.8125 - val_mean_absolute_error: 711.5961\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 670994.2500 - mean_absolute_error: 736.9524 - val_loss: 454575.1562 - val_mean_absolute_error: 640.2487\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 534497.3750 - mean_absolute_error: 659.0514 - val_loss: 360935.0000 - val_mean_absolute_error: 572.1938\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 418913.2500 - mean_absolute_error: 584.7451 - val_loss: 282000.3125 - val_mean_absolute_error: 507.4576\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 322208.5625 - mean_absolute_error: 514.0630 - val_loss: 216268.9688 - val_mean_absolute_error: 446.0580\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 242416.2031 - mean_absolute_error: 447.0230 - val_loss: 162297.8438 - val_mean_absolute_error: 388.0031\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 177653.6875 - mean_absolute_error: 383.6355 - val_loss: 118711.4141 - val_mean_absolute_error: 333.2912\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 126123.5000 - mean_absolute_error: 323.8978 - val_loss: 84210.0469 - val_mean_absolute_error: 281.9135\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 86129.4375 - mean_absolute_error: 267.8004 - val_loss: 57570.6562 - val_mean_absolute_error: 233.8483\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 56076.2227 - mean_absolute_error: 215.3199 - val_loss: 37654.9062 - val_mean_absolute_error: 189.0664\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 34479.9453 - mean_absolute_error: 166.4238 - val_loss: 23410.4883 - val_mean_absolute_error: 147.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 19970.9785 - mean_absolute_error: 124.2984 - val_loss: 13874.3467 - val_mean_absolute_error: 109.1834\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 11295.6143 - mean_absolute_error: 89.9429 - val_loss: 8172.2671 - val_mean_absolute_error: 80.2653\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7318.3076 - mean_absolute_error: 63.3794 - val_loss: 5520.1533 - val_mean_absolute_error: 65.8616\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7021.7461 - mean_absolute_error: 66.7078 - val_loss: 5221.8228 - val_mean_absolute_error: 58.3169\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 9504.2080 - mean_absolute_error: 87.1340 - val_loss: 6667.5063 - val_mean_absolute_error: 72.9839\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 13977.6680 - mean_absolute_error: 108.0173 - val_loss: 9330.1074 - val_mean_absolute_error: 86.0763\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 19762.9336 - mean_absolute_error: 127.7074 - val_loss: 12761.7568 - val_mean_absolute_error: 97.6506\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 26285.0156 - mean_absolute_error: 145.1142 - val_loss: 16588.9570 - val_mean_absolute_error: 107.7652\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 33066.0352 - mean_absolute_error: 160.3255 - val_loss: 20507.2852 - val_mean_absolute_error: 116.6822\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 39719.9141 - mean_absolute_error: 173.4359 - val_loss: 24275.6270 - val_mean_absolute_error: 128.1142\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 45942.7891 - mean_absolute_error: 184.5418 - val_loss: 27710.3008 - val_mean_absolute_error: 137.5875\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 51506.9375 - mean_absolute_error: 193.7449 - val_loss: 30678.9648 - val_mean_absolute_error: 145.2091\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 56251.7500 - mean_absolute_error: 201.1494 - val_loss: 33094.2891 - val_mean_absolute_error: 151.0892\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 60075.3633 - mean_absolute_error: 206.8614 - val_loss: 34907.1641 - val_mean_absolute_error: 155.3377\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 62926.4844 - mean_absolute_error: 210.9883 - val_loss: 36101.6953 - val_mean_absolute_error: 158.0668\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 64797.1836 - mean_absolute_error: 213.6393 - val_loss: 36688.3633 - val_mean_absolute_error: 159.3880\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 65713.8750 - mean_absolute_error: 214.9226 - val_loss: 36699.7891 - val_mean_absolute_error: 159.4137\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 65731.5859 - mean_absolute_error: 214.9472 - val_loss: 36184.3906 - val_mean_absolute_error: 158.2542\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 64926.2422 - mean_absolute_error: 213.8201 - val_loss: 35202.6953 - val_mean_absolute_error: 156.0188\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 63389.8086 - mean_absolute_error: 211.6480 - val_loss: 33823.4023 - val_mean_absolute_error: 152.8153\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 61223.5742 - mean_absolute_error: 208.5351 - val_loss: 32106.8379 - val_mean_absolute_error: 148.7189\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 58515.2266 - mean_absolute_error: 204.5549 - val_loss: 30151.2500 - val_mean_absolute_error: 143.8901\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 55411.6289 - mean_absolute_error: 199.8633 - val_loss: 28016.4727 - val_mean_absolute_error: 138.3982\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 51998.2891 - mean_absolute_error: 194.5273 - val_loss: 25770.9961 - val_mean_absolute_error: 132.3378\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 48375.8359 - mean_absolute_error: 188.6394 - val_loss: 23463.4629 - val_mean_absolute_error: 125.7562\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 44612.6133 - mean_absolute_error: 182.2442 - val_loss: 21182.0898 - val_mean_absolute_error: 118.8285\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 40844.4375 - mean_absolute_error: 175.5135 - val_loss: 18960.7812 - val_mean_absolute_error: 113.1865\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 37119.3203 - mean_absolute_error: 168.4830 - val_loss: 16841.3223 - val_mean_absolute_error: 108.3619\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 33501.1875 - mean_absolute_error: 161.2281 - val_loss: 14846.2559 - val_mean_absolute_error: 103.4038\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 30021.8281 - mean_absolute_error: 153.7721 - val_loss: 13027.7012 - val_mean_absolute_error: 98.4197\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 26768.3691 - mean_absolute_error: 146.2768 - val_loss: 11391.2559 - val_mean_absolute_error: 93.4180\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 23749.5703 - mean_absolute_error: 138.7549 - val_loss: 9948.8896 - val_mean_absolute_error: 88.4356\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 20987.9570 - mean_absolute_error: 131.2628 - val_loss: 8706.6719 - val_mean_absolute_error: 83.5079\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 18497.2227 - mean_absolute_error: 123.8527 - val_loss: 7664.3975 - val_mean_absolute_error: 78.6649\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 16282.5723 - mean_absolute_error: 116.5702 - val_loss: 6817.4482 - val_mean_absolute_error: 73.9359\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 14342.8525 - mean_absolute_error: 109.4580 - val_loss: 6156.5771 - val_mean_absolute_error: 69.3438\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 12670.6113 - mean_absolute_error: 102.5530 - val_loss: 5669.6577 - val_mean_absolute_error: 64.9115\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 11253.6338 - mean_absolute_error: 96.1155 - val_loss: 5341.7319 - val_mean_absolute_error: 60.6574\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 10075.7266 - mean_absolute_error: 90.3262 - val_loss: 5156.1538 - val_mean_absolute_error: 56.9590\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 9118.0156 - mean_absolute_error: 84.8017 - val_loss: 5095.0352 - val_mean_absolute_error: 57.2647\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 8359.4834 - mean_absolute_error: 79.5600 - val_loss: 5140.5107 - val_mean_absolute_error: 60.1582\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7775.1602 - mean_absolute_error: 74.5886 - val_loss: 5273.5498 - val_mean_absolute_error: 62.8871\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7348.7725 - mean_absolute_error: 70.1164 - val_loss: 5477.6050 - val_mean_absolute_error: 65.4436\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7053.0918 - mean_absolute_error: 67.1509 - val_loss: 5733.3047 - val_mean_absolute_error: 67.7965\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6869.5957 - mean_absolute_error: 64.6941 - val_loss: 6025.5444 - val_mean_absolute_error: 69.9579\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6775.8779 - mean_absolute_error: 62.7481 - val_loss: 6341.6133 - val_mean_absolute_error: 71.9388\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6752.9883 - mean_absolute_error: 60.9647 - val_loss: 6665.2861 - val_mean_absolute_error: 73.7204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6783.8325 - mean_absolute_error: 60.2633 - val_loss: 6986.6709 - val_mean_absolute_error: 75.3152\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6852.8076 - mean_absolute_error: 60.5323 - val_loss: 7296.2783 - val_mean_absolute_error: 76.7273\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6946.5225 - mean_absolute_error: 61.0371 - val_loss: 7587.8232 - val_mean_absolute_error: 77.9689\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7054.1387 - mean_absolute_error: 61.5803 - val_loss: 7852.0342 - val_mean_absolute_error: 79.0324\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7165.1860 - mean_absolute_error: 62.0459 - val_loss: 8085.7891 - val_mean_absolute_error: 79.9315\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7272.6240 - mean_absolute_error: 63.0314 - val_loss: 8285.8857 - val_mean_absolute_error: 80.6733\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7370.6768 - mean_absolute_error: 63.8502 - val_loss: 8450.5068 - val_mean_absolute_error: 81.2662\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7455.1191 - mean_absolute_error: 64.5049 - val_loss: 8579.5908 - val_mean_absolute_error: 81.7209\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7523.4990 - mean_absolute_error: 65.0069 - val_loss: 8672.2305 - val_mean_absolute_error: 82.0418\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7573.7266 - mean_absolute_error: 65.3617 - val_loss: 8730.2520 - val_mean_absolute_error: 82.2406\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7605.6089 - mean_absolute_error: 65.5819 - val_loss: 8755.5361 - val_mean_absolute_error: 82.3263\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7619.5571 - mean_absolute_error: 65.6775 - val_loss: 8750.5537 - val_mean_absolute_error: 82.3087\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7616.6729 - mean_absolute_error: 65.6592 - val_loss: 8718.0059 - val_mean_absolute_error: 82.1968\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7598.5547 - mean_absolute_error: 65.5374 - val_loss: 8661.2520 - val_mean_absolute_error: 82.0011\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7567.2305 - mean_absolute_error: 65.3229 - val_loss: 8583.3496 - val_mean_absolute_error: 81.7299\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7524.8325 - mean_absolute_error: 65.0253 - val_loss: 8487.7090 - val_mean_absolute_error: 81.3934\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7473.6543 - mean_absolute_error: 64.6552 - val_loss: 8376.9512 - val_mean_absolute_error: 80.9974\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7415.7930 - mean_absolute_error: 64.2204 - val_loss: 8254.8701 - val_mean_absolute_error: 80.5534\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7353.6797 - mean_absolute_error: 63.7323 - val_loss: 8124.8936 - val_mean_absolute_error: 80.0714\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7289.6665 - mean_absolute_error: 63.2024 - val_loss: 7989.2803 - val_mean_absolute_error: 79.5573\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 7225.2266 - mean_absolute_error: 62.6367 - val_loss: 7850.3501 - val_mean_absolute_error: 79.0179\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7162.0781 - mean_absolute_error: 62.0607 - val_loss: 7710.5859 - val_mean_absolute_error: 78.4618\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7101.5146 - mean_absolute_error: 61.8002 - val_loss: 7571.7046 - val_mean_absolute_error: 77.8942\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 7044.5894 - mean_absolute_error: 61.5519 - val_loss: 7435.4531 - val_mean_absolute_error: 77.3219\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6992.1499 - mean_absolute_error: 61.3018 - val_loss: 7303.1611 - val_mean_absolute_error: 76.7503\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6944.7559 - mean_absolute_error: 61.0519 - val_loss: 7176.0376 - val_mean_absolute_error: 76.1847\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6902.7285 - mean_absolute_error: 60.8043 - val_loss: 7054.4209 - val_mean_absolute_error: 75.6272\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6866.0908 - mean_absolute_error: 60.5847 - val_loss: 6939.9907 - val_mean_absolute_error: 75.0870\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6835.1006 - mean_absolute_error: 60.4936 - val_loss: 6832.6587 - val_mean_absolute_error: 74.5650\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6809.3535 - mean_absolute_error: 60.4055 - val_loss: 6732.7134 - val_mean_absolute_error: 74.0642\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6788.5547 - mean_absolute_error: 60.3210 - val_loss: 6640.2783 - val_mean_absolute_error: 73.5876\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6772.2930 - mean_absolute_error: 60.2407 - val_loss: 6555.4946 - val_mean_absolute_error: 73.1379\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6760.0376 - mean_absolute_error: 60.1684 - val_loss: 6478.0806 - val_mean_absolute_error: 72.7158\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6751.3867 - mean_absolute_error: 60.3166 - val_loss: 6407.7690 - val_mean_absolute_error: 72.3225\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6745.7051 - mean_absolute_error: 60.5267 - val_loss: 6344.7285 - val_mean_absolute_error: 71.9609\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6742.5205 - mean_absolute_error: 60.8488 - val_loss: 6288.4580 - val_mean_absolute_error: 71.6304\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6741.3213 - mean_absolute_error: 61.1427 - val_loss: 6238.6396 - val_mean_absolute_error: 71.3315\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6741.6284 - mean_absolute_error: 61.4085 - val_loss: 6194.9688 - val_mean_absolute_error: 71.0642\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6743.0356 - mean_absolute_error: 61.6461 - val_loss: 6157.0352 - val_mean_absolute_error: 70.8278\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6745.0947 - mean_absolute_error: 61.8555 - val_loss: 6124.5610 - val_mean_absolute_error: 70.6224\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6747.5361 - mean_absolute_error: 62.0375 - val_loss: 6097.0166 - val_mean_absolute_error: 70.4457\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6750.1064 - mean_absolute_error: 62.1936 - val_loss: 6074.3213 - val_mean_absolute_error: 70.2982\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6752.5063 - mean_absolute_error: 62.3229 - val_loss: 6056.0190 - val_mean_absolute_error: 70.1782\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6754.6670 - mean_absolute_error: 62.4280 - val_loss: 6041.8232 - val_mean_absolute_error: 70.0846\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6756.4043 - mean_absolute_error: 62.5093 - val_loss: 6031.3032 - val_mean_absolute_error: 70.0148\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6757.6582 - mean_absolute_error: 62.5686 - val_loss: 6024.3750 - val_mean_absolute_error: 69.9688\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6758.4023 - mean_absolute_error: 62.6071 - val_loss: 6020.5762 - val_mean_absolute_error: 69.9438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6758.6104 - mean_absolute_error: 62.6262 - val_loss: 6019.7080 - val_mean_absolute_error: 69.9383\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6758.3857 - mean_absolute_error: 62.6283 - val_loss: 6021.4751 - val_mean_absolute_error: 69.9506\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6757.6641 - mean_absolute_error: 62.6142 - val_loss: 6025.5659 - val_mean_absolute_error: 69.9785\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6756.5625 - mean_absolute_error: 62.5857 - val_loss: 6031.8696 - val_mean_absolute_error: 70.0212\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6755.1172 - mean_absolute_error: 62.5447 - val_loss: 6040.0195 - val_mean_absolute_error: 70.0758\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6753.4229 - mean_absolute_error: 62.4924 - val_loss: 6049.8247 - val_mean_absolute_error: 70.1413\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 6751.5444 - mean_absolute_error: 62.4304 - val_loss: 6061.0469 - val_mean_absolute_error: 70.2157\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6749.5312 - mean_absolute_error: 62.3602 - val_loss: 6073.4092 - val_mean_absolute_error: 70.2970\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6747.4785 - mean_absolute_error: 62.2838 - val_loss: 6086.7559 - val_mean_absolute_error: 70.3844\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6745.4150 - mean_absolute_error: 62.2021 - val_loss: 6100.8857 - val_mean_absolute_error: 70.4761\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6743.3984 - mean_absolute_error: 62.1165 - val_loss: 6115.5674 - val_mean_absolute_error: 70.5707\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6741.5039 - mean_absolute_error: 62.0286 - val_loss: 6130.5923 - val_mean_absolute_error: 70.6668\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6739.6924 - mean_absolute_error: 61.9387 - val_loss: 6145.7568 - val_mean_absolute_error: 70.7629\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6738.0264 - mean_absolute_error: 61.8489 - val_loss: 6160.9678 - val_mean_absolute_error: 70.8588\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6736.5376 - mean_absolute_error: 61.7596 - val_loss: 6176.0186 - val_mean_absolute_error: 70.9528\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6735.2070 - mean_absolute_error: 61.6720 - val_loss: 6190.7305 - val_mean_absolute_error: 71.0443\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6734.0342 - mean_absolute_error: 61.5866 - val_loss: 6205.0732 - val_mean_absolute_error: 71.1327\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6732.9990 - mean_absolute_error: 61.5039 - val_loss: 6218.8477 - val_mean_absolute_error: 71.2172\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6732.1074 - mean_absolute_error: 61.4247 - val_loss: 6231.8706 - val_mean_absolute_error: 71.2965\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6731.3633 - mean_absolute_error: 61.3498 - val_loss: 6244.1943 - val_mean_absolute_error: 71.3712\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6730.7520 - mean_absolute_error: 61.2795 - val_loss: 6255.7056 - val_mean_absolute_error: 71.4406\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6730.2119 - mean_absolute_error: 61.2142 - val_loss: 6266.2349 - val_mean_absolute_error: 71.5037\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6729.7686 - mean_absolute_error: 61.1538 - val_loss: 6275.9102 - val_mean_absolute_error: 71.5614\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6729.3740 - mean_absolute_error: 61.0986 - val_loss: 6284.6069 - val_mean_absolute_error: 71.6132\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6729.0234 - mean_absolute_error: 61.0489 - val_loss: 6292.2710 - val_mean_absolute_error: 71.6587\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6728.7393 - mean_absolute_error: 61.0047 - val_loss: 6298.9951 - val_mean_absolute_error: 71.6985\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6728.4302 - mean_absolute_error: 60.9652 - val_loss: 6304.6577 - val_mean_absolute_error: 71.7319\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6728.1680 - mean_absolute_error: 60.9318 - val_loss: 6309.5039 - val_mean_absolute_error: 71.7605\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6727.8818 - mean_absolute_error: 60.9030 - val_loss: 6313.3115 - val_mean_absolute_error: 71.7829\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6727.6123 - mean_absolute_error: 60.8794 - val_loss: 6316.2139 - val_mean_absolute_error: 71.8000\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6727.3164 - mean_absolute_error: 60.8607 - val_loss: 6318.2803 - val_mean_absolute_error: 71.8123\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6726.9971 - mean_absolute_error: 60.8463 - val_loss: 6319.5273 - val_mean_absolute_error: 71.8195\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6726.6558 - mean_absolute_error: 60.8363 - val_loss: 6320.0132 - val_mean_absolute_error: 71.8224\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6726.2944 - mean_absolute_error: 60.8302 - val_loss: 6319.8228 - val_mean_absolute_error: 71.8216\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6725.9072 - mean_absolute_error: 60.8274 - val_loss: 6318.9790 - val_mean_absolute_error: 71.8167\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6725.5186 - mean_absolute_error: 60.8281 - val_loss: 6317.5889 - val_mean_absolute_error: 71.8088\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6725.1240 - mean_absolute_error: 60.8319 - val_loss: 6315.7139 - val_mean_absolute_error: 71.7980\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6724.6924 - mean_absolute_error: 60.8382 - val_loss: 6313.3916 - val_mean_absolute_error: 71.7846\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6724.2451 - mean_absolute_error: 60.8471 - val_loss: 6310.7148 - val_mean_absolute_error: 71.7690\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6723.8037 - mean_absolute_error: 60.8573 - val_loss: 6307.7012 - val_mean_absolute_error: 71.7517\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6723.3438 - mean_absolute_error: 60.8695 - val_loss: 6304.4897 - val_mean_absolute_error: 71.7330\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6722.9062 - mean_absolute_error: 60.8829 - val_loss: 6300.9751 - val_mean_absolute_error: 71.7125\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6722.4482 - mean_absolute_error: 60.8974 - val_loss: 6297.4971 - val_mean_absolute_error: 71.6923\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6722.0000 - mean_absolute_error: 60.9125 - val_loss: 6293.7632 - val_mean_absolute_error: 71.6706\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6721.5674 - mean_absolute_error: 60.9286 - val_loss: 6290.0557 - val_mean_absolute_error: 71.6488\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6721.1465 - mean_absolute_error: 60.9448 - val_loss: 6286.3135 - val_mean_absolute_error: 71.6269\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6720.6914 - mean_absolute_error: 60.9608 - val_loss: 6282.6250 - val_mean_absolute_error: 71.6053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6720.2603 - mean_absolute_error: 60.9767 - val_loss: 6279.0547 - val_mean_absolute_error: 71.5845\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6719.8237 - mean_absolute_error: 60.9919 - val_loss: 6275.5576 - val_mean_absolute_error: 71.5638\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6719.4019 - mean_absolute_error: 61.0065 - val_loss: 6272.2036 - val_mean_absolute_error: 71.5442\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6718.9678 - mean_absolute_error: 61.0206 - val_loss: 6269.0273 - val_mean_absolute_error: 71.5256\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6718.5713 - mean_absolute_error: 61.0341 - val_loss: 6265.9468 - val_mean_absolute_error: 71.5074\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6718.1455 - mean_absolute_error: 61.0464 - val_loss: 6263.1162 - val_mean_absolute_error: 71.4907\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6717.7622 - mean_absolute_error: 61.0579 - val_loss: 6260.5005 - val_mean_absolute_error: 71.4755\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6717.3408 - mean_absolute_error: 61.0681 - val_loss: 6258.0532 - val_mean_absolute_error: 71.4612\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6716.9248 - mean_absolute_error: 61.0774 - val_loss: 6255.8369 - val_mean_absolute_error: 71.4481\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6716.5249 - mean_absolute_error: 61.0852 - val_loss: 6253.8311 - val_mean_absolute_error: 71.4364\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6716.1152 - mean_absolute_error: 61.0922 - val_loss: 6252.0537 - val_mean_absolute_error: 71.4259\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6715.7007 - mean_absolute_error: 61.0978 - val_loss: 6250.4561 - val_mean_absolute_error: 71.4166\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6715.2925 - mean_absolute_error: 61.1027 - val_loss: 6249.0562 - val_mean_absolute_error: 71.4085\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6714.9106 - mean_absolute_error: 61.1064 - val_loss: 6247.9062 - val_mean_absolute_error: 71.4017\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6714.5098 - mean_absolute_error: 61.1089 - val_loss: 6246.8555 - val_mean_absolute_error: 71.3956\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6714.1060 - mean_absolute_error: 61.1104 - val_loss: 6246.0908 - val_mean_absolute_error: 71.3913\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6713.6860 - mean_absolute_error: 61.1110 - val_loss: 6245.3896 - val_mean_absolute_error: 71.3873\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6713.2744 - mean_absolute_error: 61.1108 - val_loss: 6244.8623 - val_mean_absolute_error: 71.3844\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6712.8584 - mean_absolute_error: 61.1096 - val_loss: 6244.4272 - val_mean_absolute_error: 71.3819\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6712.4341 - mean_absolute_error: 61.1078 - val_loss: 6244.1133 - val_mean_absolute_error: 71.3803\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6712.0195 - mean_absolute_error: 61.1056 - val_loss: 6243.9233 - val_mean_absolute_error: 71.3793\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6711.6040 - mean_absolute_error: 61.1026 - val_loss: 6243.8657 - val_mean_absolute_error: 71.3792\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6711.1729 - mean_absolute_error: 61.0990 - val_loss: 6243.8228 - val_mean_absolute_error: 71.3791\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6710.7451 - mean_absolute_error: 61.0951 - val_loss: 6243.9033 - val_mean_absolute_error: 71.3799\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6710.3223 - mean_absolute_error: 61.0908 - val_loss: 6244.0288 - val_mean_absolute_error: 71.3809\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6709.8750 - mean_absolute_error: 61.0860 - val_loss: 6244.1670 - val_mean_absolute_error: 71.3818\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6709.4844 - mean_absolute_error: 61.0814 - val_loss: 6244.4048 - val_mean_absolute_error: 71.3834\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6709.0479 - mean_absolute_error: 61.0763 - val_loss: 6244.5649 - val_mean_absolute_error: 71.3846\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6708.6235 - mean_absolute_error: 61.0711 - val_loss: 6244.7695 - val_mean_absolute_error: 71.3860\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6708.1938 - mean_absolute_error: 61.0659 - val_loss: 6245.0166 - val_mean_absolute_error: 71.3877\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6707.7432 - mean_absolute_error: 61.0607 - val_loss: 6245.2158 - val_mean_absolute_error: 71.3891\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6707.3345 - mean_absolute_error: 61.0558 - val_loss: 6245.3945 - val_mean_absolute_error: 71.3903\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6706.8984 - mean_absolute_error: 61.0508 - val_loss: 6245.4961 - val_mean_absolute_error: 71.3912\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6706.4673 - mean_absolute_error: 61.0461 - val_loss: 6245.6001 - val_mean_absolute_error: 71.3920\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6706.0498 - mean_absolute_error: 61.0413 - val_loss: 6245.7129 - val_mean_absolute_error: 71.3930\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6705.6074 - mean_absolute_error: 61.0368 - val_loss: 6245.7314 - val_mean_absolute_error: 71.3931\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6705.2021 - mean_absolute_error: 61.0329 - val_loss: 6245.6787 - val_mean_absolute_error: 71.3930\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6704.7437 - mean_absolute_error: 61.0289 - val_loss: 6245.6138 - val_mean_absolute_error: 71.3929\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6704.3013 - mean_absolute_error: 61.0250 - val_loss: 6245.5352 - val_mean_absolute_error: 71.3927\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6703.8579 - mean_absolute_error: 61.0213 - val_loss: 6245.3594 - val_mean_absolute_error: 71.3917\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6703.4336 - mean_absolute_error: 61.0183 - val_loss: 6245.1309 - val_mean_absolute_error: 71.3906\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6702.9893 - mean_absolute_error: 61.0151 - val_loss: 6244.8423 - val_mean_absolute_error: 71.3891\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6702.5674 - mean_absolute_error: 61.0125 - val_loss: 6244.5508 - val_mean_absolute_error: 71.3876\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6702.1191 - mean_absolute_error: 61.0098 - val_loss: 6244.2061 - val_mean_absolute_error: 71.3857\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6701.6660 - mean_absolute_error: 61.0077 - val_loss: 6243.7720 - val_mean_absolute_error: 71.3833\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6701.2441 - mean_absolute_error: 61.0059 - val_loss: 6243.3179 - val_mean_absolute_error: 71.3809\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6700.8013 - mean_absolute_error: 61.0041 - val_loss: 6242.8257 - val_mean_absolute_error: 71.3780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6700.3359 - mean_absolute_error: 61.0022 - val_loss: 6242.3301 - val_mean_absolute_error: 71.3753\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6699.8965 - mean_absolute_error: 61.0008 - val_loss: 6241.7993 - val_mean_absolute_error: 71.3724\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6699.4727 - mean_absolute_error: 60.9998 - val_loss: 6241.2705 - val_mean_absolute_error: 71.3695\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6699.0127 - mean_absolute_error: 60.9982 - val_loss: 6240.7002 - val_mean_absolute_error: 71.3664\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6698.5806 - mean_absolute_error: 60.9971 - val_loss: 6240.0703 - val_mean_absolute_error: 71.3627\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6698.1343 - mean_absolute_error: 60.9963 - val_loss: 6239.4673 - val_mean_absolute_error: 71.3594\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6697.6509 - mean_absolute_error: 60.9951 - val_loss: 6238.8306 - val_mean_absolute_error: 71.3557\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6697.2188 - mean_absolute_error: 60.9943 - val_loss: 6238.1929 - val_mean_absolute_error: 71.3522\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6696.7656 - mean_absolute_error: 60.9936 - val_loss: 6237.5107 - val_mean_absolute_error: 71.3484\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6696.3462 - mean_absolute_error: 60.9929 - val_loss: 6236.9092 - val_mean_absolute_error: 71.3450\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6695.8779 - mean_absolute_error: 60.9919 - val_loss: 6236.2705 - val_mean_absolute_error: 71.3414\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 313us/step - loss: 6695.4263 - mean_absolute_error: 60.9910 - val_loss: 6235.6162 - val_mean_absolute_error: 71.3377\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 313us/step - loss: 6694.9653 - mean_absolute_error: 60.9904 - val_loss: 6234.9805 - val_mean_absolute_error: 71.3342\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6694.5029 - mean_absolute_error: 60.9894 - val_loss: 6234.2930 - val_mean_absolute_error: 71.3302\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6694.0098 - mean_absolute_error: 60.9883 - val_loss: 6233.6616 - val_mean_absolute_error: 71.3267\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6693.5869 - mean_absolute_error: 60.9876 - val_loss: 6232.9736 - val_mean_absolute_error: 71.3227\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6693.1387 - mean_absolute_error: 60.9869 - val_loss: 6232.3721 - val_mean_absolute_error: 71.3194\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6692.6895 - mean_absolute_error: 60.9860 - val_loss: 6231.6533 - val_mean_absolute_error: 71.3154\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6692.2197 - mean_absolute_error: 60.9850 - val_loss: 6231.0381 - val_mean_absolute_error: 71.3120\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6691.7637 - mean_absolute_error: 60.9843 - val_loss: 6230.4351 - val_mean_absolute_error: 71.3085\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6691.3018 - mean_absolute_error: 60.9833 - val_loss: 6229.8062 - val_mean_absolute_error: 71.3049\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6690.8232 - mean_absolute_error: 60.9821 - val_loss: 6229.2080 - val_mean_absolute_error: 71.3017\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6690.3643 - mean_absolute_error: 60.9811 - val_loss: 6228.5645 - val_mean_absolute_error: 71.2980\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6689.9092 - mean_absolute_error: 60.9798 - val_loss: 6228.0054 - val_mean_absolute_error: 71.2948\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6689.4531 - mean_absolute_error: 60.9787 - val_loss: 6227.3794 - val_mean_absolute_error: 71.2913\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6688.9902 - mean_absolute_error: 60.9773 - val_loss: 6226.8281 - val_mean_absolute_error: 71.2882\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6688.5244 - mean_absolute_error: 60.9762 - val_loss: 6226.2827 - val_mean_absolute_error: 71.2852\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6688.0508 - mean_absolute_error: 60.9746 - val_loss: 6225.7251 - val_mean_absolute_error: 71.2822\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6687.5708 - mean_absolute_error: 60.9729 - val_loss: 6225.1421 - val_mean_absolute_error: 71.2789\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6687.0835 - mean_absolute_error: 60.9716 - val_loss: 6224.5859 - val_mean_absolute_error: 71.2758\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6686.6318 - mean_absolute_error: 60.9701 - val_loss: 6224.0430 - val_mean_absolute_error: 71.2728\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6686.1567 - mean_absolute_error: 60.9686 - val_loss: 6223.4717 - val_mean_absolute_error: 71.2697\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6685.6904 - mean_absolute_error: 60.9671 - val_loss: 6222.9580 - val_mean_absolute_error: 71.2667\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6685.2236 - mean_absolute_error: 60.9656 - val_loss: 6222.4009 - val_mean_absolute_error: 71.2636\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6684.7695 - mean_absolute_error: 60.9640 - val_loss: 6221.9404 - val_mean_absolute_error: 71.2611\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6684.2910 - mean_absolute_error: 60.9623 - val_loss: 6221.4209 - val_mean_absolute_error: 71.2582\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6683.7944 - mean_absolute_error: 60.9602 - val_loss: 6220.9136 - val_mean_absolute_error: 71.2555\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6683.3145 - mean_absolute_error: 60.9586 - val_loss: 6220.3882 - val_mean_absolute_error: 71.2525\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6682.8369 - mean_absolute_error: 60.9567 - val_loss: 6219.8491 - val_mean_absolute_error: 71.2494\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6682.3682 - mean_absolute_error: 60.9550 - val_loss: 6219.3569 - val_mean_absolute_error: 71.2468\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6681.8955 - mean_absolute_error: 60.9532 - val_loss: 6218.8516 - val_mean_absolute_error: 71.2440\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6681.3975 - mean_absolute_error: 60.9513 - val_loss: 6218.3408 - val_mean_absolute_error: 71.2412\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6680.9619 - mean_absolute_error: 60.9498 - val_loss: 6217.8530 - val_mean_absolute_error: 71.2385\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6680.4453 - mean_absolute_error: 60.9478 - val_loss: 6217.3125 - val_mean_absolute_error: 71.2355\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6679.9512 - mean_absolute_error: 60.9457 - val_loss: 6216.8125 - val_mean_absolute_error: 71.2327\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6679.4971 - mean_absolute_error: 60.9440 - val_loss: 6216.2881 - val_mean_absolute_error: 71.2298\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6679.0264 - mean_absolute_error: 60.9425 - val_loss: 6215.7734 - val_mean_absolute_error: 71.2270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6678.5322 - mean_absolute_error: 60.9405 - val_loss: 6215.2100 - val_mean_absolute_error: 71.2238\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6678.0356 - mean_absolute_error: 60.9386 - val_loss: 6214.7490 - val_mean_absolute_error: 71.2213\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6677.5742 - mean_absolute_error: 60.9369 - val_loss: 6214.1904 - val_mean_absolute_error: 71.2182\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6677.0718 - mean_absolute_error: 60.9349 - val_loss: 6213.6445 - val_mean_absolute_error: 71.2152\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6676.5674 - mean_absolute_error: 60.9330 - val_loss: 6213.1162 - val_mean_absolute_error: 71.2122\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6676.0908 - mean_absolute_error: 60.9316 - val_loss: 6212.5786 - val_mean_absolute_error: 71.2093\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6675.5981 - mean_absolute_error: 60.9296 - val_loss: 6212.0059 - val_mean_absolute_error: 71.2061\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6675.1006 - mean_absolute_error: 60.9277 - val_loss: 6211.4756 - val_mean_absolute_error: 71.2031\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6674.6138 - mean_absolute_error: 60.9261 - val_loss: 6210.9346 - val_mean_absolute_error: 71.2002\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6674.1318 - mean_absolute_error: 60.9245 - val_loss: 6210.3779 - val_mean_absolute_error: 71.1970\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6673.6294 - mean_absolute_error: 60.9225 - val_loss: 6209.8428 - val_mean_absolute_error: 71.1941\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6673.1597 - mean_absolute_error: 60.9210 - val_loss: 6209.2852 - val_mean_absolute_error: 71.1910\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6672.6582 - mean_absolute_error: 60.9192 - val_loss: 6208.7275 - val_mean_absolute_error: 71.1878\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6672.1689 - mean_absolute_error: 60.9173 - val_loss: 6208.1680 - val_mean_absolute_error: 71.1848\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6671.6846 - mean_absolute_error: 60.9158 - val_loss: 6207.5850 - val_mean_absolute_error: 71.1815\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6671.1689 - mean_absolute_error: 60.9139 - val_loss: 6207.0244 - val_mean_absolute_error: 71.1784\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6670.6553 - mean_absolute_error: 60.9122 - val_loss: 6206.4478 - val_mean_absolute_error: 71.1752\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6670.1572 - mean_absolute_error: 60.9106 - val_loss: 6205.9053 - val_mean_absolute_error: 71.1722\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6669.6826 - mean_absolute_error: 60.9088 - val_loss: 6205.3096 - val_mean_absolute_error: 71.1688\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6669.1729 - mean_absolute_error: 60.9072 - val_loss: 6204.6885 - val_mean_absolute_error: 71.1652\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6668.6675 - mean_absolute_error: 60.9053 - val_loss: 6204.1514 - val_mean_absolute_error: 71.1625\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6668.1787 - mean_absolute_error: 60.9037 - val_loss: 6203.5630 - val_mean_absolute_error: 71.1591\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6667.6582 - mean_absolute_error: 60.9021 - val_loss: 6202.9790 - val_mean_absolute_error: 71.1559\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6667.1631 - mean_absolute_error: 60.9004 - val_loss: 6202.3887 - val_mean_absolute_error: 71.1526\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6666.6768 - mean_absolute_error: 60.8989 - val_loss: 6201.8032 - val_mean_absolute_error: 71.1493\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6666.1611 - mean_absolute_error: 60.8972 - val_loss: 6201.2471 - val_mean_absolute_error: 71.1462\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6665.6729 - mean_absolute_error: 60.8956 - val_loss: 6200.6460 - val_mean_absolute_error: 71.1428\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6665.1807 - mean_absolute_error: 60.8941 - val_loss: 6200.0322 - val_mean_absolute_error: 71.1395\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6664.6577 - mean_absolute_error: 60.8922 - val_loss: 6199.4233 - val_mean_absolute_error: 71.1358\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6664.1362 - mean_absolute_error: 60.8904 - val_loss: 6198.8687 - val_mean_absolute_error: 71.1329\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6663.6299 - mean_absolute_error: 60.8888 - val_loss: 6198.3003 - val_mean_absolute_error: 71.1297\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6663.1133 - mean_absolute_error: 60.8870 - val_loss: 6197.6743 - val_mean_absolute_error: 71.1262\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6662.6084 - mean_absolute_error: 60.8853 - val_loss: 6197.0947 - val_mean_absolute_error: 71.1231\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6662.1074 - mean_absolute_error: 60.8836 - val_loss: 6196.4653 - val_mean_absolute_error: 71.1195\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6661.5645 - mean_absolute_error: 60.8816 - val_loss: 6195.8843 - val_mean_absolute_error: 71.1162\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6661.0752 - mean_absolute_error: 60.8803 - val_loss: 6195.2969 - val_mean_absolute_error: 71.1130\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6660.5732 - mean_absolute_error: 60.8786 - val_loss: 6194.6807 - val_mean_absolute_error: 71.1095\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6660.0698 - mean_absolute_error: 60.8770 - val_loss: 6194.1064 - val_mean_absolute_error: 71.1064\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6659.5249 - mean_absolute_error: 60.8748 - val_loss: 6193.5273 - val_mean_absolute_error: 71.1029\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6659.0347 - mean_absolute_error: 60.8734 - val_loss: 6192.9204 - val_mean_absolute_error: 71.0997\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6658.5229 - mean_absolute_error: 60.8716 - val_loss: 6192.3447 - val_mean_absolute_error: 71.0965\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6657.9863 - mean_absolute_error: 60.8695 - val_loss: 6191.7744 - val_mean_absolute_error: 71.0933\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6657.5068 - mean_absolute_error: 60.8681 - val_loss: 6191.1592 - val_mean_absolute_error: 71.0898\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6656.9795 - mean_absolute_error: 60.8663 - val_loss: 6190.5977 - val_mean_absolute_error: 71.0870\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6656.4629 - mean_absolute_error: 60.8645 - val_loss: 6190.0073 - val_mean_absolute_error: 71.0835\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6655.9458 - mean_absolute_error: 60.8626 - val_loss: 6189.4419 - val_mean_absolute_error: 71.0803\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6655.4043 - mean_absolute_error: 60.8605 - val_loss: 6188.7920 - val_mean_absolute_error: 71.0766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6654.9072 - mean_absolute_error: 60.8589 - val_loss: 6188.2188 - val_mean_absolute_error: 71.0734\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6654.3657 - mean_absolute_error: 60.8570 - val_loss: 6187.6377 - val_mean_absolute_error: 71.0702\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6653.8438 - mean_absolute_error: 60.8551 - val_loss: 6187.0508 - val_mean_absolute_error: 71.0669\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6653.3271 - mean_absolute_error: 60.8534 - val_loss: 6186.4131 - val_mean_absolute_error: 71.0634\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6652.7871 - mean_absolute_error: 60.8515 - val_loss: 6185.8423 - val_mean_absolute_error: 71.0602\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6652.2793 - mean_absolute_error: 60.8497 - val_loss: 6185.2490 - val_mean_absolute_error: 71.0571\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6651.7544 - mean_absolute_error: 60.8478 - val_loss: 6184.6328 - val_mean_absolute_error: 71.0535\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6651.2334 - mean_absolute_error: 60.8463 - val_loss: 6184.0557 - val_mean_absolute_error: 71.0503\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6650.6904 - mean_absolute_error: 60.8441 - val_loss: 6183.4629 - val_mean_absolute_error: 71.0470\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6650.1670 - mean_absolute_error: 60.8424 - val_loss: 6182.7896 - val_mean_absolute_error: 71.0432\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6649.6543 - mean_absolute_error: 60.8406 - val_loss: 6182.2310 - val_mean_absolute_error: 71.0402\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6649.1064 - mean_absolute_error: 60.8386 - val_loss: 6181.6123 - val_mean_absolute_error: 71.0366\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6648.5820 - mean_absolute_error: 60.8368 - val_loss: 6181.0303 - val_mean_absolute_error: 71.0334\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6648.0581 - mean_absolute_error: 60.8350 - val_loss: 6180.3647 - val_mean_absolute_error: 71.0296\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6647.5117 - mean_absolute_error: 60.8331 - val_loss: 6179.7622 - val_mean_absolute_error: 71.0263\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6646.9990 - mean_absolute_error: 60.8316 - val_loss: 6179.1338 - val_mean_absolute_error: 71.0228\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6646.4648 - mean_absolute_error: 60.8298 - val_loss: 6178.5288 - val_mean_absolute_error: 71.0195\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6645.9292 - mean_absolute_error: 60.8278 - val_loss: 6177.9067 - val_mean_absolute_error: 71.0159\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6645.4385 - mean_absolute_error: 60.8262 - val_loss: 6177.3125 - val_mean_absolute_error: 71.0128\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6644.8896 - mean_absolute_error: 60.8243 - val_loss: 6176.6885 - val_mean_absolute_error: 71.0091\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6644.3511 - mean_absolute_error: 60.8225 - val_loss: 6176.0654 - val_mean_absolute_error: 71.0059\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6643.7979 - mean_absolute_error: 60.8207 - val_loss: 6175.4219 - val_mean_absolute_error: 71.0020\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 250us/step - loss: 6643.2710 - mean_absolute_error: 60.8187 - val_loss: 6174.8135 - val_mean_absolute_error: 70.9987\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6642.7100 - mean_absolute_error: 60.8167 - val_loss: 6174.2241 - val_mean_absolute_error: 70.9955\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6642.1865 - mean_absolute_error: 60.8150 - val_loss: 6173.5938 - val_mean_absolute_error: 70.9919\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6641.6519 - mean_absolute_error: 60.8130 - val_loss: 6172.9580 - val_mean_absolute_error: 70.9884\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6641.1182 - mean_absolute_error: 60.8115 - val_loss: 6172.3135 - val_mean_absolute_error: 70.9847\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6640.5879 - mean_absolute_error: 60.8094 - val_loss: 6171.6851 - val_mean_absolute_error: 70.9812\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6640.0659 - mean_absolute_error: 60.8078 - val_loss: 6171.0884 - val_mean_absolute_error: 70.9779\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6639.4990 - mean_absolute_error: 60.8058 - val_loss: 6170.4146 - val_mean_absolute_error: 70.9740\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6638.9604 - mean_absolute_error: 60.8040 - val_loss: 6169.8022 - val_mean_absolute_error: 70.9706\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6638.4316 - mean_absolute_error: 60.8021 - val_loss: 6169.1753 - val_mean_absolute_error: 70.9672\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6637.8574 - mean_absolute_error: 60.8001 - val_loss: 6168.5444 - val_mean_absolute_error: 70.9636\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6637.3301 - mean_absolute_error: 60.7984 - val_loss: 6167.9390 - val_mean_absolute_error: 70.9602\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6636.7637 - mean_absolute_error: 60.7963 - val_loss: 6167.3188 - val_mean_absolute_error: 70.9568\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6636.2422 - mean_absolute_error: 60.7945 - val_loss: 6166.7095 - val_mean_absolute_error: 70.9534\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6635.6719 - mean_absolute_error: 60.7923 - val_loss: 6166.1147 - val_mean_absolute_error: 70.9500\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6635.1343 - mean_absolute_error: 60.7904 - val_loss: 6165.5156 - val_mean_absolute_error: 70.9468\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6634.5996 - mean_absolute_error: 60.7884 - val_loss: 6164.8569 - val_mean_absolute_error: 70.9430\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6634.0459 - mean_absolute_error: 60.7864 - val_loss: 6164.2373 - val_mean_absolute_error: 70.9396\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6633.5195 - mean_absolute_error: 60.7847 - val_loss: 6163.6025 - val_mean_absolute_error: 70.9360\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6632.9756 - mean_absolute_error: 60.7827 - val_loss: 6163.0029 - val_mean_absolute_error: 70.9327\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6632.4434 - mean_absolute_error: 60.7807 - val_loss: 6162.3740 - val_mean_absolute_error: 70.9292\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6631.8599 - mean_absolute_error: 60.7784 - val_loss: 6161.7671 - val_mean_absolute_error: 70.9259\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6631.3052 - mean_absolute_error: 60.7763 - val_loss: 6161.1260 - val_mean_absolute_error: 70.9222\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6630.7715 - mean_absolute_error: 60.7747 - val_loss: 6160.5005 - val_mean_absolute_error: 70.9188\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6630.2231 - mean_absolute_error: 60.7726 - val_loss: 6159.8994 - val_mean_absolute_error: 70.9154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6629.6475 - mean_absolute_error: 60.7705 - val_loss: 6159.2871 - val_mean_absolute_error: 70.9120\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6629.0903 - mean_absolute_error: 60.7686 - val_loss: 6158.6265 - val_mean_absolute_error: 70.9082\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 313us/step - loss: 6628.5498 - mean_absolute_error: 60.7668 - val_loss: 6157.9985 - val_mean_absolute_error: 70.9046\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6627.9814 - mean_absolute_error: 60.7646 - val_loss: 6157.3555 - val_mean_absolute_error: 70.9011\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6627.4458 - mean_absolute_error: 60.7628 - val_loss: 6156.7451 - val_mean_absolute_error: 70.8977\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6626.9043 - mean_absolute_error: 60.7609 - val_loss: 6156.0806 - val_mean_absolute_error: 70.8940\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6626.3262 - mean_absolute_error: 60.7586 - val_loss: 6155.4355 - val_mean_absolute_error: 70.8904\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6625.7363 - mean_absolute_error: 60.7565 - val_loss: 6154.8140 - val_mean_absolute_error: 70.8869\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6625.2051 - mean_absolute_error: 60.7547 - val_loss: 6154.1924 - val_mean_absolute_error: 70.8834\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6624.6284 - mean_absolute_error: 60.7526 - val_loss: 6153.5518 - val_mean_absolute_error: 70.8798\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6624.0679 - mean_absolute_error: 60.7506 - val_loss: 6152.8813 - val_mean_absolute_error: 70.8762\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6623.5049 - mean_absolute_error: 60.7485 - val_loss: 6152.2329 - val_mean_absolute_error: 70.8726\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6622.9814 - mean_absolute_error: 60.7469 - val_loss: 6151.5625 - val_mean_absolute_error: 70.8688\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6622.3809 - mean_absolute_error: 60.7448 - val_loss: 6150.8945 - val_mean_absolute_error: 70.8650\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6621.8247 - mean_absolute_error: 60.7427 - val_loss: 6150.2759 - val_mean_absolute_error: 70.8615\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6621.2715 - mean_absolute_error: 60.7407 - val_loss: 6149.5762 - val_mean_absolute_error: 70.8576\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6620.7217 - mean_absolute_error: 60.7389 - val_loss: 6148.9282 - val_mean_absolute_error: 70.8540\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6620.1602 - mean_absolute_error: 60.7372 - val_loss: 6148.2959 - val_mean_absolute_error: 70.8503\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6619.5718 - mean_absolute_error: 60.7348 - val_loss: 6147.6670 - val_mean_absolute_error: 70.8470\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6619.0425 - mean_absolute_error: 60.7331 - val_loss: 6147.0479 - val_mean_absolute_error: 70.8435\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6618.4692 - mean_absolute_error: 60.7313 - val_loss: 6146.3540 - val_mean_absolute_error: 70.8396\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6617.9082 - mean_absolute_error: 60.7292 - val_loss: 6145.6836 - val_mean_absolute_error: 70.8357\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6617.3330 - mean_absolute_error: 60.7272 - val_loss: 6144.9990 - val_mean_absolute_error: 70.8319\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6616.7793 - mean_absolute_error: 60.7254 - val_loss: 6144.3696 - val_mean_absolute_error: 70.8283\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6616.1826 - mean_absolute_error: 60.7231 - val_loss: 6143.7393 - val_mean_absolute_error: 70.8249\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6615.5923 - mean_absolute_error: 60.7209 - val_loss: 6143.0303 - val_mean_absolute_error: 70.8209\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6615.0703 - mean_absolute_error: 60.7194 - val_loss: 6142.4087 - val_mean_absolute_error: 70.8175\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6614.4888 - mean_absolute_error: 60.7174 - val_loss: 6141.6865 - val_mean_absolute_error: 70.8132\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6613.9165 - mean_absolute_error: 60.7152 - val_loss: 6141.0679 - val_mean_absolute_error: 70.8100\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6613.3276 - mean_absolute_error: 60.7131 - val_loss: 6140.3867 - val_mean_absolute_error: 70.8061\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6612.7754 - mean_absolute_error: 60.7112 - val_loss: 6139.7471 - val_mean_absolute_error: 70.8025\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6612.1914 - mean_absolute_error: 60.7091 - val_loss: 6139.1006 - val_mean_absolute_error: 70.7989\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6611.6406 - mean_absolute_error: 60.7072 - val_loss: 6138.3931 - val_mean_absolute_error: 70.7948\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6611.0586 - mean_absolute_error: 60.7051 - val_loss: 6137.7476 - val_mean_absolute_error: 70.7912\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6610.4805 - mean_absolute_error: 60.7029 - val_loss: 6137.1118 - val_mean_absolute_error: 70.7877\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6609.9102 - mean_absolute_error: 60.7009 - val_loss: 6136.4404 - val_mean_absolute_error: 70.7839\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6609.3242 - mean_absolute_error: 60.6989 - val_loss: 6135.8130 - val_mean_absolute_error: 70.7804\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6608.7393 - mean_absolute_error: 60.6966 - val_loss: 6135.1377 - val_mean_absolute_error: 70.7766\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6608.1875 - mean_absolute_error: 60.6946 - val_loss: 6134.4346 - val_mean_absolute_error: 70.7727\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6607.6035 - mean_absolute_error: 60.6926 - val_loss: 6133.7539 - val_mean_absolute_error: 70.7688\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6607.0283 - mean_absolute_error: 60.6906 - val_loss: 6133.0718 - val_mean_absolute_error: 70.7650\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6606.4648 - mean_absolute_error: 60.6889 - val_loss: 6132.4019 - val_mean_absolute_error: 70.7612\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6605.8838 - mean_absolute_error: 60.6866 - val_loss: 6131.7607 - val_mean_absolute_error: 70.7576\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6605.3062 - mean_absolute_error: 60.6845 - val_loss: 6131.0854 - val_mean_absolute_error: 70.7537\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6604.7222 - mean_absolute_error: 60.6825 - val_loss: 6130.4585 - val_mean_absolute_error: 70.7504\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6604.1226 - mean_absolute_error: 60.6803 - val_loss: 6129.7998 - val_mean_absolute_error: 70.7467\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6603.5620 - mean_absolute_error: 60.6781 - val_loss: 6129.1929 - val_mean_absolute_error: 70.7433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6602.9590 - mean_absolute_error: 60.6761 - val_loss: 6128.4775 - val_mean_absolute_error: 70.7394\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6602.3945 - mean_absolute_error: 60.6742 - val_loss: 6127.8076 - val_mean_absolute_error: 70.7356\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6601.8076 - mean_absolute_error: 60.6718 - val_loss: 6127.1587 - val_mean_absolute_error: 70.7319\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6601.2129 - mean_absolute_error: 60.6696 - val_loss: 6126.4688 - val_mean_absolute_error: 70.7280\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6600.6216 - mean_absolute_error: 60.6673 - val_loss: 6125.7632 - val_mean_absolute_error: 70.7241\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6600.0645 - mean_absolute_error: 60.6654 - val_loss: 6125.0850 - val_mean_absolute_error: 70.7201\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6599.4839 - mean_absolute_error: 60.6635 - val_loss: 6124.4575 - val_mean_absolute_error: 70.7167\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6598.8774 - mean_absolute_error: 60.6613 - val_loss: 6123.7998 - val_mean_absolute_error: 70.7130\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6598.3066 - mean_absolute_error: 60.6591 - val_loss: 6123.0679 - val_mean_absolute_error: 70.7089\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6597.7061 - mean_absolute_error: 60.6570 - val_loss: 6122.4009 - val_mean_absolute_error: 70.7052\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6597.1055 - mean_absolute_error: 60.6549 - val_loss: 6121.7446 - val_mean_absolute_error: 70.7015\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6596.5146 - mean_absolute_error: 60.6526 - val_loss: 6121.0522 - val_mean_absolute_error: 70.6976\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6595.9453 - mean_absolute_error: 60.6508 - val_loss: 6120.3672 - val_mean_absolute_error: 70.6938\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6595.3320 - mean_absolute_error: 60.6485 - val_loss: 6119.6265 - val_mean_absolute_error: 70.6894\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6594.7529 - mean_absolute_error: 60.6464 - val_loss: 6118.9775 - val_mean_absolute_error: 70.6859\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6594.1626 - mean_absolute_error: 60.6444 - val_loss: 6118.3188 - val_mean_absolute_error: 70.6822\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6593.5610 - mean_absolute_error: 60.6421 - val_loss: 6117.6240 - val_mean_absolute_error: 70.6784\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6592.9829 - mean_absolute_error: 60.6401 - val_loss: 6116.8765 - val_mean_absolute_error: 70.6741\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6592.3955 - mean_absolute_error: 60.6381 - val_loss: 6116.1797 - val_mean_absolute_error: 70.6702\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6591.7944 - mean_absolute_error: 60.6359 - val_loss: 6115.5361 - val_mean_absolute_error: 70.6668\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6591.1943 - mean_absolute_error: 60.6338 - val_loss: 6114.7944 - val_mean_absolute_error: 70.6625\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6590.6055 - mean_absolute_error: 60.6317 - val_loss: 6114.1260 - val_mean_absolute_error: 70.6586\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6589.9946 - mean_absolute_error: 60.6294 - val_loss: 6113.4473 - val_mean_absolute_error: 70.6548\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6589.3979 - mean_absolute_error: 60.6273 - val_loss: 6112.7900 - val_mean_absolute_error: 70.6512\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6588.8320 - mean_absolute_error: 60.6253 - val_loss: 6112.1025 - val_mean_absolute_error: 70.6473\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6588.2319 - mean_absolute_error: 60.6232 - val_loss: 6111.4150 - val_mean_absolute_error: 70.6435\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6587.6396 - mean_absolute_error: 60.6210 - val_loss: 6110.6924 - val_mean_absolute_error: 70.6394\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6587.0547 - mean_absolute_error: 60.6190 - val_loss: 6110.0029 - val_mean_absolute_error: 70.6355\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6586.4268 - mean_absolute_error: 60.6166 - val_loss: 6109.2905 - val_mean_absolute_error: 70.6314\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6585.8105 - mean_absolute_error: 60.6144 - val_loss: 6108.6099 - val_mean_absolute_error: 70.6276\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6585.2227 - mean_absolute_error: 60.6122 - val_loss: 6107.9062 - val_mean_absolute_error: 70.6238\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6584.6152 - mean_absolute_error: 60.6100 - val_loss: 6107.2090 - val_mean_absolute_error: 70.6197\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6584.0332 - mean_absolute_error: 60.6080 - val_loss: 6106.5454 - val_mean_absolute_error: 70.6160\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6583.4307 - mean_absolute_error: 60.6057 - val_loss: 6105.8384 - val_mean_absolute_error: 70.6121\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6582.8296 - mean_absolute_error: 60.6035 - val_loss: 6105.1250 - val_mean_absolute_error: 70.6081\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6582.2324 - mean_absolute_error: 60.6015 - val_loss: 6104.4463 - val_mean_absolute_error: 70.6043\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6581.6211 - mean_absolute_error: 60.5991 - val_loss: 6103.7686 - val_mean_absolute_error: 70.6004\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6581.0293 - mean_absolute_error: 60.5970 - val_loss: 6103.0732 - val_mean_absolute_error: 70.5966\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6580.4316 - mean_absolute_error: 60.5950 - val_loss: 6102.3315 - val_mean_absolute_error: 70.5923\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6579.7925 - mean_absolute_error: 60.5924 - val_loss: 6101.6509 - val_mean_absolute_error: 70.5884\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6579.2012 - mean_absolute_error: 60.5904 - val_loss: 6100.9639 - val_mean_absolute_error: 70.5846\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6578.6064 - mean_absolute_error: 60.5881 - val_loss: 6100.2603 - val_mean_absolute_error: 70.5808\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6578.0039 - mean_absolute_error: 60.5860 - val_loss: 6099.5708 - val_mean_absolute_error: 70.5768\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6577.4126 - mean_absolute_error: 60.5837 - val_loss: 6098.8604 - val_mean_absolute_error: 70.5728\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6576.7764 - mean_absolute_error: 60.5813 - val_loss: 6098.1680 - val_mean_absolute_error: 70.5690\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6576.1689 - mean_absolute_error: 60.5792 - val_loss: 6097.4658 - val_mean_absolute_error: 70.5650\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6575.5649 - mean_absolute_error: 60.5771 - val_loss: 6096.7798 - val_mean_absolute_error: 70.5610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6574.9624 - mean_absolute_error: 60.5748 - val_loss: 6096.0903 - val_mean_absolute_error: 70.5572\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6574.3477 - mean_absolute_error: 60.5725 - val_loss: 6095.3877 - val_mean_absolute_error: 70.5533\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6573.7559 - mean_absolute_error: 60.5704 - val_loss: 6094.6548 - val_mean_absolute_error: 70.5491\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6573.1270 - mean_absolute_error: 60.5680 - val_loss: 6093.9756 - val_mean_absolute_error: 70.5453\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6572.5254 - mean_absolute_error: 60.5658 - val_loss: 6093.2563 - val_mean_absolute_error: 70.5412\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6571.9189 - mean_absolute_error: 60.5637 - val_loss: 6092.5938 - val_mean_absolute_error: 70.5376\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6571.3052 - mean_absolute_error: 60.5613 - val_loss: 6091.8428 - val_mean_absolute_error: 70.5334\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6570.6782 - mean_absolute_error: 60.5590 - val_loss: 6091.1504 - val_mean_absolute_error: 70.5295\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6570.0713 - mean_absolute_error: 60.5567 - val_loss: 6090.4424 - val_mean_absolute_error: 70.5254\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6569.4487 - mean_absolute_error: 60.5544 - val_loss: 6089.7461 - val_mean_absolute_error: 70.5216\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6568.8433 - mean_absolute_error: 60.5524 - val_loss: 6089.0127 - val_mean_absolute_error: 70.5175\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6568.2090 - mean_absolute_error: 60.5499 - val_loss: 6088.2432 - val_mean_absolute_error: 70.5130\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6567.6348 - mean_absolute_error: 60.5479 - val_loss: 6087.5479 - val_mean_absolute_error: 70.5091\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6567.0332 - mean_absolute_error: 60.5460 - val_loss: 6086.8281 - val_mean_absolute_error: 70.5049\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6566.4014 - mean_absolute_error: 60.5438 - val_loss: 6086.1328 - val_mean_absolute_error: 70.5012\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6565.7876 - mean_absolute_error: 60.5414 - val_loss: 6085.4116 - val_mean_absolute_error: 70.4971\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6565.1699 - mean_absolute_error: 60.5390 - val_loss: 6084.6904 - val_mean_absolute_error: 70.4931\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6564.5273 - mean_absolute_error: 60.5367 - val_loss: 6083.9673 - val_mean_absolute_error: 70.4889\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6563.9141 - mean_absolute_error: 60.5345 - val_loss: 6083.2554 - val_mean_absolute_error: 70.4848\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6563.2861 - mean_absolute_error: 60.5322 - val_loss: 6082.5469 - val_mean_absolute_error: 70.4809\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6562.6719 - mean_absolute_error: 60.5298 - val_loss: 6081.8193 - val_mean_absolute_error: 70.4768\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6562.0879 - mean_absolute_error: 60.5279 - val_loss: 6081.0679 - val_mean_absolute_error: 70.4725\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6561.4526 - mean_absolute_error: 60.5256 - val_loss: 6080.3525 - val_mean_absolute_error: 70.4685\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6560.8311 - mean_absolute_error: 60.5235 - val_loss: 6079.6172 - val_mean_absolute_error: 70.4644\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6560.2412 - mean_absolute_error: 60.5215 - val_loss: 6078.8594 - val_mean_absolute_error: 70.4600\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6559.5645 - mean_absolute_error: 60.5187 - val_loss: 6078.1631 - val_mean_absolute_error: 70.4561\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6558.9600 - mean_absolute_error: 60.5166 - val_loss: 6077.4634 - val_mean_absolute_error: 70.4522\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6558.3330 - mean_absolute_error: 60.5144 - val_loss: 6076.6978 - val_mean_absolute_error: 70.4478\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6557.6992 - mean_absolute_error: 60.5119 - val_loss: 6075.9702 - val_mean_absolute_error: 70.4437\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 188us/step - loss: 6557.0859 - mean_absolute_error: 60.5099 - val_loss: 6075.2148 - val_mean_absolute_error: 70.4396\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6556.4678 - mean_absolute_error: 60.5075 - val_loss: 6074.4873 - val_mean_absolute_error: 70.4353\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 125us/step - loss: 6555.8350 - mean_absolute_error: 60.5052 - val_loss: 6073.7642 - val_mean_absolute_error: 70.4314\n"
     ]
    }
   ],
   "source": [
    "input_size=len(XX_train[0])#feature數量\n",
    "batch_size=50#每批樣本大小\n",
    "epochs=500#處理幾輪\n",
    "\n",
    "model=Sequential()  #定義model\n",
    "model.add(Dense(40,input_dim=input_size)) #加入層(緊密層) 產出個數40 輸入個數8 次元\n",
    "model.add(Activation('relu')) #啟動函數\n",
    "# model.add(Dense(3))  \n",
    "# model.add(Activation('linear')) #啟動函數\n",
    "model.add(Dense(1))  \n",
    "model.add(Activation('linear')) #啟動函數\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "train_history=model.fit(XX_train,YY_train,batch_size=batch_size,epochs=epochs,validation_split=0.2,verbose=1)\n",
    "#劃出準確度歷程\n",
    "import matplotlib.pyplot as plt\n",
    "def show_tarin_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title(\"Train History\")\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(['train','validation'],loc=\"upper left\")\n",
    "    plt.show()\n",
    "# show_tarin_history(train_history,'loss','loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XuYXXV97/H3d/bsuV+SuRCGXJhE\nIkJCSEKEcBFBKAWKWmuEUFDxQNMiVvTxtILtUfBYa0/7IKUqNB5RUYrQAIKWi6ChwAECCYYQEpAQ\nAhlym0syl8wkk5n5nj/Wms1msueSZNbszF6f1/PsZ+912Xt/VxjmM7/fb63fMndHREQEIC/bBYiI\nyOFDoSAiIikKBRERSVEoiIhIikJBRERSFAoiIpKiUJDYM7OEmXWY2bSIPn+GmXVE8dkio02hIONO\n+Au8/9FnZl1py5cd6Oe5e6+7l7n72wdRyzFmtt/FPmb2czO7Ifz8je5eNoLPusrMnjjQGkRGU362\nCxA5UOm/YM1sE3CVuz8+2P5mlu/uPWNRWzbF5TglWmopSM4xs2+Z2d1mdpeZtQOXm9mpZvacme0y\ns61mdouZJcP9883Mzaw+XP55uP1hM2s3s2fNbPoh1POe1oSZXWlmm8LP3mhmi83sBOB7wIfCFk9T\nuO+EsJ7G8D3Xm5mF264ysyfDWluAb4XHd1zad9WZWaeZVR9s/RIvCgXJVZ8A/gOoBO4GeoBrgRrg\ndOB84C+HeP+fA/8LqALeBv73aBRlZhXATcAfuXt5WMsad38Z+ALwVNiVVRO+5QdACTAD+AhwJfCZ\ntI88DVgP1AI3AvcAlw84jkfdvXk06pfcNy5DwcxuN7MdZrZ2BPt+18xWh48/mNmusahRsu5pd/+V\nu/e5e5e7v+DuK9y9x903AkuBDw/x/mXuvtLd9wF3AnOH+rLwL/TUA7h4iN0dmG1mRe6+1d3XDfKZ\nyfBzrnP39rDu7wKfTtvtbXe/NRwX6QJ+Cvx5f2si3PdnQ9Uukm5chgLwE4K/9Ibl7l9297nuPhf4\nN+C+KAuTw8bm9AUz+4CZ/ZeZbTOzNuCbBK2GwWxLe90JDDlQ7O4T0h8Ef7Fn2q8NuBS4BthmZr82\ns/cP8rFHAAngrbR1bwGT05bfc5zu/v8IWkVnmNlsYBrwX0PVLpJuXIaCuz8JtKSvM7P3mdkjZrbK\nzJ4ysw9keOulwF1jUqRk28Azgv4dWAsc4+4VwNcB2+9dY8DdH3b3c4E6YENYG+xf8w6gFzg6bd00\n4J30j8vwFXcQdCF9GrjH3feORt0SD+MyFAaxFPhrdz8J+J8EfbEpZnY0MB34XRZqk+wrB1qB3eFA\n7FDjCZEJB34/amYlQDewm+AXP8B2YEr/AHjYdbUM+LaZlYWD3V8Gfj7M1/wMWEQwnnBHBIchOSwn\nQsHMyggG3P7TzFYT/OVVN2C3xQT9xL0D3y+x8BXgs0A7wc/H3VmqIwH8DbAVaCb4uf1CuO0x4HVg\nu5n1d199niA83gT+m2DMYMhf9O6+CXgZ6Hb3Z0a5fslxNl5vshOePvhrd58dntHxmrsPDIL0/X8P\nXKP/SSQOzOwOYKO735DtWmR8yYmWQjh496aZfQrAAif2bzezY4GJwLNZKlFkzJjZDODjwO3ZrkXG\nn3EZCmZ2F8Ev+GPNrMHMrgQuA640s5eAVwj+p+h3KfALH6/NIpERMrN/BF4Cvn0w03aIjNvuIxER\nGX3jsqUgIiLRGHcT4tXU1Hh9fX22yxARGVdWrVrV5O61w+037kKhvr6elStXZrsMEZFxxczeGn4v\ndR+JiEgahYKIiKQoFEREJGXcjSlksm/fPhoaGtizZ0+2S8kZRUVFTJkyhWQyme1SRGQM5UQoNDQ0\nUF5eTn19Pe9OIy8Hy91pbm6moaGB6dMP+oZjIjIO5UT30Z49e6iurlYgjBIzo7q6Wi0vkRjKiVAA\nFAijTP+eIvGUM6EwnD37etnWuoee3r5slyIictiKTSjs7ellR/se9vWO/lxPu3bt4gc/+MHwOw5w\n4YUXsmuXbhktIoeP2IRCXtgd0hfBBICDhUJv79D383nooYeYMGHCqNcjInKwcuLso5FI5AWh0Ns3\n+qFw3XXX8cYbbzB37lySySRlZWXU1dWxevVq1q1bx5/+6Z+yefNm9uzZw7XXXsuSJUuAd6fs6Ojo\n4IILLuCMM87gmWeeYfLkyTzwwAMUFxePeq0iIkPJuVC48VevsG5L237r+9zp6u6lMJkgP+/ABlGP\nP6qCb3x01qDbv/Od77B27VpWr17NE088wZ/8yZ+wdu3a1Omct99+O1VVVXR1dfHBD36QT37yk1RX\nV7/nM15//XXuuusufvjDH3LxxRdz7733cvnllx9QnSIihyrnQmEwqbNp3IFoz6w5+eST33N+/y23\n3ML9998PwObNm3n99df3C4Xp06czd+5cAE466SQ2bdoUaY0iIpnkXCgM9hd9b5/zypZWjqws4ojy\nokhrKC0tTb1+4oknePzxx3n22WcpKSnhrLPOynj+f2FhYep1IpGgq6sr0hpFRDKJ0UAzGEZfBGek\nlpeX097ennFba2srEydOpKSkhFdffZXnnntu9AsQERklOddSGIyZkZcXzdlH1dXVnH766cyePZvi\n4mImTZqU2nb++edz2223MWfOHI499lgWLlw46t8vIjJaxt09mhcsWOADb7Kzfv16jjvuuGHf++rW\nNkoL85laVRJVeTllpP+uInL4M7NV7r5guP1i030EkJdnkZySKiKSK2IVCgmzSLqPRERyRaxCIS/P\n6FUoiIgMKlahkLBozj4SEckVsQqFvLxoprkQEckVsQqFRJ7GFEREhhJZKJhZkZk9b2YvmdkrZnZj\nhn0KzexuM9tgZivMrD6qeiCYKbXPPevBUFZWBsCWLVtYtGhRxn3OOussBp56O9DNN99MZ2dnallT\ncYvIoYqypbAX+Ii7nwjMBc43s4FXbl0J7HT3Y4DvAv8UYT2pmVL7DpMupKOOOoply5Yd9PsHhoKm\n4haRQxVZKHigI1xMho+Bv40/Dvw0fL0MOMcivA9kVPdU+OpXv/qe+ynccMMN3HjjjZxzzjnMnz+f\nE044gQceeGC/923atInZs2cD0NXVxeLFi5kzZw6XXHLJe+Y+uvrqq1mwYAGzZs3iG9/4BhBMsrdl\nyxbOPvtszj77bCCYirupqQmAm266idmzZzN79mxuvvnm1Pcdd9xx/MVf/AWzZs3ivPPO0xxLIvIe\nkU5zYWYJYBVwDPB9d18xYJfJwGYAd+8xs1agGmg66C99+DrY9nLGTRV9fczY10eiIAEHkj1HngAX\nfGfQzYsXL+ZLX/oSn//85wG45557eOSRR/jyl79MRUUFTU1NLFy4kI997GOD3vv41ltvpaSkhDVr\n1rBmzRrmz5+f2vYP//APVFVV0dvbyznnnMOaNWv44he/yE033cTy5cupqal5z2etWrWKH//4x6xY\nsQJ355RTTuHDH/4wEydO1BTdIjKkSAea3b3X3ecCU4CTzWz2gF0y/Ybc7894M1tiZivNbGVjY+NB\n1xNVE2TevHns2LGDLVu28NJLLzFx4kTq6ur42te+xpw5czj33HN555132L59+6Cf8eSTT6Z+Oc+Z\nM4c5c+aktt1zzz3Mnz+fefPm8corr7Bu3boh63n66af5xCc+QWlpKWVlZfzZn/0ZTz31FKApukVk\naGMyIZ677zKzJ4DzgbVpmxqAqUCDmeUDlUBLhvcvBZZCMPfRkF82xF/0e7t72Lijg/rqUiqKkwd4\nFENbtGgRy5YtY9u2bSxevJg777yTxsZGVq1aRTKZpL6+PuOU2ekytSLefPNN/uVf/oUXXniBiRMn\ncsUVVwz7OUPNZ6UpukVkKFGefVRrZhPC18XAucCrA3Z7EPhs+HoR8DuPcIa+KO/TvHjxYn7xi1+w\nbNkyFi1aRGtrK0cccQTJZJLly5fz1ltvDfn+M888kzvvvBOAtWvXsmbNGgDa2tooLS2lsrKS7du3\n8/DDD6feM9iU3WeeeSa//OUv6ezsZPfu3dx///186EMfGsWjFZFcFWVLoQ74aTiukAfc4+6/NrNv\nAivd/UHgR8DPzGwDQQthcYT1RHqf5lmzZtHe3s7kyZOpq6vjsssu46Mf/SgLFixg7ty5fOADHxjy\n/VdffTWf+9znmDNnDnPnzuXkk08G4MQTT2TevHnMmjWLGTNmcPrpp6fes2TJEi644ALq6upYvnx5\nav38+fO54oorUp9x1VVXMW/ePHUViciwYjV1dl+fs3aM7r6WCzR1tkju0NTZGVjq7mvjKwhFRMZK\nzEIhuPtarzJBRCSjnAmFkXaDBTOlKhWGM966FUVkdOREKBQVFdHc3DyiX2S6+9rw3J3m5maKijTu\nIhI3Y3KdQtSmTJlCQ0MDI7mwrbF9LwBdjYXD7BlvRUVFTJkyJdtliMgYy4lQSCaTTJ8+fUT7/p8f\nP09TRze/+uu5EVclIjL+5ET30YEoL0rSsbcn22WIiByWYhcKZUX5tO9RKIiIZBK7UCgvzKd9z75s\nlyEicliKXygU5bO3p4/unr5slyIictiJXSj0z46q1oKIyP7iFwpFQSi0aVxBRGQ/sQuF8qLgLNy2\nLrUUREQGil0o9Hcftan7SERkP/ELhf7uoy51H4mIDBS/UCgOu4/UUhAR2U/sQqG8SGcfiYgMJnah\nUFqQIM/UfSQikknsQsHMqChOqvtIRCSD2IUCBIPNOiVVRGR/8QyF4nxdvCYikkFkoWBmU81suZmt\nN7NXzOzaDPucZWatZrY6fHw9qnrSlRcmNdAsIpJBlDfZ6QG+4u4vmlk5sMrMHnP3dQP2e8rdL4qw\njv1UFOezqalzLL9SRGRciKyl4O5b3f3F8HU7sB6YHNX3HYiKIg00i4hkMiZjCmZWD8wDVmTYfKqZ\nvWRmD5vZrLGop6JYA80iIplEfo9mMysD7gW+5O5tAza/CBzt7h1mdiHwS2Bmhs9YAiwBmDZt2iHX\nVF6Uz+7uXnp6+8hPxHKsXUQko0h/I5pZkiAQ7nT3+wZud/c2d+8IXz8EJM2sJsN+S919gbsvqK2t\nPeS6KlJXNesMJBGRdFGefWTAj4D17n7TIPscGe6HmZ0c1tMcVU393r3RjkJBRCRdlN1HpwOfBl42\ns9Xhuq8B0wDc/TZgEXC1mfUAXcBid/dIqtmyGn7/czjrOiqKNCmeiEgmkYWCuz8N2DD7fA/4XlQ1\nvEfrZnjhhzD/01QUTwF0ox0RkYHiM8paXBU8d7a8e/c1tRRERN4jPqFQUh08dzbrRjsiIoOIUSiE\nLYWunbolp4jIIOITCsUTg+fOFsoL8zFDk+KJiAwQn1BIJKGwArpayMszygryNdAsIjJAfEIBgtZC\nZwuAbrQjIpJBvEKhpAq6glAoL8rXQLOIyADxCoXiKrUURESGEK9QSGsp6JacIiL7i1coFFdB504A\nKjV9tojIfuIVCiXVsLcVevcxoSRJq0JBROQ9YhYK717AVlmcZHd3L909fdmtSUTkMBKvUEi7gG1C\nSXBVs1oLIiLvilcopFoKLVQW94dCdxYLEhE5vMQrFNJmSp1QUgCopSAiki5eoZChpbCrU6EgItIv\nXqGQ3lJQKIiI7CdeoVBQCokC6Hp3oHmXuo9ERFLiFQpmqakuyouSmGlMQUQkXbxCAYJxhc4WEnlG\nRVGS1k6dfSQi0i+GoVCdmv+osjip7iMRkTSRhYKZTTWz5Wa23sxeMbNrM+xjZnaLmW0wszVmNj+q\nelLS7qkwoSSpgWYRkTT5EX52D/AVd3/RzMqBVWb2mLuvS9vnAmBm+DgFuDV8jk7aTKmVxZr/SEQk\nXWQtBXff6u4vhq/bgfXA5AG7fRy4wwPPARPMrC6qmoB376ngzoSSAoWCiEiaMRlTMLN6YB6wYsCm\nycDmtOUG9g8OzGyJma00s5WNjY2HVkxpDXgv7NlFZXE+uzTQLCKSEnkomFkZcC/wJXdvG7g5w1t8\nvxXuS919gbsvqK2tPbSCSmqC593NTCgOWgp9fft9pYhILEUaCmaWJAiEO939vgy7NABT05anAFui\nrInS6uC5s4kJJUn6HDq6da9mERGI9uwjA34ErHf3mwbZ7UHgM+FZSAuBVnffGlVNQFpLoendmVJ1\nBpKICBDt2UenA58GXjaz1eG6rwHTANz9NuAh4EJgA9AJfC7CegKlYffT7sbUTKm7OvcxtSrybxYR\nOexFFgru/jSZxwzS93HgmqhqyKg0bCl0NlFZ3T//kQabRUQgjlc05xdCQXkw0Ky7r4mIvEf8QgGC\nwebOJk2fLSIyQExDoRZ2N1JRrJaCiEi6eIZCSQ3sbqYomaAomacL2EREQvEMhbD7CEhdwCYiInEN\nhZIa2N0Uzn+UpGW3QkFEBOIaCqW10LcP9rRSVVrATnUfiYgAsQ2F/msVmplYWsDO3QoFERGIayik\nTXVRXVpAi1oKIiLACEPBzK41s4pwjqIfmdmLZnZe1MVFJm1SvInhPRV6evuyW5OIyGFgpC2F/xFO\ne30eUEswR9F3Iqsqaqn5j5qoKi3AHd2rWUSEkYdC/xxGFwI/dveXGGZeo8NaqvuokarSYFI8jSuI\niIw8FFaZ2W8IQuHR8J7L47e/JVkEBWXQ2ZwKhRaFgojIiGdJvRKYC2x0904zq2IsprmOUkk17A7G\nFEChICICI28pnAq85u67zOxy4O+B1ujKGgOltdDZRHVZGAo6A0lEZMShcCvQaWYnAn8LvAXcEVlV\nY6G0JrzRTjApnsYURERGHgo94Q1xPg78q7v/K1AeXVljoLQWOhopzE9QVphPs0JBRGTEYwrtZnY9\nwe01P2RmCSAZXVljoGwS7G6Evt5gqguFgojIiFsKlwB7Ca5X2AZMBv45sqrGQtkk8F7obGFiaQEt\nutGOiMjIQiEMgjuBSjO7CNjj7uN7TKHsiOC5YztVJUm1FEREGPk0FxcDzwOfAi4GVpjZoigLi1zZ\npOC5YztVpYU6JVVEhJGPKfwd8EF33wFgZrXA48Cywd5gZrcDFwE73H12hu1nAQ8Ab4ar7nP3b468\n9EPU31LY3UhVaZ1CQUSEkYdCXn8ghJoZvpXxE+B7DH3q6lPuftEIaxhdaS2FiaUFdO3rpau7l+KC\nRFbKERE5HIw0FB4xs0eBu8LlS4CHhnqDuz9pZvUHX1rECssgWQodO6iuevcCtskFxVkuTEQke0Y6\n0Pw3wFJgDnAisNTdvzoK33+qmb1kZg+b2azBdjKzJWa20sxWNjY2jsLXhsqOCFoKJZoUT0QERt5S\nwN3vBe4dxe9+ETja3TvM7ELgl8DMQb57KUEosWDBAh+1CsomhQPNmv9IRASGaSmYWbuZtWV4tJtZ\n26F8sbu3uXtH+PohIGlmNYfymQes7Ajo2KFQEBEJDdlScPfIprIwsyOB7e7uZnYyQUA1R/V9GZVN\ngk1PKRREREIj7j46UGZ2F3AWUGNmDcA3CKfGcPfbgEXA1WbWA3QBi8P5lcZO2STo2klFfh+JPKN5\n994x/XoRkcNNZKHg7pcOs/17BKesZk94rUJeVxM1ZQU0taulICLxNtK5j3JT2rUKNWWFNHaopSAi\n8RbzUOif/2gHteWFNCkURCTmYh4K77YUassKaWxXKIhIvMU7FEprg+eOHdSELYWxHusWETmcxDsU\n8guguCrVUtjX67R26b4KIhJf8Q4FCLqQ2rdRW14IoC4kEYk1hUJFHbRtoaZMoSAiolCoOAratrzb\nUtAZSCISYwqFisnBmEJx8E+hloKIxJlCobwOcCp6mylI5KmlICKxplComAyAhYPNmupCROJMoVBR\nFzy3vUNNWYFaCiISawqFsKXQP9isMQURiTOFQvFEyC+C9i2a/0hEYk+hYBYMNofXKjR37KW3T1Nd\niEg8KRQg6EIKu4/6XHdgE5H4UihA6qrm2vCqZnUhiUhcKRQguKq5fSs1ZcG9mndosFlEYkqhAFB+\nFPR2U5fcDcCOtj1ZLkhEJDsUChC0FIBabwZga6tCQUTiSaEAqWsVCju3UV1aoFAQkdiKLBTM7HYz\n22FmawfZbmZ2i5ltMLM1ZjY/qlqGlbqqeQt1E4rY1tqVtVJERLIpypbCT4Dzh9h+ATAzfCwBbo2w\nlqGVTYK8fGht4MiKYrUURCS2IgsFd38SaBlil48Dd3jgOWCCmdVFVc+Q8hJBF9Kut6mrLGKbBppF\nJKayOaYwGdicttwQrtuPmS0xs5VmtrKxsTGaaiZMg11vc2RlEbs699HV3RvN94iIHMayGQqWYV3G\n+SXcfam7L3D3BbW1tdFUM+HoVEsBYKvGFUQkhrIZCg3A1LTlKcCWLNUCE4+Gjm0cVRb8k2zTuIKI\nxFA2Q+FB4DPhWUgLgVZ335q1aiZMA2CKBd1TGmwWkTjKj+qDzewu4CygxswagG8ASQB3vw14CLgQ\n2AB0Ap+LqpYRCUPhiN7tABpsFpFYiiwU3P3SYbY7cE1U33/AJhwNQEFHAxNLprBll8YURCR+dEVz\nv/IjIS8ZnoFUrDEFEYklhUK/vARUToGdb3FUZZHGFEQklhQK6SYenbpWQWMKIhJHCoV04QVsdZVF\ntOzuZs8+XcAmIvGiUEg3YRrs3sFRZcF1depCEpG4USikm1APwIz84L4KDTs7s1iMiMjYUyikC69V\nmGo7ANjcotNSRSReFArpqmYAMHHPZpIJ4+0WtRREJF4UCulKa6CokryWN5g8oZjN6j4SkZhRKKQz\ng+pjoOl1plaV0KCWgojEjEJhoOqZ0LyBqVUlbN6pMQURiReFwkA1x0DbO0yvMFp2d9OxtyfbFYmI\njBmFwkDVMwE4NhnMlrpZXUgiEiMKhYGqjwHgaA/u96MzkEQkThQKA1W/DzAmdQe3j36reXd26xER\nGUMKhYGSxVA5laK2jVSXFvBmk0JBROJDoZBJ9fug6XWm15TyRqNCQUTiQ6GQSc1MaH6DGTUlaimI\nSKwoFDKpngnd7cyq2ENj+17a9+zLdkUiImNCoZBJ7bEAHJ//DoBaCyISGwqFTCbNAqC+500ANmpc\nQURiItJQMLPzzew1M9tgZtdl2H6FmTWa2erwcVWU9YxYaQ2UHUlVx+vkGWxs7Mh2RSIiYyKyUDCz\nBPB94ALgeOBSMzs+w653u/vc8PF/o6rngE2aRaJxHdOqStigUBCRmIiypXAysMHdN7p7N/AL4OMR\nft/omjQLGl/lA0eU8Oq29mxXIyIyJqIMhcnA5rTlhnDdQJ80szVmtszMpmb6IDNbYmYrzWxlY2Nj\nFLXub9Js6O1mYWULm5p2s2df79h8r4hIFkUZCpZhnQ9Y/hVQ7+5zgMeBn2b6IHdf6u4L3H1BbW3t\nKJc5iHCw+cSCBvocNuxQF5KI5L4oQ6EBSP/LfwqwJX0Hd292973h4g+BkyKs58DUHgv5RUzf+wcA\ndSGJSCxEGQovADPNbLqZFQCLgQfTdzCzurTFjwHrI6znwCSScOQcKltepjA/j9e2tWW7IhGRyOVH\n9cHu3mNmXwAeBRLA7e7+ipl9E1jp7g8CXzSzjwE9QAtwRVT1HJTJ87EX7+DYI4rVUhCRWIgsFADc\n/SHgoQHrvp72+nrg+ihrOCSTT4IVt3FWVQs/21iGu2OWaahERCQ36IrmoRw1H4DTijaxs3MfDbpn\ns4jkOIXCUKpmQFEl79/3GgCrN+/KckEiItFSKAwlLw+mncrExpUU5OfxkkJBRHKcQmE4R5+GtWzg\nQ5N6WNPQmu1qREQipVAYztFnAHBBxUZefqeVnt6+LBckIhIdhcJw6k6EgjI+yHq69vXy8jtqLYhI\n7lIoDCeRD9MWMrn1BQCe3dic5YJERKKjUBiJY84lv2UDZ9Z28OwbCgURyV0KhZGYeR4AiyvXsXLT\nTrp7NK4gIrlJoTAS1e+D6pl8cN9Kuvb18vu3d2a7IhGRSCgURur9f0xN0/NMTHTx2Lrt2a5GRCQS\nCoWRmvUJrLebayat4zfrtuM+8NYQIiLjn0JhpCafBFUzuMie5u2WTs2aKiI5SaEwUmYw5xImNT9P\nnTXzX2u2ZrsiEZFRp1A4ECcuxoDra5/hP1dt1tXNIpJzFAoHYmI9HHcR5+95mLa2Vpa/1pjtikRE\nRpVC4UCd+gUKundxVclT/PSZTdmuRkRkVCkUDtTUU2D6mVyTdx9rNmzi+Tdbsl2RiMioUSgcKDP4\n429T2NPG35fcxz8/+ip9fTo9VURyg0LhYBx5AnbKX3Jx3yNUvP04P3vurWxXJCIyKhQKB+vcG/Ej\n5/Bvhbfyq4d+xaq31I0kIuNfpKFgZueb2WtmtsHMrsuwvdDM7g63rzCz+ijrGVXJImzxf1BYUctP\n8r/NXbf/K0+/3pTtqkREDolFNV2DmSWAPwB/BDQALwCXuvu6tH0+D8xx978ys8XAJ9z9kqE+d8GC\nBb5y5cpIaj4orQ103/VpCra9yHN9x/GHKZ/ihDMuYvb7jyGZn9hv9+6ePjq7e+jY20Nndy/7evtI\n5Bn5eUaeGclEXvgwkvl5FCSCR16eZeHgRCRXmNkqd18w7H4RhsKpwA3u/sfh8vUA7v6Pafs8Gu7z\nrJnlA9uAWh+iqMMuFAB699H97FL2/Pd3qdgXXLvQ4UW0Wxl9lqDX8zD6yPNeEgSPfPoGPPeSMKfH\n8+glL9wrjz4sfM6jjwR91v86D7cEfWPdA6hsEsmarTM+xcLLbzio9440FPIP6tNHZjKwOW25AThl\nsH3cvcfMWoFq4D39MGa2BFgCMG3atKjqPXiJJAVnXEPBaX9Fx8YVvP3y03Rt3wB7W/HeHvLNIS+f\nvEQ+ifwkeYkk+flJEskklpcIgoMEfW54Xw/e14v39dLX2xMu96XWE24jfBh9+Bj9ptYkgCLZlV9Z\nF/13RPjZmX5TDfytMpJ9cPelwFIIWgqHXlpE8hKUHXMaxx9zWrYrERE5KFH2PTQAU9OWpwBbBtsn\n7D6qBHQaj4hIlkQZCi8AM81supkVAIuBBwfs8yDw2fD1IuB3Q40niIhItCLrPgrHCL4APAokgNvd\n/RUz+yaw0t0fBH4E/MzMNhC0EBZHVY+IiAwvyjEF3P0h4KEB676e9noP8KkoaxARkZHTFc0iIpKi\nUBARkRSFgoiIpCgUREQkJbJhBnrmAAAFjklEQVRpLqJiZo3Awc5VXcOAq6VjQMccDzrmeDiUYz7a\n3WuH22nchcKhMLOVI5n7I5fomONBxxwPY3HM6j4SEZEUhYKIiKTELRSWZruALNAxx4OOOR4iP+ZY\njSmIiMjQ4tZSEBGRISgUREQkJTahYGbnm9lrZrbBzK7Ldj2jxcxuN7MdZrY2bV2VmT1mZq+HzxPD\n9WZmt4T/BmvMbH72Kj94ZjbVzJab2Xoze8XMrg3X5+xxm1mRmT1vZi+Fx3xjuH66ma0Ij/nucJp6\nzKwwXN4Qbq/PZv0Hy8wSZvZ7M/t1uJzTxwtgZpvM7GUzW21mK8N1Y/azHYtQMLME8H3gAuB44FIz\nOz67VY2anwDnD1h3HfBbd58J/DZchuD4Z4aPJcCtY1TjaOsBvuLuxwELgWvC/565fNx7gY+4+4nA\nXOB8M1sI/BPw3fCYdwJXhvtfCex092OA74b7jUfXAuvTlnP9ePud7e5z065JGLufbXfP+QdwKvBo\n2vL1wPXZrmsUj68eWJu2/BpQF76uA14LX/87cGmm/cbzA3gA+KO4HDdQArxIcM/zJiA/XJ/6OSe4\nj8mp4ev8cD/Ldu0HeJxTwl+AHwF+TXD73pw93rTj3gTUDFg3Zj/bsWgpAJOBzWnLDeG6XDXJ3bcC\nhM9HhOtz7t8h7CaYB6wgx4877EpZDewAHgPeAHa5e0+4S/pxpY453N4KVI9txYfsZuBvgb5wuZrc\nPt5+DvzGzFaZ2ZJw3Zj9bEd6k53DiGVYF8dzcXPq38HMyoB7gS+5e5tZpsMLds2wbtwdt7v3AnPN\nbAJwP3Bcpt3C53F9zGZ2EbDD3VeZ2Vn9qzPsmhPHO8Dp7r7FzI4AHjOzV4fYd9SPOy4thQZgatry\nFGBLlmoZC9vNrA4gfN4Rrs+ZfwczSxIEwp3ufl+4OuePG8DddwFPEIynTDCz/j/u0o8rdczh9kqC\nW96OF6cDHzOzTcAvCLqQbiZ3jzfF3beEzzsIwv9kxvBnOy6h8AIwMzxzoYDgXtAPZrmmKD0IfDZ8\n/VmCPvf+9Z8Jz1hYCLT2N0nHEwuaBD8C1rv7TWmbcva4zaw2bCFgZsXAuQQDsMuBReFuA4+5/99i\nEfA7DzudxwN3v97dp7h7PcH/r79z98vI0ePtZ2alZlbe/xo4D1jLWP5sZ3tQZQwHby4E/kDQD/t3\n2a5nFI/rLmArsI/gr4YrCfpSfwu8Hj5XhfsawVlYbwAvAwuyXf9BHvMZBE3kNcDq8HFhLh83MAf4\nfXjMa4Gvh+tnAM8DG4D/BArD9UXh8oZw+4xsH8MhHPtZwK/jcLzh8b0UPl7p/101lj/bmuZCRERS\n4tJ9JCIiI6BQEBGRFIWCiIikKBRERCRFoSAiIikKBZEBzKw3nKGy/zFqs+qaWb2lzWgrcriJyzQX\nIgeiy93nZrsIkWxQS0FkhMJ57v8pvK/B82Z2TLj+aDP7bTif/W/NbFq4fpKZ3R/eA+ElMzst/KiE\nmf0wvC/Cb8IrlEUOCwoFkf0VD+g+uiRtW5u7nwx8j2AuHsLXd7j7HOBO4JZw/S3Af3twD4T5BFeo\nQjD3/ffdfRawC/hkxMcjMmK6ollkADPrcPeyDOs3EdzoZmM4Id82d682syaCOez3heu3unuNmTUC\nU9x9b9pn1AOPeXCzFMzsq0DS3b8V/ZGJDE8tBZED44O8HmyfTPamve5FY3tyGFEoiByYS9Kenw1f\nP0MwkyfAZcDT4evfAldD6gY5FWNVpMjB0l8oIvsrDu9w1u8Rd+8/LbXQzFYQ/EF1abjui8DtZvY3\nQCPwuXD9tcBSM7uSoEVwNcGMtiKHLY0piIxQOKawwN2bsl2LSFTUfSQiIilqKYiISIpaCiIikqJQ\nEBGRFIWCiIikKBRERCRFoSAiIin/HzPcah+kYq+3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a662908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tarin_history(train_history,'loss','val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.25324490663803"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=model.predict(XX_train).reshape([len(XX_train)])-np.array(YY_train)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.58980012328833"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=model.predict(XX_test).reshape([len(XX_test)])-np.array(YY_test)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c3e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a939860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict=model.predict(XX_train)\n",
    "plotPaint(predict,YY_train,title=\"train\")\n",
    "predict=model.predict(XX_test)\n",
    "plotPaint(predict,YY_test,title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicted_sales = model.predict(newDataxxG)\n",
    "# print(\"好店家預測\")\n",
    "# print(predicted_sales)\n",
    "# predicted_sales = model.predict(newDataxxB)\n",
    "# print(\"差店家預測\")\n",
    "# print(predicted_sales)\n",
    "# predict=model.predict(newDataxxG)\n",
    "# plotPaint(predict,YG,R=1)\n",
    "# predict=model.predict(newDataxxB)\n",
    "# plotPaint(predict,YB,R=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多層(DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=len(XX_train[0])#feature數量\n",
    "batch_size=50#每批樣本大小\n",
    "# epochs=5000#處理幾輪\n",
    "epochs=1500#處理幾輪\n",
    "\n",
    "model=Sequential()  #定義model\n",
    "model.add(Dense(40,input_dim=input_size)) #加入層(緊密層) 產出個數40 輸入個數8 次元\n",
    "model.add(Activation('relu')) #啟動函數\n",
    "model.add(Dense(200)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(200)) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(250)) \n",
    "model.add(Activation('relu')) \n",
    "for i in range(20):\n",
    "    model.add(Dense(200-i*8)) \n",
    "    model.add(Activation('relu')) \n",
    "model.add(Dense(20)) \n",
    "# model.add(Dense(50)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Dense(1))  \n",
    "model.add(Activation('linear')) #啟動函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 40)                600       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 200)               8200      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 250)               50250     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 200)               50200     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 192)               38592     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 184)               35512     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 184)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 176)               32560     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 176)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 168)               29736     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 160)               27040     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 152)               24472     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 152)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 144)               22032     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 136)               19720     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 136)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 128)               17536     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 112)               13552     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 104)               11752     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 104)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 96)                10080     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 88)                8536      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 80)                7120      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 72)                5832      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                4672      \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 56)                3640      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 48)                2736      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 20)                980       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 481,051\n",
      "Trainable params: 481,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss成本函數mse(均方差)  optimizer最佳化工具adam(會自動調整學習速率、並繼承上一步的方法) metrics性能評估方法()\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "# model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "# model.compile(loss=\"MSE\",metrics=['accuracy'],optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 5 samples\n",
      "Epoch 1/1500\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 48448.5664 - mean_absolute_error: 210.6412 - val_loss: 49789.3359 - val_mean_absolute_error: 217.2049\n",
      "Epoch 2/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 45040.2109 - mean_absolute_error: 202.7350 - val_loss: 36756.4531 - val_mean_absolute_error: 184.9533\n",
      "Epoch 3/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 30615.1777 - mean_absolute_error: 163.6228 - val_loss: 9745.4160 - val_mean_absolute_error: 88.6626\n",
      "Epoch 4/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14500.0967 - mean_absolute_error: 103.9683 - val_loss: 8467.1748 - val_mean_absolute_error: 77.2878\n",
      "Epoch 5/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 19358.7383 - mean_absolute_error: 119.3987 - val_loss: 20523.1934 - val_mean_absolute_error: 131.3344\n",
      "Epoch 6/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16150.5352 - mean_absolute_error: 99.5486 - val_loss: 26812.2559 - val_mean_absolute_error: 154.8679\n",
      "Epoch 7/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 21934.5312 - mean_absolute_error: 128.5967 - val_loss: 24359.2148 - val_mean_absolute_error: 146.2562\n",
      "Epoch 8/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 21267.4961 - mean_absolute_error: 123.5768 - val_loss: 15479.1777 - val_mean_absolute_error: 108.1401\n",
      "Epoch 9/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13193.0859 - mean_absolute_error: 89.4084 - val_loss: 8506.9004 - val_mean_absolute_error: 78.1359\n",
      "Epoch 10/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17390.5312 - mean_absolute_error: 116.5701 - val_loss: 10758.3906 - val_mean_absolute_error: 93.5328\n",
      "Epoch 11/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 15417.2793 - mean_absolute_error: 99.2290 - val_loss: 17887.2949 - val_mean_absolute_error: 119.6652\n",
      "Epoch 12/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13860.7031 - mean_absolute_error: 98.4373 - val_loss: 21913.1055 - val_mean_absolute_error: 136.9903\n",
      "Epoch 13/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 22155.4609 - mean_absolute_error: 128.3944 - val_loss: 20650.3164 - val_mean_absolute_error: 131.8771\n",
      "Epoch 14/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 19520.7461 - mean_absolute_error: 116.9843 - val_loss: 14797.4531 - val_mean_absolute_error: 106.4216\n",
      "Epoch 15/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16522.5547 - mean_absolute_error: 103.0027 - val_loss: 10109.9111 - val_mean_absolute_error: 90.5861\n",
      "Epoch 16/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15353.5303 - mean_absolute_error: 96.3252 - val_loss: 8665.8242 - val_mean_absolute_error: 80.5130\n",
      "Epoch 17/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14357.0518 - mean_absolute_error: 95.8784 - val_loss: 10542.3535 - val_mean_absolute_error: 92.6026\n",
      "Epoch 18/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15233.6826 - mean_absolute_error: 102.4379 - val_loss: 16120.9971 - val_mean_absolute_error: 110.9225\n",
      "Epoch 19/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13616.6133 - mean_absolute_error: 97.9550 - val_loss: 19672.8906 - val_mean_absolute_error: 127.7439\n",
      "Epoch 20/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15698.3926 - mean_absolute_error: 99.4146 - val_loss: 20785.5820 - val_mean_absolute_error: 132.4477\n",
      "Epoch 21/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13570.1875 - mean_absolute_error: 94.5431 - val_loss: 19460.3184 - val_mean_absolute_error: 126.8243\n",
      "Epoch 22/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13547.9043 - mean_absolute_error: 91.2142 - val_loss: 14338.4316 - val_mean_absolute_error: 105.2140\n",
      "Epoch 23/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13873.0977 - mean_absolute_error: 86.7255 - val_loss: 11494.3389 - val_mean_absolute_error: 96.4245\n",
      "Epoch 24/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15360.1201 - mean_absolute_error: 109.8921 - val_loss: 14232.7393 - val_mean_absolute_error: 104.9300\n",
      "Epoch 25/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13940.8262 - mean_absolute_error: 91.0655 - val_loss: 17386.1699 - val_mean_absolute_error: 117.2999\n",
      "Epoch 26/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15339.5020 - mean_absolute_error: 104.6834 - val_loss: 17495.3789 - val_mean_absolute_error: 117.8285\n",
      "Epoch 27/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15171.7412 - mean_absolute_error: 99.4925 - val_loss: 15508.8770 - val_mean_absolute_error: 108.1867\n",
      "Epoch 28/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10950.9209 - mean_absolute_error: 76.4473 - val_loss: 12591.5449 - val_mean_absolute_error: 100.1558\n",
      "Epoch 29/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15281.8721 - mean_absolute_error: 93.6325 - val_loss: 13405.1191 - val_mean_absolute_error: 102.6142\n",
      "Epoch 30/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13608.3525 - mean_absolute_error: 90.6536 - val_loss: 13291.7783 - val_mean_absolute_error: 102.2797\n",
      "Epoch 31/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 16542.0391 - mean_absolute_error: 107.8251 - val_loss: 11672.8262 - val_mean_absolute_error: 97.0672\n",
      "Epoch 32/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11553.8369 - mean_absolute_error: 84.4507 - val_loss: 10696.4824 - val_mean_absolute_error: 93.2780\n",
      "Epoch 33/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 19247.8828 - mean_absolute_error: 118.0663 - val_loss: 14385.8730 - val_mean_absolute_error: 105.3217\n",
      "Epoch 34/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14938.8926 - mean_absolute_error: 92.6473 - val_loss: 17838.7402 - val_mean_absolute_error: 119.4954\n",
      "Epoch 35/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 14761.2100 - mean_absolute_error: 90.7161 - val_loss: 18089.7910 - val_mean_absolute_error: 120.6753\n",
      "Epoch 36/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16411.4199 - mean_absolute_error: 97.4888 - val_loss: 17619.5254 - val_mean_absolute_error: 118.4595\n",
      "Epoch 37/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13534.5059 - mean_absolute_error: 84.4568 - val_loss: 14366.3574 - val_mean_absolute_error: 105.2648\n",
      "Epoch 38/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15632.9102 - mean_absolute_error: 103.2134 - val_loss: 12010.6934 - val_mean_absolute_error: 98.2419\n",
      "Epoch 39/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16942.4297 - mean_absolute_error: 101.4615 - val_loss: 11421.2393 - val_mean_absolute_error: 96.1498\n",
      "Epoch 40/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 14156.7324 - mean_absolute_error: 95.3834 - val_loss: 11824.6895 - val_mean_absolute_error: 97.6010\n",
      "Epoch 41/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15607.5176 - mean_absolute_error: 103.3742 - val_loss: 13107.9355 - val_mean_absolute_error: 101.7286\n",
      "Epoch 42/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12486.4756 - mean_absolute_error: 88.9223 - val_loss: 14529.4453 - val_mean_absolute_error: 105.6934\n",
      "Epoch 43/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12706.7715 - mean_absolute_error: 83.0653 - val_loss: 13189.8047 - val_mean_absolute_error: 101.9721\n",
      "Epoch 44/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10547.1523 - mean_absolute_error: 80.9035 - val_loss: 10198.7920 - val_mean_absolute_error: 91.0468\n",
      "Epoch 45/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17423.8672 - mean_absolute_error: 117.3438 - val_loss: 13259.4590 - val_mean_absolute_error: 102.1768\n",
      "Epoch 46/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15297.7383 - mean_absolute_error: 102.0192 - val_loss: 14877.6982 - val_mean_absolute_error: 106.5915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13504.7520 - mean_absolute_error: 86.1864 - val_loss: 15640.4688 - val_mean_absolute_error: 108.4796\n",
      "Epoch 48/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12637.8926 - mean_absolute_error: 85.7084 - val_loss: 14102.9893 - val_mean_absolute_error: 104.5536\n",
      "Epoch 49/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13739.3555 - mean_absolute_error: 89.8258 - val_loss: 11600.8359 - val_mean_absolute_error: 96.8074\n",
      "Epoch 50/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14234.7559 - mean_absolute_error: 93.9047 - val_loss: 12452.3389 - val_mean_absolute_error: 99.6974\n",
      "Epoch 51/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11942.1426 - mean_absolute_error: 80.5635 - val_loss: 13153.9951 - val_mean_absolute_error: 101.8603\n",
      "Epoch 52/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12368.8828 - mean_absolute_error: 87.1962 - val_loss: 13310.3564 - val_mean_absolute_error: 102.3197\n",
      "Epoch 53/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10681.3262 - mean_absolute_error: 79.6902 - val_loss: 13122.8096 - val_mean_absolute_error: 101.7664\n",
      "Epoch 54/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9782.4004 - mean_absolute_error: 74.6940 - val_loss: 11888.3438 - val_mean_absolute_error: 97.8199\n",
      "Epoch 55/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12320.7168 - mean_absolute_error: 87.8501 - val_loss: 10714.8496 - val_mean_absolute_error: 93.3648\n",
      "Epoch 56/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11970.3535 - mean_absolute_error: 92.1648 - val_loss: 11468.6582 - val_mean_absolute_error: 96.3250\n",
      "Epoch 57/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11297.4014 - mean_absolute_error: 84.3689 - val_loss: 11202.2705 - val_mean_absolute_error: 95.3239\n",
      "Epoch 58/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12558.4023 - mean_absolute_error: 90.0914 - val_loss: 13748.3623 - val_mean_absolute_error: 103.5657\n",
      "Epoch 59/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15980.1143 - mean_absolute_error: 104.1725 - val_loss: 14246.5801 - val_mean_absolute_error: 104.9286\n",
      "Epoch 60/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12554.1367 - mean_absolute_error: 81.1047 - val_loss: 14503.3799 - val_mean_absolute_error: 105.6073\n",
      "Epoch 61/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15575.2012 - mean_absolute_error: 98.4752 - val_loss: 12447.9395 - val_mean_absolute_error: 99.6779\n",
      "Epoch 62/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11399.0361 - mean_absolute_error: 76.9219 - val_loss: 9213.8965 - val_mean_absolute_error: 85.5265\n",
      "Epoch 63/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16523.1914 - mean_absolute_error: 110.9176 - val_loss: 12002.0732 - val_mean_absolute_error: 98.2058\n",
      "Epoch 64/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13427.2812 - mean_absolute_error: 95.6441 - val_loss: 13483.6436 - val_mean_absolute_error: 102.8116\n",
      "Epoch 65/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13076.1318 - mean_absolute_error: 93.0303 - val_loss: 13963.2920 - val_mean_absolute_error: 104.1553\n",
      "Epoch 66/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14677.7617 - mean_absolute_error: 96.3847 - val_loss: 14497.5049 - val_mean_absolute_error: 105.5852\n",
      "Epoch 67/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12839.4062 - mean_absolute_error: 88.3485 - val_loss: 13923.0020 - val_mean_absolute_error: 104.0416\n",
      "Epoch 68/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15607.8213 - mean_absolute_error: 105.9800 - val_loss: 14488.7783 - val_mean_absolute_error: 105.5596\n",
      "Epoch 69/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14236.3506 - mean_absolute_error: 91.5112 - val_loss: 13389.0059 - val_mean_absolute_error: 102.5333\n",
      "Epoch 70/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 13803.2129 - mean_absolute_error: 99.1671 - val_loss: 13734.8809 - val_mean_absolute_error: 103.5155\n",
      "Epoch 71/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14321.0996 - mean_absolute_error: 95.1004 - val_loss: 13396.3389 - val_mean_absolute_error: 102.5515\n",
      "Epoch 72/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12972.4629 - mean_absolute_error: 87.2957 - val_loss: 13448.0938 - val_mean_absolute_error: 102.7000\n",
      "Epoch 73/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14395.3955 - mean_absolute_error: 95.2698 - val_loss: 11342.2559 - val_mean_absolute_error: 95.8528\n",
      "Epoch 74/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12685.7637 - mean_absolute_error: 87.5438 - val_loss: 10276.1172 - val_mean_absolute_error: 91.4381\n",
      "Epoch 75/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13164.0156 - mean_absolute_error: 83.9698 - val_loss: 10921.4688 - val_mean_absolute_error: 94.2207\n",
      "Epoch 76/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12467.6680 - mean_absolute_error: 81.1975 - val_loss: 10853.8809 - val_mean_absolute_error: 93.9470\n",
      "Epoch 77/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11703.0664 - mean_absolute_error: 93.9101 - val_loss: 12694.6982 - val_mean_absolute_error: 100.4429\n",
      "Epoch 78/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13471.4492 - mean_absolute_error: 95.7442 - val_loss: 13010.6699 - val_mean_absolute_error: 101.4074\n",
      "Epoch 79/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16712.2617 - mean_absolute_error: 106.1492 - val_loss: 11594.6094 - val_mean_absolute_error: 96.7754\n",
      "Epoch 80/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13809.7949 - mean_absolute_error: 96.8351 - val_loss: 9331.6074 - val_mean_absolute_error: 86.3660\n",
      "Epoch 81/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13586.0352 - mean_absolute_error: 92.0069 - val_loss: 9785.1855 - val_mean_absolute_error: 89.0093\n",
      "Epoch 82/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12453.4512 - mean_absolute_error: 91.5723 - val_loss: 13808.8340 - val_mean_absolute_error: 103.7071\n",
      "Epoch 83/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13683.1680 - mean_absolute_error: 90.2750 - val_loss: 14701.0332 - val_mean_absolute_error: 106.0910\n",
      "Epoch 84/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14171.1465 - mean_absolute_error: 94.9165 - val_loss: 13476.3330 - val_mean_absolute_error: 102.7680\n",
      "Epoch 85/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13656.5752 - mean_absolute_error: 91.8376 - val_loss: 11858.0762 - val_mean_absolute_error: 97.7006\n",
      "Epoch 86/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11313.6553 - mean_absolute_error: 82.6000 - val_loss: 9514.3203 - val_mean_absolute_error: 87.5016\n",
      "Epoch 87/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15858.5020 - mean_absolute_error: 99.5356 - val_loss: 9723.9424 - val_mean_absolute_error: 88.6883\n",
      "Epoch 88/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14875.9941 - mean_absolute_error: 103.4273 - val_loss: 13019.2617 - val_mean_absolute_error: 101.4253\n",
      "Epoch 89/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12229.1006 - mean_absolute_error: 88.3681 - val_loss: 15658.6533 - val_mean_absolute_error: 108.6708\n",
      "Epoch 90/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13186.9336 - mean_absolute_error: 88.8161 - val_loss: 14572.8779 - val_mean_absolute_error: 105.7512\n",
      "Epoch 91/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13486.3750 - mean_absolute_error: 86.6301 - val_loss: 11651.8105 - val_mean_absolute_error: 96.9765\n",
      "Epoch 92/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14396.9551 - mean_absolute_error: 96.0205 - val_loss: 10065.2119 - val_mean_absolute_error: 90.4503\n",
      "Epoch 93/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12164.6367 - mean_absolute_error: 89.8425 - val_loss: 9207.3545 - val_mean_absolute_error: 85.5862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 16565.7363 - mean_absolute_error: 111.5593 - val_loss: 13116.6768 - val_mean_absolute_error: 101.7114\n",
      "Epoch 95/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13178.0234 - mean_absolute_error: 85.6345 - val_loss: 15429.6719 - val_mean_absolute_error: 107.9023\n",
      "Epoch 96/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12705.5078 - mean_absolute_error: 87.0164 - val_loss: 13792.9248 - val_mean_absolute_error: 103.6491\n",
      "Epoch 97/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14835.5703 - mean_absolute_error: 92.3437 - val_loss: 12046.1709 - val_mean_absolute_error: 98.3364\n",
      "Epoch 98/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14554.4336 - mean_absolute_error: 98.3274 - val_loss: 11318.8828 - val_mean_absolute_error: 95.7620\n",
      "Epoch 99/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 14709.9551 - mean_absolute_error: 101.5419 - val_loss: 14255.7969 - val_mean_absolute_error: 104.9061\n",
      "Epoch 100/1500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 14182.2803 - mean_absolute_error: 98.9786 - val_loss: 16332.4189 - val_mean_absolute_error: 112.2301\n",
      "Epoch 101/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14876.2529 - mean_absolute_error: 98.0253 - val_loss: 15023.5723 - val_mean_absolute_error: 106.8895\n",
      "Epoch 102/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14188.5479 - mean_absolute_error: 93.8743 - val_loss: 12745.8779 - val_mean_absolute_error: 100.5827\n",
      "Epoch 103/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 15067.9883 - mean_absolute_error: 99.1998 - val_loss: 12796.2969 - val_mean_absolute_error: 100.7369\n",
      "Epoch 104/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13075.6455 - mean_absolute_error: 84.1015 - val_loss: 10602.9707 - val_mean_absolute_error: 92.9087\n",
      "Epoch 105/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12717.5664 - mean_absolute_error: 97.7446 - val_loss: 12798.4043 - val_mean_absolute_error: 100.7428\n",
      "Epoch 106/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13239.1816 - mean_absolute_error: 85.3669 - val_loss: 14389.1221 - val_mean_absolute_error: 105.2514\n",
      "Epoch 107/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14545.4990 - mean_absolute_error: 98.7065 - val_loss: 13450.5986 - val_mean_absolute_error: 102.6728\n",
      "Epoch 108/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14226.3203 - mean_absolute_error: 87.5827 - val_loss: 12375.7852 - val_mean_absolute_error: 99.4136\n",
      "Epoch 109/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13506.7207 - mean_absolute_error: 94.7482 - val_loss: 10897.7500 - val_mean_absolute_error: 94.1310\n",
      "Epoch 110/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9989.6543 - mean_absolute_error: 75.1167 - val_loss: 9132.4980 - val_mean_absolute_error: 85.1273\n",
      "Epoch 111/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13356.7490 - mean_absolute_error: 97.9917 - val_loss: 10312.4863 - val_mean_absolute_error: 91.6334\n",
      "Epoch 112/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13737.2119 - mean_absolute_error: 92.2306 - val_loss: 13938.3076 - val_mean_absolute_error: 104.0337\n",
      "Epoch 113/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12933.5625 - mean_absolute_error: 88.2155 - val_loss: 15580.1309 - val_mean_absolute_error: 108.3151\n",
      "Epoch 114/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16197.5068 - mean_absolute_error: 108.1220 - val_loss: 13861.2998 - val_mean_absolute_error: 103.8190\n",
      "Epoch 115/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15178.5693 - mean_absolute_error: 96.5447 - val_loss: 13729.2891 - val_mean_absolute_error: 103.4514\n",
      "Epoch 116/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14989.1602 - mean_absolute_error: 102.8791 - val_loss: 11640.2129 - val_mean_absolute_error: 96.9268\n",
      "Epoch 117/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14268.6445 - mean_absolute_error: 97.4905 - val_loss: 13157.6875 - val_mean_absolute_error: 101.8104\n",
      "Epoch 118/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11825.4424 - mean_absolute_error: 88.4514 - val_loss: 11594.6035 - val_mean_absolute_error: 96.7638\n",
      "Epoch 119/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11492.8994 - mean_absolute_error: 79.7785 - val_loss: 10854.0127 - val_mean_absolute_error: 93.9562\n",
      "Epoch 120/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14974.3984 - mean_absolute_error: 95.8728 - val_loss: 13405.9941 - val_mean_absolute_error: 102.5298\n",
      "Epoch 121/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12805.2129 - mean_absolute_error: 88.8653 - val_loss: 14867.7441 - val_mean_absolute_error: 106.4682\n",
      "Epoch 122/1500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 13691.9355 - mean_absolute_error: 90.1071 - val_loss: 13688.8066 - val_mean_absolute_error: 103.3297\n",
      "Epoch 123/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12525.6094 - mean_absolute_error: 84.1238 - val_loss: 11545.4365 - val_mean_absolute_error: 96.5863\n",
      "Epoch 124/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12809.4570 - mean_absolute_error: 84.6288 - val_loss: 10712.0801 - val_mean_absolute_error: 93.3755\n",
      "Epoch 125/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12441.5205 - mean_absolute_error: 93.3423 - val_loss: 11198.7246 - val_mean_absolute_error: 95.3059\n",
      "Epoch 126/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10919.1777 - mean_absolute_error: 80.2630 - val_loss: 11429.4268 - val_mean_absolute_error: 96.1648\n",
      "Epoch 127/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17764.0898 - mean_absolute_error: 112.3506 - val_loss: 17103.9414 - val_mean_absolute_error: 116.1657\n",
      "Epoch 128/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13990.7969 - mean_absolute_error: 91.1555 - val_loss: 19207.0605 - val_mean_absolute_error: 125.9040\n",
      "Epoch 129/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15940.4521 - mean_absolute_error: 102.7181 - val_loss: 16968.4102 - val_mean_absolute_error: 115.4948\n",
      "Epoch 130/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13360.6191 - mean_absolute_error: 93.9520 - val_loss: 12167.1064 - val_mean_absolute_error: 98.7215\n",
      "Epoch 131/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12762.7734 - mean_absolute_error: 91.3767 - val_loss: 8477.4932 - val_mean_absolute_error: 79.2257\n",
      "Epoch 132/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 18710.5898 - mean_absolute_error: 116.2798 - val_loss: 11507.2051 - val_mean_absolute_error: 96.4463\n",
      "Epoch 133/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14302.0586 - mean_absolute_error: 90.0734 - val_loss: 18558.6973 - val_mean_absolute_error: 123.0173\n",
      "Epoch 134/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 15066.6328 - mean_absolute_error: 90.4041 - val_loss: 22170.9727 - val_mean_absolute_error: 138.1646\n",
      "Epoch 135/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 18029.5645 - mean_absolute_error: 115.3941 - val_loss: 20772.3867 - val_mean_absolute_error: 132.5685\n",
      "Epoch 136/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17018.0859 - mean_absolute_error: 110.2344 - val_loss: 15913.4121 - val_mean_absolute_error: 110.1601\n",
      "Epoch 137/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12436.2559 - mean_absolute_error: 87.9426 - val_loss: 9660.4873 - val_mean_absolute_error: 88.4298\n",
      "Epoch 138/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12100.8945 - mean_absolute_error: 93.2115 - val_loss: 8407.8262 - val_mean_absolute_error: 74.3649\n",
      "Epoch 139/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16688.0527 - mean_absolute_error: 113.8314 - val_loss: 9509.6035 - val_mean_absolute_error: 87.5890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12095.5059 - mean_absolute_error: 93.2847 - val_loss: 15710.1074 - val_mean_absolute_error: 109.0951\n",
      "Epoch 141/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13853.6494 - mean_absolute_error: 95.5651 - val_loss: 21658.6699 - val_mean_absolute_error: 136.1579\n",
      "Epoch 142/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 20644.8867 - mean_absolute_error: 117.4312 - val_loss: 23174.9961 - val_mean_absolute_error: 142.0214\n",
      "Epoch 143/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 16476.0254 - mean_absolute_error: 102.5324 - val_loss: 20015.4219 - val_mean_absolute_error: 129.4186\n",
      "Epoch 144/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 16739.0000 - mean_absolute_error: 105.8783 - val_loss: 13158.2783 - val_mean_absolute_error: 101.7884\n",
      "Epoch 145/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11378.3848 - mean_absolute_error: 81.8171 - val_loss: 8527.6055 - val_mean_absolute_error: 80.0144\n",
      "Epoch 146/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16591.8867 - mean_absolute_error: 107.9733 - val_loss: 9834.7227 - val_mean_absolute_error: 89.3591\n",
      "Epoch 147/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16949.2305 - mean_absolute_error: 111.5123 - val_loss: 17308.0723 - val_mean_absolute_error: 117.1975\n",
      "Epoch 148/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15469.8662 - mean_absolute_error: 97.5953 - val_loss: 22550.2070 - val_mean_absolute_error: 139.6480\n",
      "Epoch 149/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16744.7812 - mean_absolute_error: 106.7796 - val_loss: 22953.4062 - val_mean_absolute_error: 141.1903\n",
      "Epoch 150/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 21692.9023 - mean_absolute_error: 124.4095 - val_loss: 18995.6191 - val_mean_absolute_error: 125.0107\n",
      "Epoch 151/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13642.0713 - mean_absolute_error: 92.9864 - val_loss: 12376.1973 - val_mean_absolute_error: 99.3838\n",
      "Epoch 152/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14202.8008 - mean_absolute_error: 99.4088 - val_loss: 8243.9004 - val_mean_absolute_error: 71.2484\n",
      "Epoch 153/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 23174.9668 - mean_absolute_error: 127.4797 - val_loss: 9132.9951 - val_mean_absolute_error: 85.2723\n",
      "Epoch 154/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11961.1699 - mean_absolute_error: 85.7630 - val_loss: 11654.5781 - val_mean_absolute_error: 96.9635\n",
      "Epoch 155/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15269.9541 - mean_absolute_error: 102.5032 - val_loss: 14602.4980 - val_mean_absolute_error: 105.7488\n",
      "Epoch 156/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12505.2188 - mean_absolute_error: 88.2804 - val_loss: 17713.0059 - val_mean_absolute_error: 119.1511\n",
      "Epoch 157/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13700.6211 - mean_absolute_error: 89.8238 - val_loss: 17564.2324 - val_mean_absolute_error: 118.4480\n",
      "Epoch 158/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13645.5459 - mean_absolute_error: 92.1167 - val_loss: 13876.6094 - val_mean_absolute_error: 103.8130\n",
      "Epoch 159/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12284.7412 - mean_absolute_error: 91.0342 - val_loss: 9681.8379 - val_mean_absolute_error: 88.5757\n",
      "Epoch 160/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13016.0508 - mean_absolute_error: 95.1638 - val_loss: 8445.3262 - val_mean_absolute_error: 79.1288\n",
      "Epoch 161/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11589.6416 - mean_absolute_error: 86.4777 - val_loss: 8717.7373 - val_mean_absolute_error: 82.0730\n",
      "Epoch 162/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 18465.8750 - mean_absolute_error: 116.9886 - val_loss: 15331.7402 - val_mean_absolute_error: 107.5717\n",
      "Epoch 163/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15714.6406 - mean_absolute_error: 98.5158 - val_loss: 20687.0371 - val_mean_absolute_error: 132.2546\n",
      "Epoch 164/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16517.1133 - mean_absolute_error: 98.3495 - val_loss: 22860.6914 - val_mean_absolute_error: 140.8543\n",
      "Epoch 165/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 18318.3809 - mean_absolute_error: 116.0752 - val_loss: 20624.9902 - val_mean_absolute_error: 132.0004\n",
      "Epoch 166/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17483.8047 - mean_absolute_error: 107.7052 - val_loss: 15218.9795 - val_mean_absolute_error: 107.2905\n",
      "Epoch 167/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12027.2676 - mean_absolute_error: 79.8104 - val_loss: 10002.4512 - val_mean_absolute_error: 90.2233\n",
      "Epoch 168/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12643.3262 - mean_absolute_error: 92.0815 - val_loss: 8973.0049 - val_mean_absolute_error: 84.1900\n",
      "Epoch 169/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12564.1992 - mean_absolute_error: 95.2720 - val_loss: 9631.1787 - val_mean_absolute_error: 88.3190\n",
      "Epoch 170/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12141.5430 - mean_absolute_error: 93.6292 - val_loss: 11209.4189 - val_mean_absolute_error: 95.3418\n",
      "Epoch 171/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10481.5400 - mean_absolute_error: 79.4847 - val_loss: 12122.6016 - val_mean_absolute_error: 98.5486\n",
      "Epoch 172/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13469.3457 - mean_absolute_error: 87.6913 - val_loss: 12582.4297 - val_mean_absolute_error: 100.0209\n",
      "Epoch 173/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11941.7598 - mean_absolute_error: 84.9130 - val_loss: 11780.7100 - val_mean_absolute_error: 97.3949\n",
      "Epoch 174/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13872.5117 - mean_absolute_error: 97.8037 - val_loss: 13219.6279 - val_mean_absolute_error: 101.9381\n",
      "Epoch 175/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 14168.8506 - mean_absolute_error: 99.0214 - val_loss: 12352.1660 - val_mean_absolute_error: 99.2915\n",
      "Epoch 176/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11646.9414 - mean_absolute_error: 82.9131 - val_loss: 10321.1982 - val_mean_absolute_error: 91.7146\n",
      "Epoch 177/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11772.2539 - mean_absolute_error: 83.0733 - val_loss: 9694.9141 - val_mean_absolute_error: 88.6752\n",
      "Epoch 178/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14176.9912 - mean_absolute_error: 103.0167 - val_loss: 11823.4004 - val_mean_absolute_error: 97.5397\n",
      "Epoch 179/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 14138.2451 - mean_absolute_error: 95.9555 - val_loss: 15446.0967 - val_mean_absolute_error: 107.8264\n",
      "Epoch 180/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15216.8750 - mean_absolute_error: 92.1288 - val_loss: 16779.6328 - val_mean_absolute_error: 114.6875\n",
      "Epoch 181/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13647.5996 - mean_absolute_error: 88.5270 - val_loss: 15365.7910 - val_mean_absolute_error: 107.6288\n",
      "Epoch 182/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16185.1582 - mean_absolute_error: 102.0101 - val_loss: 12427.6270 - val_mean_absolute_error: 99.5273\n",
      "Epoch 183/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12194.8262 - mean_absolute_error: 90.2510 - val_loss: 9725.7617 - val_mean_absolute_error: 88.8462\n",
      "Epoch 184/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14190.0391 - mean_absolute_error: 94.3422 - val_loss: 9915.1660 - val_mean_absolute_error: 89.8125\n",
      "Epoch 185/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14514.0186 - mean_absolute_error: 100.0058 - val_loss: 12674.3232 - val_mean_absolute_error: 100.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11637.6250 - mean_absolute_error: 80.4923 - val_loss: 14667.7441 - val_mean_absolute_error: 105.8776\n",
      "Epoch 187/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11977.9785 - mean_absolute_error: 82.2257 - val_loss: 14517.9746 - val_mean_absolute_error: 105.4901\n",
      "Epoch 188/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11914.1270 - mean_absolute_error: 81.8375 - val_loss: 11913.7578 - val_mean_absolute_error: 97.8431\n",
      "Epoch 189/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17899.4570 - mean_absolute_error: 107.7597 - val_loss: 10614.3721 - val_mean_absolute_error: 92.9863\n",
      "Epoch 190/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13575.8516 - mean_absolute_error: 93.7087 - val_loss: 10751.2129 - val_mean_absolute_error: 93.5507\n",
      "Epoch 191/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13353.4297 - mean_absolute_error: 94.7025 - val_loss: 12072.4893 - val_mean_absolute_error: 98.3719\n",
      "Epoch 192/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12702.6826 - mean_absolute_error: 80.7694 - val_loss: 12826.9453 - val_mean_absolute_error: 100.7571\n",
      "Epoch 193/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13097.4141 - mean_absolute_error: 92.5027 - val_loss: 11862.3574 - val_mean_absolute_error: 97.6658\n",
      "Epoch 194/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12985.3496 - mean_absolute_error: 93.3348 - val_loss: 10316.4902 - val_mean_absolute_error: 91.7034\n",
      "Epoch 195/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10815.4375 - mean_absolute_error: 72.2492 - val_loss: 8618.6357 - val_mean_absolute_error: 81.3521\n",
      "Epoch 196/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11373.7588 - mean_absolute_error: 87.2051 - val_loss: 8318.0518 - val_mean_absolute_error: 77.7127\n",
      "Epoch 197/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16365.2490 - mean_absolute_error: 106.3040 - val_loss: 11498.8359 - val_mean_absolute_error: 96.3968\n",
      "Epoch 198/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11562.4072 - mean_absolute_error: 76.4275 - val_loss: 14764.1465 - val_mean_absolute_error: 106.1102\n",
      "Epoch 199/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12903.3301 - mean_absolute_error: 83.9503 - val_loss: 16205.5059 - val_mean_absolute_error: 111.8303\n",
      "Epoch 200/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14237.3105 - mean_absolute_error: 88.1355 - val_loss: 15057.9688 - val_mean_absolute_error: 106.8470\n",
      "Epoch 201/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14876.7725 - mean_absolute_error: 101.3378 - val_loss: 12752.3887 - val_mean_absolute_error: 100.5245\n",
      "Epoch 202/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10538.6484 - mean_absolute_error: 81.4367 - val_loss: 9336.3828 - val_mean_absolute_error: 86.6924\n",
      "Epoch 203/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13947.0762 - mean_absolute_error: 96.8773 - val_loss: 9385.7939 - val_mean_absolute_error: 86.9889\n",
      "Epoch 204/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16402.6328 - mean_absolute_error: 111.0148 - val_loss: 13072.0361 - val_mean_absolute_error: 101.4788\n",
      "Epoch 205/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12491.5078 - mean_absolute_error: 84.3203 - val_loss: 15216.7373 - val_mean_absolute_error: 107.2317\n",
      "Epoch 206/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13717.9756 - mean_absolute_error: 90.6563 - val_loss: 14985.1934 - val_mean_absolute_error: 106.6552\n",
      "Epoch 207/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14627.1572 - mean_absolute_error: 93.8867 - val_loss: 14068.5332 - val_mean_absolute_error: 104.2771\n",
      "Epoch 208/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12117.8750 - mean_absolute_error: 82.6128 - val_loss: 11048.0107 - val_mean_absolute_error: 94.7238\n",
      "Epoch 209/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9701.7168 - mean_absolute_error: 72.4980 - val_loss: 8755.2520 - val_mean_absolute_error: 82.6585\n",
      "Epoch 210/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14249.0244 - mean_absolute_error: 96.0219 - val_loss: 8745.9717 - val_mean_absolute_error: 82.5864\n",
      "Epoch 211/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11171.6406 - mean_absolute_error: 93.4286 - val_loss: 10519.2949 - val_mean_absolute_error: 92.5931\n",
      "Epoch 212/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10818.0312 - mean_absolute_error: 90.2497 - val_loss: 12571.9229 - val_mean_absolute_error: 99.9579\n",
      "Epoch 213/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13663.7129 - mean_absolute_error: 95.6805 - val_loss: 15649.8691 - val_mean_absolute_error: 108.9690\n",
      "Epoch 214/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12858.3145 - mean_absolute_error: 84.0650 - val_loss: 16324.9326 - val_mean_absolute_error: 112.4828\n",
      "Epoch 215/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14639.0625 - mean_absolute_error: 96.9402 - val_loss: 13990.3887 - val_mean_absolute_error: 104.0583\n",
      "Epoch 216/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14127.1777 - mean_absolute_error: 93.9278 - val_loss: 10569.0332 - val_mean_absolute_error: 92.8049\n",
      "Epoch 217/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10542.6426 - mean_absolute_error: 81.0988 - val_loss: 8609.1699 - val_mean_absolute_error: 81.4044\n",
      "Epoch 218/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13336.6934 - mean_absolute_error: 95.1871 - val_loss: 8576.8145 - val_mean_absolute_error: 81.0987\n",
      "Epoch 219/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17657.5156 - mean_absolute_error: 118.4484 - val_loss: 13020.8301 - val_mean_absolute_error: 101.3129\n",
      "Epoch 220/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13585.8408 - mean_absolute_error: 91.4577 - val_loss: 18884.3984 - val_mean_absolute_error: 124.6406\n",
      "Epoch 221/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13784.3926 - mean_absolute_error: 92.5654 - val_loss: 21725.2168 - val_mean_absolute_error: 136.5233\n",
      "Epoch 222/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 18562.0762 - mean_absolute_error: 112.6988 - val_loss: 20832.2754 - val_mean_absolute_error: 132.9365\n",
      "Epoch 223/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16529.5000 - mean_absolute_error: 103.5769 - val_loss: 17004.4648 - val_mean_absolute_error: 115.8932\n",
      "Epoch 224/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14750.5244 - mean_absolute_error: 100.0940 - val_loss: 11495.4785 - val_mean_absolute_error: 96.3770\n",
      "Epoch 225/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12732.1426 - mean_absolute_error: 88.8405 - val_loss: 8254.6699 - val_mean_absolute_error: 77.0518\n",
      "Epoch 226/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 17587.1719 - mean_absolute_error: 106.2052 - val_loss: 8336.0645 - val_mean_absolute_error: 78.3673\n",
      "Epoch 227/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 18255.1172 - mean_absolute_error: 119.2918 - val_loss: 13457.6045 - val_mean_absolute_error: 102.5693\n",
      "Epoch 228/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14117.7578 - mean_absolute_error: 97.1609 - val_loss: 20440.6621 - val_mean_absolute_error: 131.3360\n",
      "Epoch 229/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 19474.0273 - mean_absolute_error: 114.7392 - val_loss: 23640.9668 - val_mean_absolute_error: 143.8546\n",
      "Epoch 230/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 20834.1836 - mean_absolute_error: 127.0721 - val_loss: 23040.0996 - val_mean_absolute_error: 141.6120\n",
      "Epoch 231/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 19137.3145 - mean_absolute_error: 117.0594 - val_loss: 19342.5293 - val_mean_absolute_error: 126.6769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15263.6133 - mean_absolute_error: 93.9197 - val_loss: 14138.3535 - val_mean_absolute_error: 104.4337\n",
      "Epoch 233/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14991.8496 - mean_absolute_error: 94.4451 - val_loss: 9530.3379 - val_mean_absolute_error: 87.8796\n",
      "Epoch 234/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12417.4727 - mean_absolute_error: 91.4779 - val_loss: 8112.3018 - val_mean_absolute_error: 72.6751\n",
      "Epoch 235/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 18166.2695 - mean_absolute_error: 117.6212 - val_loss: 8776.4023 - val_mean_absolute_error: 82.9685\n",
      "Epoch 236/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17261.6641 - mean_absolute_error: 104.2820 - val_loss: 13820.2676 - val_mean_absolute_error: 103.5673\n",
      "Epoch 237/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14832.9609 - mean_absolute_error: 94.6487 - val_loss: 18521.0488 - val_mean_absolute_error: 123.0495\n",
      "Epoch 238/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13529.6523 - mean_absolute_error: 91.7489 - val_loss: 19533.3027 - val_mean_absolute_error: 127.5146\n",
      "Epoch 239/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 17930.4746 - mean_absolute_error: 112.9123 - val_loss: 17070.3730 - val_mean_absolute_error: 116.2575\n",
      "Epoch 240/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15174.2324 - mean_absolute_error: 96.5636 - val_loss: 13648.1592 - val_mean_absolute_error: 103.0889\n",
      "Epoch 241/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13538.4746 - mean_absolute_error: 92.6987 - val_loss: 10077.2217 - val_mean_absolute_error: 90.6481\n",
      "Epoch 242/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11801.1367 - mean_absolute_error: 84.8087 - val_loss: 8286.3770 - val_mean_absolute_error: 77.8681\n",
      "Epoch 243/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14262.6934 - mean_absolute_error: 104.8071 - val_loss: 8508.1113 - val_mean_absolute_error: 80.6047\n",
      "Epoch 244/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14828.2686 - mean_absolute_error: 96.2295 - val_loss: 10891.9512 - val_mean_absolute_error: 94.1181\n",
      "Epoch 245/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13493.3047 - mean_absolute_error: 91.1147 - val_loss: 15122.6719 - val_mean_absolute_error: 106.9392\n",
      "Epoch 246/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13800.1289 - mean_absolute_error: 89.0383 - val_loss: 16586.2910 - val_mean_absolute_error: 113.8907\n",
      "Epoch 247/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13829.7207 - mean_absolute_error: 96.5388 - val_loss: 15998.8535 - val_mean_absolute_error: 110.9050\n",
      "Epoch 248/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14031.3447 - mean_absolute_error: 90.8125 - val_loss: 13280.8564 - val_mean_absolute_error: 102.0377\n",
      "Epoch 249/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11772.2188 - mean_absolute_error: 85.9254 - val_loss: 9431.5312 - val_mean_absolute_error: 87.3619\n",
      "Epoch 250/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14036.4326 - mean_absolute_error: 94.0943 - val_loss: 8825.9189 - val_mean_absolute_error: 83.4330\n",
      "Epoch 251/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12718.4561 - mean_absolute_error: 94.1255 - val_loss: 9770.9277 - val_mean_absolute_error: 89.1768\n",
      "Epoch 252/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11989.1221 - mean_absolute_error: 85.8822 - val_loss: 10638.3457 - val_mean_absolute_error: 93.1023\n",
      "Epoch 253/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12141.4717 - mean_absolute_error: 84.3061 - val_loss: 10601.0986 - val_mean_absolute_error: 92.9492\n",
      "Epoch 254/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15985.2129 - mean_absolute_error: 99.4426 - val_loss: 12435.8496 - val_mean_absolute_error: 99.4942\n",
      "Epoch 255/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 13114.7178 - mean_absolute_error: 87.4030 - val_loss: 14772.8887 - val_mean_absolute_error: 106.0462\n",
      "Epoch 256/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12947.6670 - mean_absolute_error: 93.3225 - val_loss: 15168.5566 - val_mean_absolute_error: 107.0324\n",
      "Epoch 257/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12354.5205 - mean_absolute_error: 84.4441 - val_loss: 15436.9404 - val_mean_absolute_error: 107.9734\n",
      "Epoch 258/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11486.1768 - mean_absolute_error: 77.2705 - val_loss: 13852.5107 - val_mean_absolute_error: 103.6259\n",
      "Epoch 259/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15221.2549 - mean_absolute_error: 103.6554 - val_loss: 12782.3066 - val_mean_absolute_error: 100.5589\n",
      "Epoch 260/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12612.3896 - mean_absolute_error: 87.5714 - val_loss: 9704.0859 - val_mean_absolute_error: 88.8474\n",
      "Epoch 261/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11697.7422 - mean_absolute_error: 84.3920 - val_loss: 8167.1416 - val_mean_absolute_error: 76.1256\n",
      "Epoch 262/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16598.3789 - mean_absolute_error: 112.5350 - val_loss: 9228.0596 - val_mean_absolute_error: 86.1988\n",
      "Epoch 263/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 18374.4277 - mean_absolute_error: 117.0060 - val_loss: 15764.3379 - val_mean_absolute_error: 109.7309\n",
      "Epoch 264/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13917.2559 - mean_absolute_error: 90.2535 - val_loss: 21226.7363 - val_mean_absolute_error: 134.6064\n",
      "Epoch 265/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17571.7422 - mean_absolute_error: 111.3794 - val_loss: 23257.2383 - val_mean_absolute_error: 142.4735\n",
      "Epoch 266/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 17326.1680 - mean_absolute_error: 111.5766 - val_loss: 22143.3203 - val_mean_absolute_error: 138.2310\n",
      "Epoch 267/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 18959.8828 - mean_absolute_error: 113.4327 - val_loss: 18267.8906 - val_mean_absolute_error: 121.9702\n",
      "Epoch 268/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14608.8301 - mean_absolute_error: 96.0592 - val_loss: 12943.4395 - val_mean_absolute_error: 101.0301\n",
      "Epoch 269/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 11927.6455 - mean_absolute_error: 82.3804 - val_loss: 9091.5703 - val_mean_absolute_error: 85.3625\n",
      "Epoch 270/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12262.8975 - mean_absolute_error: 85.9587 - val_loss: 8136.2178 - val_mean_absolute_error: 75.6149\n",
      "Epoch 271/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15627.3467 - mean_absolute_error: 100.6725 - val_loss: 8494.4102 - val_mean_absolute_error: 80.6884\n",
      "Epoch 272/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 18802.0527 - mean_absolute_error: 123.9643 - val_loss: 13257.9482 - val_mean_absolute_error: 101.9444\n",
      "Epoch 273/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13419.0029 - mean_absolute_error: 90.8378 - val_loss: 18338.0586 - val_mean_absolute_error: 122.3021\n",
      "Epoch 274/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16837.6289 - mean_absolute_error: 107.8296 - val_loss: 20339.2129 - val_mean_absolute_error: 130.9962\n",
      "Epoch 275/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 15575.3066 - mean_absolute_error: 105.6451 - val_loss: 19101.4473 - val_mean_absolute_error: 125.7146\n",
      "Epoch 276/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16398.0117 - mean_absolute_error: 103.2228 - val_loss: 15517.4873 - val_mean_absolute_error: 108.4649\n",
      "Epoch 277/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13967.0605 - mean_absolute_error: 98.6062 - val_loss: 11021.8535 - val_mean_absolute_error: 94.6174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11362.9648 - mean_absolute_error: 83.3241 - val_loss: 8725.8105 - val_mean_absolute_error: 82.7828\n",
      "Epoch 279/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11744.6641 - mean_absolute_error: 84.9624 - val_loss: 8465.3887 - val_mean_absolute_error: 80.4565\n",
      "Epoch 280/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 14043.1738 - mean_absolute_error: 102.6713 - val_loss: 10295.0781 - val_mean_absolute_error: 91.6601\n",
      "Epoch 281/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14310.5781 - mean_absolute_error: 93.4118 - val_loss: 14573.0752 - val_mean_absolute_error: 105.4992\n",
      "Epoch 282/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13577.9092 - mean_absolute_error: 85.3770 - val_loss: 17612.7168 - val_mean_absolute_error: 118.9665\n",
      "Epoch 283/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14488.5371 - mean_absolute_error: 94.4195 - val_loss: 18672.7363 - val_mean_absolute_error: 123.8328\n",
      "Epoch 284/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17357.6055 - mean_absolute_error: 109.5032 - val_loss: 17198.2227 - val_mean_absolute_error: 116.9969\n",
      "Epoch 285/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13693.9180 - mean_absolute_error: 90.8427 - val_loss: 14158.3457 - val_mean_absolute_error: 104.4115\n",
      "Epoch 286/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13343.9951 - mean_absolute_error: 93.5411 - val_loss: 10595.1719 - val_mean_absolute_error: 92.9334\n",
      "Epoch 287/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12536.3643 - mean_absolute_error: 93.2676 - val_loss: 8512.2988 - val_mean_absolute_error: 80.9811\n",
      "Epoch 288/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 17682.4863 - mean_absolute_error: 112.0064 - val_loss: 8978.5078 - val_mean_absolute_error: 84.6775\n",
      "Epoch 289/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14104.2363 - mean_absolute_error: 96.8015 - val_loss: 11129.6465 - val_mean_absolute_error: 95.0213\n",
      "Epoch 290/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12615.5449 - mean_absolute_error: 80.5129 - val_loss: 14222.3047 - val_mean_absolute_error: 104.5741\n",
      "Epoch 291/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11882.2725 - mean_absolute_error: 77.9724 - val_loss: 15624.7051 - val_mean_absolute_error: 109.0811\n",
      "Epoch 292/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13572.8408 - mean_absolute_error: 94.9342 - val_loss: 14630.9893 - val_mean_absolute_error: 105.6317\n",
      "Epoch 293/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11408.6934 - mean_absolute_error: 85.1217 - val_loss: 12185.6309 - val_mean_absolute_error: 98.6665\n",
      "Epoch 294/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14585.6357 - mean_absolute_error: 100.5967 - val_loss: 10561.9482 - val_mean_absolute_error: 92.7982\n",
      "Epoch 295/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12466.2793 - mean_absolute_error: 84.0218 - val_loss: 9407.1123 - val_mean_absolute_error: 87.3228\n",
      "Epoch 296/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13448.2324 - mean_absolute_error: 95.0462 - val_loss: 9275.7393 - val_mean_absolute_error: 86.5735\n",
      "Epoch 297/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12667.1797 - mean_absolute_error: 94.4921 - val_loss: 10616.6377 - val_mean_absolute_error: 93.0227\n",
      "Epoch 298/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11599.0225 - mean_absolute_error: 84.5177 - val_loss: 12520.5859 - val_mean_absolute_error: 99.7170\n",
      "Epoch 299/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11719.1934 - mean_absolute_error: 88.0338 - val_loss: 13714.6299 - val_mean_absolute_error: 103.1920\n",
      "Epoch 300/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13221.0254 - mean_absolute_error: 89.6181 - val_loss: 14366.3779 - val_mean_absolute_error: 104.9343\n",
      "Epoch 301/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12165.8242 - mean_absolute_error: 87.1698 - val_loss: 13239.1582 - val_mean_absolute_error: 101.8540\n",
      "Epoch 302/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11420.9785 - mean_absolute_error: 86.6875 - val_loss: 11631.6455 - val_mean_absolute_error: 96.8141\n",
      "Epoch 303/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12971.4219 - mean_absolute_error: 88.8906 - val_loss: 10610.6758 - val_mean_absolute_error: 92.9995\n",
      "Epoch 304/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11937.2266 - mean_absolute_error: 89.5815 - val_loss: 10416.1816 - val_mean_absolute_error: 92.1924\n",
      "Epoch 305/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12596.1572 - mean_absolute_error: 94.1155 - val_loss: 11622.2637 - val_mean_absolute_error: 96.7794\n",
      "Epoch 306/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13772.1250 - mean_absolute_error: 91.9945 - val_loss: 14200.0127 - val_mean_absolute_error: 104.4869\n",
      "Epoch 307/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13497.4092 - mean_absolute_error: 96.0886 - val_loss: 15035.1904 - val_mean_absolute_error: 106.6181\n",
      "Epoch 308/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11448.1328 - mean_absolute_error: 81.0044 - val_loss: 14133.2471 - val_mean_absolute_error: 104.3064\n",
      "Epoch 309/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12375.4414 - mean_absolute_error: 88.5019 - val_loss: 12775.0908 - val_mean_absolute_error: 100.4792\n",
      "Epoch 310/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10088.1465 - mean_absolute_error: 79.3867 - val_loss: 10696.0938 - val_mean_absolute_error: 93.3448\n",
      "Epoch 311/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12027.1602 - mean_absolute_error: 88.1756 - val_loss: 10263.2363 - val_mean_absolute_error: 91.5385\n",
      "Epoch 312/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10274.5801 - mean_absolute_error: 78.1799 - val_loss: 9342.0752 - val_mean_absolute_error: 86.9995\n",
      "Epoch 313/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10697.2891 - mean_absolute_error: 90.3334 - val_loss: 9840.2852 - val_mean_absolute_error: 89.6003\n",
      "Epoch 314/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10657.5332 - mean_absolute_error: 81.9926 - val_loss: 10790.9609 - val_mean_absolute_error: 93.7204\n",
      "Epoch 315/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13557.4805 - mean_absolute_error: 93.5984 - val_loss: 11373.2295 - val_mean_absolute_error: 95.8977\n",
      "Epoch 316/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12382.4697 - mean_absolute_error: 84.9497 - val_loss: 12726.3770 - val_mean_absolute_error: 100.3219\n",
      "Epoch 317/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9813.9717 - mean_absolute_error: 72.5819 - val_loss: 12553.2344 - val_mean_absolute_error: 99.7934\n",
      "Epoch 318/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12491.9033 - mean_absolute_error: 89.8420 - val_loss: 12233.5879 - val_mean_absolute_error: 98.7920\n",
      "Epoch 319/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11901.8408 - mean_absolute_error: 84.3397 - val_loss: 11352.7393 - val_mean_absolute_error: 95.8217\n",
      "Epoch 320/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11008.7070 - mean_absolute_error: 80.6231 - val_loss: 11005.4346 - val_mean_absolute_error: 94.5447\n",
      "Epoch 321/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 17127.4316 - mean_absolute_error: 115.2684 - val_loss: 12584.6152 - val_mean_absolute_error: 99.8837\n",
      "Epoch 322/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13390.0732 - mean_absolute_error: 92.7258 - val_loss: 14733.2090 - val_mean_absolute_error: 105.8303\n",
      "Epoch 323/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12054.5439 - mean_absolute_error: 84.8864 - val_loss: 15521.5000 - val_mean_absolute_error: 108.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14737.5156 - mean_absolute_error: 95.7140 - val_loss: 13535.8652 - val_mean_absolute_error: 102.6535\n",
      "Epoch 325/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11672.2461 - mean_absolute_error: 87.7568 - val_loss: 11418.1992 - val_mean_absolute_error: 96.0498\n",
      "Epoch 326/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 12547.5391 - mean_absolute_error: 84.6170 - val_loss: 8806.1436 - val_mean_absolute_error: 83.6519\n",
      "Epoch 327/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13738.7373 - mean_absolute_error: 95.9423 - val_loss: 8302.8906 - val_mean_absolute_error: 79.2069\n",
      "Epoch 328/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12285.5088 - mean_absolute_error: 97.7386 - val_loss: 9395.2344 - val_mean_absolute_error: 87.3359\n",
      "Epoch 329/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11627.8828 - mean_absolute_error: 87.2339 - val_loss: 10891.8701 - val_mean_absolute_error: 94.1098\n",
      "Epoch 330/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10308.7109 - mean_absolute_error: 86.6919 - val_loss: 11852.5566 - val_mean_absolute_error: 97.5388\n",
      "Epoch 331/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13117.4834 - mean_absolute_error: 88.1025 - val_loss: 13502.4785 - val_mean_absolute_error: 102.5502\n",
      "Epoch 332/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11737.9014 - mean_absolute_error: 83.6066 - val_loss: 14694.0654 - val_mean_absolute_error: 105.7133\n",
      "Epoch 333/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12481.6035 - mean_absolute_error: 82.9500 - val_loss: 14316.7051 - val_mean_absolute_error: 104.7421\n",
      "Epoch 334/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13654.9961 - mean_absolute_error: 90.5271 - val_loss: 11884.5391 - val_mean_absolute_error: 97.6413\n",
      "Epoch 335/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12802.9043 - mean_absolute_error: 85.5581 - val_loss: 10327.1162 - val_mean_absolute_error: 91.8258\n",
      "Epoch 336/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11651.3926 - mean_absolute_error: 87.5523 - val_loss: 9707.4609 - val_mean_absolute_error: 88.9798\n",
      "Epoch 337/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13400.4414 - mean_absolute_error: 96.5571 - val_loss: 10802.5996 - val_mean_absolute_error: 93.7635\n",
      "Epoch 338/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12160.2812 - mean_absolute_error: 87.3667 - val_loss: 12414.2441 - val_mean_absolute_error: 99.3396\n",
      "Epoch 339/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12708.5820 - mean_absolute_error: 90.2502 - val_loss: 15110.8623 - val_mean_absolute_error: 106.7421\n",
      "Epoch 340/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12836.2158 - mean_absolute_error: 90.3077 - val_loss: 15781.1973 - val_mean_absolute_error: 110.0861\n",
      "Epoch 341/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12618.0547 - mean_absolute_error: 85.7603 - val_loss: 14026.1230 - val_mean_absolute_error: 103.9636\n",
      "Epoch 342/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12479.4395 - mean_absolute_error: 84.9966 - val_loss: 12140.4062 - val_mean_absolute_error: 98.4697\n",
      "Epoch 343/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11405.6543 - mean_absolute_error: 82.2732 - val_loss: 9613.2891 - val_mean_absolute_error: 88.5166\n",
      "Epoch 344/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15532.8535 - mean_absolute_error: 100.6902 - val_loss: 9479.2080 - val_mean_absolute_error: 87.8215\n",
      "Epoch 345/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13887.6758 - mean_absolute_error: 100.8018 - val_loss: 12209.5684 - val_mean_absolute_error: 98.6871\n",
      "Epoch 346/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11175.4219 - mean_absolute_error: 83.9574 - val_loss: 13820.5107 - val_mean_absolute_error: 103.4009\n",
      "Epoch 347/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13978.7119 - mean_absolute_error: 91.8559 - val_loss: 14392.3857 - val_mean_absolute_error: 104.9136\n",
      "Epoch 348/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14878.3672 - mean_absolute_error: 94.1192 - val_loss: 12697.0430 - val_mean_absolute_error: 100.1913\n",
      "Epoch 349/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12188.6211 - mean_absolute_error: 80.2487 - val_loss: 10634.8887 - val_mean_absolute_error: 93.1011\n",
      "Epoch 350/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13056.7021 - mean_absolute_error: 92.9554 - val_loss: 10137.7168 - val_mean_absolute_error: 91.0106\n",
      "Epoch 351/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12029.0000 - mean_absolute_error: 86.5974 - val_loss: 10850.9746 - val_mean_absolute_error: 93.9479\n",
      "Epoch 352/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13148.6660 - mean_absolute_error: 99.6779 - val_loss: 13922.0254 - val_mean_absolute_error: 103.6624\n",
      "Epoch 353/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14637.3633 - mean_absolute_error: 89.8777 - val_loss: 15821.5566 - val_mean_absolute_error: 110.3511\n",
      "Epoch 354/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13585.3867 - mean_absolute_error: 85.8972 - val_loss: 15558.4873 - val_mean_absolute_error: 108.9790\n",
      "Epoch 355/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 12164.3477 - mean_absolute_error: 84.2629 - val_loss: 13798.2236 - val_mean_absolute_error: 103.3204\n",
      "Epoch 356/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13637.9453 - mean_absolute_error: 91.3839 - val_loss: 10447.4971 - val_mean_absolute_error: 92.3383\n",
      "Epoch 357/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10611.2090 - mean_absolute_error: 76.1517 - val_loss: 8078.1587 - val_mean_absolute_error: 76.6402\n",
      "Epoch 358/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10796.4863 - mean_absolute_error: 86.7487 - val_loss: 8006.9492 - val_mean_absolute_error: 70.8950\n",
      "Epoch 359/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 23658.0352 - mean_absolute_error: 121.6471 - val_loss: 11115.2402 - val_mean_absolute_error: 94.9388\n",
      "Epoch 360/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9604.2666 - mean_absolute_error: 68.7006 - val_loss: 17208.6289 - val_mean_absolute_error: 117.2830\n",
      "Epoch 361/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 17169.2422 - mean_absolute_error: 105.7177 - val_loss: 21235.6660 - val_mean_absolute_error: 134.8220\n",
      "Epoch 362/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16982.3516 - mean_absolute_error: 107.1330 - val_loss: 22076.7461 - val_mean_absolute_error: 138.1333\n",
      "Epoch 363/1500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 16713.9336 - mean_absolute_error: 107.8311 - val_loss: 20091.4199 - val_mean_absolute_error: 130.1510\n",
      "Epoch 364/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 16406.4414 - mean_absolute_error: 109.7971 - val_loss: 15402.4502 - val_mean_absolute_error: 108.1876\n",
      "Epoch 365/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12351.3730 - mean_absolute_error: 83.8540 - val_loss: 9981.2891 - val_mean_absolute_error: 90.3183\n",
      "Epoch 366/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12266.7109 - mean_absolute_error: 88.8598 - val_loss: 7924.7671 - val_mean_absolute_error: 69.7453\n",
      "Epoch 367/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11958.8613 - mean_absolute_error: 90.0846 - val_loss: 8185.3037 - val_mean_absolute_error: 74.6141\n",
      "Epoch 368/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 22866.7344 - mean_absolute_error: 128.8401 - val_loss: 9133.6113 - val_mean_absolute_error: 85.9465\n",
      "Epoch 369/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13206.0264 - mean_absolute_error: 94.9222 - val_loss: 13780.1270 - val_mean_absolute_error: 103.2492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13118.8379 - mean_absolute_error: 87.0341 - val_loss: 17678.0508 - val_mean_absolute_error: 119.5290\n",
      "Epoch 371/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15136.1348 - mean_absolute_error: 93.1641 - val_loss: 19394.5176 - val_mean_absolute_error: 127.2128\n",
      "Epoch 372/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15614.1270 - mean_absolute_error: 100.4161 - val_loss: 18847.2930 - val_mean_absolute_error: 124.8326\n",
      "Epoch 373/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15055.5137 - mean_absolute_error: 103.8356 - val_loss: 15637.8232 - val_mean_absolute_error: 109.4656\n",
      "Epoch 374/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12921.3613 - mean_absolute_error: 91.6347 - val_loss: 10974.2646 - val_mean_absolute_error: 94.4116\n",
      "Epoch 375/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10911.2539 - mean_absolute_error: 81.4093 - val_loss: 8358.4766 - val_mean_absolute_error: 80.2692\n",
      "Epoch 376/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17244.5918 - mean_absolute_error: 102.2804 - val_loss: 8599.1758 - val_mean_absolute_error: 82.3649\n",
      "Epoch 377/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13268.3457 - mean_absolute_error: 92.1065 - val_loss: 10867.6514 - val_mean_absolute_error: 94.0078\n",
      "Epoch 378/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10721.4678 - mean_absolute_error: 81.8200 - val_loss: 15032.1436 - val_mean_absolute_error: 106.4683\n",
      "Epoch 379/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14786.4062 - mean_absolute_error: 99.6232 - val_loss: 18003.4102 - val_mean_absolute_error: 121.0539\n",
      "Epoch 380/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15025.0391 - mean_absolute_error: 100.1395 - val_loss: 18049.8359 - val_mean_absolute_error: 121.2697\n",
      "Epoch 381/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14957.4316 - mean_absolute_error: 103.4699 - val_loss: 16064.9795 - val_mean_absolute_error: 111.6976\n",
      "Epoch 382/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15000.3203 - mean_absolute_error: 97.7055 - val_loss: 12921.0752 - val_mean_absolute_error: 100.8116\n",
      "Epoch 383/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13273.2090 - mean_absolute_error: 83.4657 - val_loss: 9577.4268 - val_mean_absolute_error: 88.4001\n",
      "Epoch 384/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12450.2207 - mean_absolute_error: 80.1802 - val_loss: 8351.4854 - val_mean_absolute_error: 80.2740\n",
      "Epoch 385/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14747.6592 - mean_absolute_error: 102.9955 - val_loss: 8967.0166 - val_mean_absolute_error: 84.9772\n",
      "Epoch 386/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13499.8242 - mean_absolute_error: 90.6038 - val_loss: 11360.3438 - val_mean_absolute_error: 95.8098\n",
      "Epoch 387/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16831.8281 - mean_absolute_error: 102.8255 - val_loss: 14882.1934 - val_mean_absolute_error: 106.0828\n",
      "Epoch 388/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12939.1904 - mean_absolute_error: 92.3947 - val_loss: 17425.2500 - val_mean_absolute_error: 118.3867\n",
      "Epoch 389/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14665.7695 - mean_absolute_error: 91.4992 - val_loss: 18633.0566 - val_mean_absolute_error: 123.9207\n",
      "Epoch 390/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 17357.7734 - mean_absolute_error: 103.9933 - val_loss: 17213.6289 - val_mean_absolute_error: 117.3883\n",
      "Epoch 391/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13372.8965 - mean_absolute_error: 86.1191 - val_loss: 14218.8936 - val_mean_absolute_error: 104.3843\n",
      "Epoch 392/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11804.3457 - mean_absolute_error: 80.9843 - val_loss: 11191.9531 - val_mean_absolute_error: 95.2067\n",
      "Epoch 393/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13500.6719 - mean_absolute_error: 93.4661 - val_loss: 9109.8145 - val_mean_absolute_error: 85.8748\n",
      "Epoch 394/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15720.4248 - mean_absolute_error: 103.3579 - val_loss: 9235.2070 - val_mean_absolute_error: 86.6016\n",
      "Epoch 395/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11350.0615 - mean_absolute_error: 79.1009 - val_loss: 10025.6895 - val_mean_absolute_error: 90.5448\n",
      "Epoch 396/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12203.5566 - mean_absolute_error: 93.9372 - val_loss: 12098.4316 - val_mean_absolute_error: 98.2736\n",
      "Epoch 397/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12760.6836 - mean_absolute_error: 83.9709 - val_loss: 14366.3779 - val_mean_absolute_error: 104.7483\n",
      "Epoch 398/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14381.3701 - mean_absolute_error: 95.1251 - val_loss: 15881.1826 - val_mean_absolute_error: 110.8280\n",
      "Epoch 399/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13989.9199 - mean_absolute_error: 94.3921 - val_loss: 15545.4590 - val_mean_absolute_error: 109.0859\n",
      "Epoch 400/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12711.7539 - mean_absolute_error: 80.2619 - val_loss: 14027.8750 - val_mean_absolute_error: 103.8561\n",
      "Epoch 401/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13572.4150 - mean_absolute_error: 89.6878 - val_loss: 11131.7920 - val_mean_absolute_error: 94.9794\n",
      "Epoch 402/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13224.0449 - mean_absolute_error: 89.1836 - val_loss: 9384.6045 - val_mean_absolute_error: 87.4412\n",
      "Epoch 403/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11860.9131 - mean_absolute_error: 87.4314 - val_loss: 8748.1855 - val_mean_absolute_error: 83.6180\n",
      "Epoch 404/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14159.3203 - mean_absolute_error: 99.7212 - val_loss: 9230.7988 - val_mean_absolute_error: 86.6083\n",
      "Epoch 405/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10581.2822 - mean_absolute_error: 77.0872 - val_loss: 9697.4824 - val_mean_absolute_error: 89.0301\n",
      "Epoch 406/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15146.1494 - mean_absolute_error: 108.0648 - val_loss: 12607.7734 - val_mean_absolute_error: 99.8377\n",
      "Epoch 407/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15151.0654 - mean_absolute_error: 101.9327 - val_loss: 14797.7598 - val_mean_absolute_error: 105.8175\n",
      "Epoch 408/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13261.6748 - mean_absolute_error: 86.5237 - val_loss: 15345.9580 - val_mean_absolute_error: 108.0750\n",
      "Epoch 409/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13832.2812 - mean_absolute_error: 92.6441 - val_loss: 13579.1768 - val_mean_absolute_error: 102.6236\n",
      "Epoch 410/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11876.8867 - mean_absolute_error: 81.3766 - val_loss: 11395.0439 - val_mean_absolute_error: 95.9083\n",
      "Epoch 411/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13252.4590 - mean_absolute_error: 91.5797 - val_loss: 9979.0420 - val_mean_absolute_error: 90.3490\n",
      "Epoch 412/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15112.3340 - mean_absolute_error: 103.3007 - val_loss: 11322.4766 - val_mean_absolute_error: 95.6520\n",
      "Epoch 413/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14488.3223 - mean_absolute_error: 92.4747 - val_loss: 12515.9434 - val_mean_absolute_error: 99.5462\n",
      "Epoch 414/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11951.0176 - mean_absolute_error: 78.4925 - val_loss: 13739.2471 - val_mean_absolute_error: 103.0502\n",
      "Epoch 415/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12414.5713 - mean_absolute_error: 83.8239 - val_loss: 13456.7812 - val_mean_absolute_error: 102.2736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14885.3340 - mean_absolute_error: 100.7938 - val_loss: 12997.0000 - val_mean_absolute_error: 100.9685\n",
      "Epoch 417/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13574.3467 - mean_absolute_error: 90.9150 - val_loss: 11207.4922 - val_mean_absolute_error: 95.2401\n",
      "Epoch 418/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12641.9814 - mean_absolute_error: 89.3998 - val_loss: 10080.2402 - val_mean_absolute_error: 90.8024\n",
      "Epoch 419/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11120.4717 - mean_absolute_error: 82.7116 - val_loss: 8682.7051 - val_mean_absolute_error: 83.2589\n",
      "Epoch 420/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15201.8398 - mean_absolute_error: 103.8230 - val_loss: 9208.4609 - val_mean_absolute_error: 86.5329\n",
      "Epoch 421/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10053.2217 - mean_absolute_error: 77.8484 - val_loss: 10292.3750 - val_mean_absolute_error: 91.7108\n",
      "Epoch 422/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11472.9209 - mean_absolute_error: 78.9260 - val_loss: 11377.9541 - val_mean_absolute_error: 95.8384\n",
      "Epoch 423/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11402.9365 - mean_absolute_error: 82.6602 - val_loss: 11510.9023 - val_mean_absolute_error: 96.2966\n",
      "Epoch 424/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9932.1670 - mean_absolute_error: 78.6652 - val_loss: 11469.3906 - val_mean_absolute_error: 96.1529\n",
      "Epoch 425/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11943.7969 - mean_absolute_error: 87.3285 - val_loss: 11998.8281 - val_mean_absolute_error: 97.9126\n",
      "Epoch 426/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11105.5645 - mean_absolute_error: 78.4719 - val_loss: 11626.1475 - val_mean_absolute_error: 96.6836\n",
      "Epoch 427/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12930.8594 - mean_absolute_error: 92.4260 - val_loss: 10473.4717 - val_mean_absolute_error: 92.4539\n",
      "Epoch 428/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12467.4824 - mean_absolute_error: 90.1830 - val_loss: 10462.4834 - val_mean_absolute_error: 92.4102\n",
      "Epoch 429/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11077.5527 - mean_absolute_error: 81.8273 - val_loss: 9542.3916 - val_mean_absolute_error: 88.3088\n",
      "Epoch 430/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12255.0156 - mean_absolute_error: 93.5798 - val_loss: 9749.0078 - val_mean_absolute_error: 89.3105\n",
      "Epoch 431/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10830.7305 - mean_absolute_error: 85.3642 - val_loss: 10122.1787 - val_mean_absolute_error: 90.9916\n",
      "Epoch 432/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13565.2227 - mean_absolute_error: 99.5814 - val_loss: 11254.2979 - val_mean_absolute_error: 95.3943\n",
      "Epoch 433/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8850.3223 - mean_absolute_error: 68.1350 - val_loss: 11556.8154 - val_mean_absolute_error: 96.4417\n",
      "Epoch 434/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12331.0596 - mean_absolute_error: 86.8991 - val_loss: 12895.8408 - val_mean_absolute_error: 100.6408\n",
      "Epoch 435/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12342.0156 - mean_absolute_error: 86.4284 - val_loss: 14051.3252 - val_mean_absolute_error: 103.8366\n",
      "Epoch 436/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12878.3545 - mean_absolute_error: 91.7461 - val_loss: 14656.9736 - val_mean_absolute_error: 105.3942\n",
      "Epoch 437/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11631.7051 - mean_absolute_error: 86.2376 - val_loss: 12935.0557 - val_mean_absolute_error: 100.7499\n",
      "Epoch 438/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13116.8193 - mean_absolute_error: 90.1180 - val_loss: 10699.7236 - val_mean_absolute_error: 93.3436\n",
      "Epoch 439/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11306.5518 - mean_absolute_error: 82.6006 - val_loss: 9702.0000 - val_mean_absolute_error: 89.0997\n",
      "Epoch 440/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10088.5020 - mean_absolute_error: 74.7118 - val_loss: 8988.3027 - val_mean_absolute_error: 85.3308\n",
      "Epoch 441/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12393.3535 - mean_absolute_error: 86.7999 - val_loss: 9324.9736 - val_mean_absolute_error: 87.2204\n",
      "Epoch 442/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11082.9043 - mean_absolute_error: 85.6931 - val_loss: 10655.7861 - val_mean_absolute_error: 93.1707\n",
      "Epoch 443/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13124.9648 - mean_absolute_error: 93.2200 - val_loss: 13234.1826 - val_mean_absolute_error: 101.5891\n",
      "Epoch 444/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11307.8535 - mean_absolute_error: 79.0685 - val_loss: 15340.3652 - val_mean_absolute_error: 108.2254\n",
      "Epoch 445/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 13362.7666 - mean_absolute_error: 89.4158 - val_loss: 17273.0586 - val_mean_absolute_error: 117.8906\n",
      "Epoch 446/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14249.6270 - mean_absolute_error: 94.2411 - val_loss: 17493.7598 - val_mean_absolute_error: 118.9324\n",
      "Epoch 447/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13584.3428 - mean_absolute_error: 90.7735 - val_loss: 15340.8105 - val_mean_absolute_error: 108.2454\n",
      "Epoch 448/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15645.0322 - mean_absolute_error: 102.0085 - val_loss: 12531.2441 - val_mean_absolute_error: 99.5262\n",
      "Epoch 449/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 11815.2129 - mean_absolute_error: 85.3080 - val_loss: 9691.7793 - val_mean_absolute_error: 89.0673\n",
      "Epoch 450/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12575.2500 - mean_absolute_error: 89.2494 - val_loss: 8257.7510 - val_mean_absolute_error: 80.1338\n",
      "Epoch 451/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15012.5693 - mean_absolute_error: 102.7180 - val_loss: 9435.6074 - val_mean_absolute_error: 87.8200\n",
      "Epoch 452/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11929.5430 - mean_absolute_error: 91.2649 - val_loss: 12430.3516 - val_mean_absolute_error: 99.2086\n",
      "Epoch 453/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11918.2119 - mean_absolute_error: 78.2809 - val_loss: 15079.4355 - val_mean_absolute_error: 106.8936\n",
      "Epoch 454/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12683.1143 - mean_absolute_error: 86.6469 - val_loss: 15892.3203 - val_mean_absolute_error: 111.1606\n",
      "Epoch 455/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13056.0557 - mean_absolute_error: 92.4681 - val_loss: 14325.6221 - val_mean_absolute_error: 104.4853\n",
      "Epoch 456/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12101.3428 - mean_absolute_error: 86.3453 - val_loss: 11116.6191 - val_mean_absolute_error: 94.8750\n",
      "Epoch 457/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10274.7520 - mean_absolute_error: 77.3383 - val_loss: 9127.2246 - val_mean_absolute_error: 86.2058\n",
      "Epoch 458/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10847.7129 - mean_absolute_error: 88.3770 - val_loss: 8915.9512 - val_mean_absolute_error: 84.9871\n",
      "Epoch 459/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14107.7539 - mean_absolute_error: 100.2479 - val_loss: 10472.9707 - val_mean_absolute_error: 92.4464\n",
      "Epoch 460/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9699.1660 - mean_absolute_error: 80.2518 - val_loss: 12252.6035 - val_mean_absolute_error: 98.6428\n",
      "Epoch 461/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8931.9785 - mean_absolute_error: 72.7120 - val_loss: 12734.0645 - val_mean_absolute_error: 100.0950\n",
      "Epoch 462/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 750us/step - loss: 13049.8057 - mean_absolute_error: 92.2864 - val_loss: 13834.7871 - val_mean_absolute_error: 103.1751\n",
      "Epoch 463/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11213.7012 - mean_absolute_error: 83.4599 - val_loss: 14002.7090 - val_mean_absolute_error: 103.6164\n",
      "Epoch 464/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12079.9922 - mean_absolute_error: 84.9521 - val_loss: 12892.6074 - val_mean_absolute_error: 100.5488\n",
      "Epoch 465/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10312.4980 - mean_absolute_error: 76.7094 - val_loss: 11559.6553 - val_mean_absolute_error: 96.3965\n",
      "Epoch 466/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12978.8906 - mean_absolute_error: 83.5438 - val_loss: 10419.5938 - val_mean_absolute_error: 92.2313\n",
      "Epoch 467/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14318.0605 - mean_absolute_error: 105.9549 - val_loss: 10742.5498 - val_mean_absolute_error: 93.4856\n",
      "Epoch 468/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10352.9346 - mean_absolute_error: 83.0648 - val_loss: 9877.9082 - val_mean_absolute_error: 89.9475\n",
      "Epoch 469/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8624.6465 - mean_absolute_error: 72.4928 - val_loss: 9566.3027 - val_mean_absolute_error: 88.5069\n",
      "Epoch 470/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13493.6484 - mean_absolute_error: 93.3524 - val_loss: 11160.7832 - val_mean_absolute_error: 95.0116\n",
      "Epoch 471/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14549.3926 - mean_absolute_error: 101.0032 - val_loss: 14230.1875 - val_mean_absolute_error: 104.1808\n",
      "Epoch 472/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11306.1592 - mean_absolute_error: 83.8488 - val_loss: 15878.0811 - val_mean_absolute_error: 111.2043\n",
      "Epoch 473/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 13170.6387 - mean_absolute_error: 81.5638 - val_loss: 15675.6748 - val_mean_absolute_error: 110.1730\n",
      "Epoch 474/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13189.2480 - mean_absolute_error: 84.0822 - val_loss: 13676.3457 - val_mean_absolute_error: 102.7078\n",
      "Epoch 475/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12957.1982 - mean_absolute_error: 85.5688 - val_loss: 10937.8828 - val_mean_absolute_error: 94.2049\n",
      "Epoch 476/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12063.9697 - mean_absolute_error: 86.0776 - val_loss: 8467.5703 - val_mean_absolute_error: 82.1499\n",
      "Epoch 477/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12882.5645 - mean_absolute_error: 97.1994 - val_loss: 8390.5674 - val_mean_absolute_error: 81.5794\n",
      "Epoch 478/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15457.0781 - mean_absolute_error: 107.2116 - val_loss: 10184.4766 - val_mean_absolute_error: 91.2737\n",
      "Epoch 479/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10505.6133 - mean_absolute_error: 78.2704 - val_loss: 12591.5684 - val_mean_absolute_error: 99.6229\n",
      "Epoch 480/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11862.4902 - mean_absolute_error: 81.8392 - val_loss: 14182.9004 - val_mean_absolute_error: 104.0308\n",
      "Epoch 481/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11105.3730 - mean_absolute_error: 80.3265 - val_loss: 13881.4004 - val_mean_absolute_error: 103.2383\n",
      "Epoch 482/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11777.1133 - mean_absolute_error: 85.3229 - val_loss: 13312.1719 - val_mean_absolute_error: 101.6899\n",
      "Epoch 483/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13324.2285 - mean_absolute_error: 91.4284 - val_loss: 12540.8467 - val_mean_absolute_error: 99.4647\n",
      "Epoch 484/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10320.6426 - mean_absolute_error: 71.1689 - val_loss: 10541.5449 - val_mean_absolute_error: 92.7070\n",
      "Epoch 485/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11077.1152 - mean_absolute_error: 89.0921 - val_loss: 10076.1719 - val_mean_absolute_error: 90.8211\n",
      "Epoch 486/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11386.9805 - mean_absolute_error: 86.2137 - val_loss: 10969.4219 - val_mean_absolute_error: 94.3095\n",
      "Epoch 487/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11407.9492 - mean_absolute_error: 81.7037 - val_loss: 12091.5840 - val_mean_absolute_error: 98.0816\n",
      "Epoch 488/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12042.3379 - mean_absolute_error: 86.1135 - val_loss: 11242.7129 - val_mean_absolute_error: 95.2759\n",
      "Epoch 489/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11546.7920 - mean_absolute_error: 86.2360 - val_loss: 10199.0879 - val_mean_absolute_error: 91.3352\n",
      "Epoch 490/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12717.1270 - mean_absolute_error: 87.4270 - val_loss: 10310.0078 - val_mean_absolute_error: 91.7879\n",
      "Epoch 491/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12102.0312 - mean_absolute_error: 92.1743 - val_loss: 11151.6172 - val_mean_absolute_error: 94.9547\n",
      "Epoch 492/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11507.4434 - mean_absolute_error: 84.4672 - val_loss: 12136.9395 - val_mean_absolute_error: 98.2118\n",
      "Epoch 493/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13146.2695 - mean_absolute_error: 92.5211 - val_loss: 12754.9482 - val_mean_absolute_error: 100.0716\n",
      "Epoch 494/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12062.5576 - mean_absolute_error: 87.7375 - val_loss: 12500.3281 - val_mean_absolute_error: 99.3157\n",
      "Epoch 495/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13678.8027 - mean_absolute_error: 85.7005 - val_loss: 13029.9688 - val_mean_absolute_error: 100.8576\n",
      "Epoch 496/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13084.9326 - mean_absolute_error: 89.2940 - val_loss: 12456.8643 - val_mean_absolute_error: 99.1791\n",
      "Epoch 497/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11616.6963 - mean_absolute_error: 80.7607 - val_loss: 11566.4541 - val_mean_absolute_error: 96.3633\n",
      "Epoch 498/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12868.0410 - mean_absolute_error: 90.7188 - val_loss: 10035.4980 - val_mean_absolute_error: 90.6535\n",
      "Epoch 499/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13545.0078 - mean_absolute_error: 88.0739 - val_loss: 10034.3096 - val_mean_absolute_error: 90.6490\n",
      "Epoch 500/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9789.2480 - mean_absolute_error: 82.9734 - val_loss: 10436.0615 - val_mean_absolute_error: 92.2873\n",
      "Epoch 501/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13015.7256 - mean_absolute_error: 97.8507 - val_loss: 11452.9629 - val_mean_absolute_error: 95.9734\n",
      "Epoch 502/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10185.3262 - mean_absolute_error: 78.7264 - val_loss: 12691.5762 - val_mean_absolute_error: 99.8540\n",
      "Epoch 503/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11453.5518 - mean_absolute_error: 81.6353 - val_loss: 13464.7637 - val_mean_absolute_error: 102.0391\n",
      "Epoch 504/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14450.3984 - mean_absolute_error: 97.1911 - val_loss: 14066.1611 - val_mean_absolute_error: 103.6382\n",
      "Epoch 505/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11684.5039 - mean_absolute_error: 83.6331 - val_loss: 12575.2520 - val_mean_absolute_error: 99.5026\n",
      "Epoch 506/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12377.3828 - mean_absolute_error: 87.7821 - val_loss: 12216.1045 - val_mean_absolute_error: 98.4131\n",
      "Epoch 507/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12893.7480 - mean_absolute_error: 86.6583 - val_loss: 12923.7266 - val_mean_absolute_error: 100.5063\n",
      "Epoch 508/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 688us/step - loss: 10380.0371 - mean_absolute_error: 74.1302 - val_loss: 12522.4551 - val_mean_absolute_error: 99.3299\n",
      "Epoch 509/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10906.5312 - mean_absolute_error: 75.4350 - val_loss: 11801.4219 - val_mean_absolute_error: 97.0959\n",
      "Epoch 510/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12516.8193 - mean_absolute_error: 89.5611 - val_loss: 10875.1465 - val_mean_absolute_error: 93.9310\n",
      "Epoch 511/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11143.0859 - mean_absolute_error: 82.7082 - val_loss: 10718.2812 - val_mean_absolute_error: 93.3518\n",
      "Epoch 512/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10688.7080 - mean_absolute_error: 80.8268 - val_loss: 10462.2090 - val_mean_absolute_error: 92.3775\n",
      "Epoch 513/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11944.2617 - mean_absolute_error: 95.9393 - val_loss: 11499.7949 - val_mean_absolute_error: 96.0954\n",
      "Epoch 514/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11661.3828 - mean_absolute_error: 83.6447 - val_loss: 13272.4658 - val_mean_absolute_error: 101.4565\n",
      "Epoch 515/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13292.3184 - mean_absolute_error: 85.9268 - val_loss: 14846.6562 - val_mean_absolute_error: 106.1136\n",
      "Epoch 516/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10924.8545 - mean_absolute_error: 83.2589 - val_loss: 14652.6973 - val_mean_absolute_error: 105.0753\n",
      "Epoch 517/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13173.7695 - mean_absolute_error: 97.6127 - val_loss: 12770.6035 - val_mean_absolute_error: 100.0165\n",
      "Epoch 518/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12312.2656 - mean_absolute_error: 86.1491 - val_loss: 10740.3174 - val_mean_absolute_error: 93.4192\n",
      "Epoch 519/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11338.5762 - mean_absolute_error: 87.8770 - val_loss: 9313.1621 - val_mean_absolute_error: 87.3965\n",
      "Epoch 520/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12402.1123 - mean_absolute_error: 88.0220 - val_loss: 9180.9854 - val_mean_absolute_error: 86.7364\n",
      "Epoch 521/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14546.1426 - mean_absolute_error: 104.9169 - val_loss: 11109.2080 - val_mean_absolute_error: 94.7349\n",
      "Epoch 522/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11332.3906 - mean_absolute_error: 80.0122 - val_loss: 12991.0957 - val_mean_absolute_error: 100.6270\n",
      "Epoch 523/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11986.0586 - mean_absolute_error: 87.1906 - val_loss: 14918.8311 - val_mean_absolute_error: 106.5957\n",
      "Epoch 524/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12595.5000 - mean_absolute_error: 84.8566 - val_loss: 15559.7598 - val_mean_absolute_error: 109.9713\n",
      "Epoch 525/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14860.4883 - mean_absolute_error: 97.8497 - val_loss: 14323.4844 - val_mean_absolute_error: 104.1746\n",
      "Epoch 526/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12279.1445 - mean_absolute_error: 82.6015 - val_loss: 12284.1064 - val_mean_absolute_error: 98.5215\n",
      "Epoch 527/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9342.1533 - mean_absolute_error: 69.4314 - val_loss: 10118.5625 - val_mean_absolute_error: 90.9940\n",
      "Epoch 528/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12140.8877 - mean_absolute_error: 81.3131 - val_loss: 8843.9395 - val_mean_absolute_error: 84.9774\n",
      "Epoch 529/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11601.5986 - mean_absolute_error: 95.0520 - val_loss: 9316.4873 - val_mean_absolute_error: 87.4427\n",
      "Epoch 530/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12051.8008 - mean_absolute_error: 85.8632 - val_loss: 11280.8174 - val_mean_absolute_error: 95.2933\n",
      "Epoch 531/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12563.1855 - mean_absolute_error: 90.6541 - val_loss: 13794.7158 - val_mean_absolute_error: 102.7605\n",
      "Epoch 532/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11328.2656 - mean_absolute_error: 78.8993 - val_loss: 15503.3691 - val_mean_absolute_error: 109.7900\n",
      "Epoch 533/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10660.1328 - mean_absolute_error: 75.4634 - val_loss: 15117.9863 - val_mean_absolute_error: 107.7991\n",
      "Epoch 534/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14281.3496 - mean_absolute_error: 102.1731 - val_loss: 12855.1973 - val_mean_absolute_error: 100.1523\n",
      "Epoch 535/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10747.7588 - mean_absolute_error: 81.2441 - val_loss: 11480.0723 - val_mean_absolute_error: 95.9338\n",
      "Epoch 536/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11059.4424 - mean_absolute_error: 81.0094 - val_loss: 10641.9170 - val_mean_absolute_error: 93.0072\n",
      "Epoch 537/1500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 12533.3320 - mean_absolute_error: 89.5612 - val_loss: 11107.7988 - val_mean_absolute_error: 94.6692\n",
      "Epoch 538/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12257.2812 - mean_absolute_error: 91.0457 - val_loss: 12697.6738 - val_mean_absolute_error: 99.6816\n",
      "Epoch 539/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10368.8848 - mean_absolute_error: 71.8182 - val_loss: 13269.3779 - val_mean_absolute_error: 101.2937\n",
      "Epoch 540/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11934.0078 - mean_absolute_error: 83.7722 - val_loss: 12653.4268 - val_mean_absolute_error: 99.5484\n",
      "Epoch 541/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11781.2852 - mean_absolute_error: 83.8568 - val_loss: 11122.6895 - val_mean_absolute_error: 94.7131\n",
      "Epoch 542/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11725.3730 - mean_absolute_error: 78.2936 - val_loss: 9889.5215 - val_mean_absolute_error: 90.0432\n",
      "Epoch 543/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11904.0986 - mean_absolute_error: 91.2049 - val_loss: 9526.5254 - val_mean_absolute_error: 88.4607\n",
      "Epoch 544/1500\n",
      "16/16 [==============================] - 0s 563us/step - loss: 8364.9707 - mean_absolute_error: 71.6356 - val_loss: 8937.0957 - val_mean_absolute_error: 85.5863\n",
      "Epoch 545/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13699.8652 - mean_absolute_error: 99.5754 - val_loss: 10562.2246 - val_mean_absolute_error: 92.6947\n",
      "Epoch 546/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12973.2812 - mean_absolute_error: 91.8003 - val_loss: 12743.4639 - val_mean_absolute_error: 99.7620\n",
      "Epoch 547/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11742.0791 - mean_absolute_error: 78.2567 - val_loss: 15052.7344 - val_mean_absolute_error: 107.6390\n",
      "Epoch 548/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13055.3770 - mean_absolute_error: 87.8838 - val_loss: 15025.4902 - val_mean_absolute_error: 107.5143\n",
      "Epoch 549/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11034.3164 - mean_absolute_error: 80.6073 - val_loss: 13415.1621 - val_mean_absolute_error: 101.6016\n",
      "Epoch 550/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10244.7959 - mean_absolute_error: 74.9839 - val_loss: 10829.3887 - val_mean_absolute_error: 93.6430\n",
      "Epoch 551/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9130.3066 - mean_absolute_error: 70.5110 - val_loss: 9071.3125 - val_mean_absolute_error: 86.3154\n",
      "Epoch 552/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12258.6611 - mean_absolute_error: 88.1029 - val_loss: 9697.6680 - val_mean_absolute_error: 89.2260\n",
      "Epoch 553/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14543.3672 - mean_absolute_error: 96.1215 - val_loss: 11061.1309 - val_mean_absolute_error: 94.4483\n",
      "Epoch 554/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 688us/step - loss: 10316.6553 - mean_absolute_error: 79.9905 - val_loss: 12076.4482 - val_mean_absolute_error: 97.7318\n",
      "Epoch 555/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13669.3379 - mean_absolute_error: 97.0751 - val_loss: 12642.4365 - val_mean_absolute_error: 99.4049\n",
      "Epoch 556/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10922.5625 - mean_absolute_error: 80.8906 - val_loss: 12106.2129 - val_mean_absolute_error: 97.8076\n",
      "Epoch 557/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13198.1650 - mean_absolute_error: 98.6675 - val_loss: 12348.9189 - val_mean_absolute_error: 98.5325\n",
      "Epoch 558/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 13779.0059 - mean_absolute_error: 89.9131 - val_loss: 12993.5781 - val_mean_absolute_error: 100.3794\n",
      "Epoch 559/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11553.5996 - mean_absolute_error: 78.6590 - val_loss: 12133.5547 - val_mean_absolute_error: 97.8697\n",
      "Epoch 560/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11574.1934 - mean_absolute_error: 80.9836 - val_loss: 10684.0811 - val_mean_absolute_error: 93.0873\n",
      "Epoch 561/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10128.8477 - mean_absolute_error: 72.9544 - val_loss: 8154.0454 - val_mean_absolute_error: 81.0528\n",
      "Epoch 562/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12951.5859 - mean_absolute_error: 88.6793 - val_loss: 8074.7578 - val_mean_absolute_error: 80.5028\n",
      "Epoch 563/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10746.9082 - mean_absolute_error: 83.5970 - val_loss: 10277.8789 - val_mean_absolute_error: 91.5641\n",
      "Epoch 564/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10765.5381 - mean_absolute_error: 83.8015 - val_loss: 12752.5684 - val_mean_absolute_error: 99.6516\n",
      "Epoch 565/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9753.5264 - mean_absolute_error: 76.8007 - val_loss: 14497.2891 - val_mean_absolute_error: 104.9519\n",
      "Epoch 566/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 11607.3584 - mean_absolute_error: 80.8995 - val_loss: 14427.5293 - val_mean_absolute_error: 104.5921\n",
      "Epoch 567/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11238.0254 - mean_absolute_error: 84.6072 - val_loss: 13155.1816 - val_mean_absolute_error: 100.7441\n",
      "Epoch 568/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11321.0732 - mean_absolute_error: 84.8172 - val_loss: 11314.4121 - val_mean_absolute_error: 95.2159\n",
      "Epoch 569/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11784.7578 - mean_absolute_error: 88.6966 - val_loss: 10537.1406 - val_mean_absolute_error: 92.5126\n",
      "Epoch 570/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 11693.5068 - mean_absolute_error: 85.5910 - val_loss: 12053.7715 - val_mean_absolute_error: 97.5353\n",
      "Epoch 571/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 10163.6816 - mean_absolute_error: 67.3759 - val_loss: 12155.8184 - val_mean_absolute_error: 97.8351\n",
      "Epoch 572/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10447.0723 - mean_absolute_error: 78.8003 - val_loss: 11447.3877 - val_mean_absolute_error: 95.6197\n",
      "Epoch 573/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10054.7305 - mean_absolute_error: 75.2794 - val_loss: 11408.4736 - val_mean_absolute_error: 95.4956\n",
      "Epoch 574/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13714.2754 - mean_absolute_error: 100.0536 - val_loss: 11918.2129 - val_mean_absolute_error: 97.0970\n",
      "Epoch 575/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10523.7920 - mean_absolute_error: 83.5716 - val_loss: 11716.5029 - val_mean_absolute_error: 96.4530\n",
      "Epoch 576/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9246.8330 - mean_absolute_error: 77.7969 - val_loss: 12157.2266 - val_mean_absolute_error: 97.7826\n",
      "Epoch 577/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 12865.4668 - mean_absolute_error: 93.3304 - val_loss: 12116.1660 - val_mean_absolute_error: 97.6483\n",
      "Epoch 578/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12447.7832 - mean_absolute_error: 83.0385 - val_loss: 12562.6924 - val_mean_absolute_error: 98.9478\n",
      "Epoch 579/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10541.0859 - mean_absolute_error: 84.8660 - val_loss: 12803.3389 - val_mean_absolute_error: 99.6177\n",
      "Epoch 580/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11529.1211 - mean_absolute_error: 87.7330 - val_loss: 13112.5596 - val_mean_absolute_error: 100.4659\n",
      "Epoch 581/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10210.5254 - mean_absolute_error: 74.5615 - val_loss: 11431.8301 - val_mean_absolute_error: 95.4774\n",
      "Epoch 582/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9382.7744 - mean_absolute_error: 69.7855 - val_loss: 8954.6582 - val_mean_absolute_error: 85.8844\n",
      "Epoch 583/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15875.7852 - mean_absolute_error: 99.2541 - val_loss: 8906.3682 - val_mean_absolute_error: 85.6546\n",
      "Epoch 584/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11473.4082 - mean_absolute_error: 89.5427 - val_loss: 10207.1191 - val_mean_absolute_error: 91.2068\n",
      "Epoch 585/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11036.2275 - mean_absolute_error: 85.9153 - val_loss: 13371.4004 - val_mean_absolute_error: 101.1493\n",
      "Epoch 586/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11102.0000 - mean_absolute_error: 76.0728 - val_loss: 16148.0840 - val_mean_absolute_error: 113.7891\n",
      "Epoch 587/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12661.8281 - mean_absolute_error: 83.2802 - val_loss: 16198.6094 - val_mean_absolute_error: 114.0368\n",
      "Epoch 588/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11208.2666 - mean_absolute_error: 78.3306 - val_loss: 13983.9951 - val_mean_absolute_error: 102.7019\n",
      "Epoch 589/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12680.9766 - mean_absolute_error: 86.9294 - val_loss: 10243.8223 - val_mean_absolute_error: 91.3012\n",
      "Epoch 590/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11508.7285 - mean_absolute_error: 88.7477 - val_loss: 8561.0742 - val_mean_absolute_error: 83.9461\n",
      "Epoch 591/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13833.0088 - mean_absolute_error: 99.2723 - val_loss: 9065.4561 - val_mean_absolute_error: 86.4457\n",
      "Epoch 592/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9516.2988 - mean_absolute_error: 75.8489 - val_loss: 10138.0928 - val_mean_absolute_error: 90.9234\n",
      "Epoch 593/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9749.6309 - mean_absolute_error: 76.8130 - val_loss: 12499.0527 - val_mean_absolute_error: 98.6170\n",
      "Epoch 594/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10499.9053 - mean_absolute_error: 81.7056 - val_loss: 13937.0918 - val_mean_absolute_error: 102.5599\n",
      "Epoch 595/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 13171.0977 - mean_absolute_error: 89.3020 - val_loss: 13871.3047 - val_mean_absolute_error: 102.2898\n",
      "Epoch 596/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12450.2598 - mean_absolute_error: 83.4735 - val_loss: 11482.1357 - val_mean_absolute_error: 95.4505\n",
      "Epoch 597/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11754.6025 - mean_absolute_error: 91.4396 - val_loss: 9829.7568 - val_mean_absolute_error: 89.6690\n",
      "Epoch 598/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9524.1504 - mean_absolute_error: 82.0655 - val_loss: 8542.5918 - val_mean_absolute_error: 83.9227\n",
      "Epoch 599/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 15271.0479 - mean_absolute_error: 110.8264 - val_loss: 11260.3857 - val_mean_absolute_error: 94.7003\n",
      "Epoch 600/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14073.4238 - mean_absolute_error: 100.8719 - val_loss: 14946.4082 - val_mean_absolute_error: 108.1607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 601/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11778.2539 - mean_absolute_error: 79.7675 - val_loss: 17244.8320 - val_mean_absolute_error: 119.1901\n",
      "Epoch 602/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11326.9180 - mean_absolute_error: 79.9433 - val_loss: 16975.1445 - val_mean_absolute_error: 117.9642\n",
      "Epoch 603/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11831.4600 - mean_absolute_error: 84.3767 - val_loss: 14137.3809 - val_mean_absolute_error: 103.8865\n",
      "Epoch 604/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11092.6953 - mean_absolute_error: 80.2556 - val_loss: 10020.0820 - val_mean_absolute_error: 90.3966\n",
      "Epoch 605/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9525.2266 - mean_absolute_error: 75.5042 - val_loss: 7231.3032 - val_mean_absolute_error: 75.3049\n",
      "Epoch 606/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9361.3164 - mean_absolute_error: 73.9946 - val_loss: 6923.9429 - val_mean_absolute_error: 71.8555\n",
      "Epoch 607/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11135.7510 - mean_absolute_error: 85.9477 - val_loss: 8023.1494 - val_mean_absolute_error: 81.1950\n",
      "Epoch 608/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13745.4629 - mean_absolute_error: 101.0353 - val_loss: 14530.0215 - val_mean_absolute_error: 106.0947\n",
      "Epoch 609/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10879.3486 - mean_absolute_error: 76.5249 - val_loss: 19728.9258 - val_mean_absolute_error: 129.8586\n",
      "Epoch 610/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 16742.2930 - mean_absolute_error: 104.9735 - val_loss: 21416.8105 - val_mean_absolute_error: 136.5440\n",
      "Epoch 611/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 15402.3115 - mean_absolute_error: 106.5917 - val_loss: 19623.6797 - val_mean_absolute_error: 129.4169\n",
      "Epoch 612/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12747.0508 - mean_absolute_error: 88.8042 - val_loss: 15413.1064 - val_mean_absolute_error: 110.3836\n",
      "Epoch 613/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12057.0059 - mean_absolute_error: 83.1719 - val_loss: 9457.6582 - val_mean_absolute_error: 88.2305\n",
      "Epoch 614/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12125.2910 - mean_absolute_error: 99.2629 - val_loss: 8232.6895 - val_mean_absolute_error: 82.3377\n",
      "Epoch 615/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13534.0830 - mean_absolute_error: 88.5030 - val_loss: 8054.6611 - val_mean_absolute_error: 81.3380\n",
      "Epoch 616/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15451.3535 - mean_absolute_error: 110.3908 - val_loss: 12359.6465 - val_mean_absolute_error: 97.9727\n",
      "Epoch 617/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9400.1582 - mean_absolute_error: 75.4588 - val_loss: 16101.3418 - val_mean_absolute_error: 114.1258\n",
      "Epoch 618/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11258.2969 - mean_absolute_error: 81.3675 - val_loss: 17623.7812 - val_mean_absolute_error: 121.1795\n",
      "Epoch 619/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13354.4375 - mean_absolute_error: 93.0201 - val_loss: 16619.4551 - val_mean_absolute_error: 116.6676\n",
      "Epoch 620/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16019.1719 - mean_absolute_error: 99.2889 - val_loss: 13185.6064 - val_mean_absolute_error: 100.0735\n",
      "Epoch 621/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9392.4492 - mean_absolute_error: 72.7022 - val_loss: 9079.3770 - val_mean_absolute_error: 86.5045\n",
      "Epoch 622/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13545.7686 - mean_absolute_error: 92.1814 - val_loss: 7919.2432 - val_mean_absolute_error: 80.7625\n",
      "Epoch 623/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10168.4785 - mean_absolute_error: 82.9615 - val_loss: 7572.1475 - val_mean_absolute_error: 78.6219\n",
      "Epoch 624/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11647.6523 - mean_absolute_error: 100.3913 - val_loss: 11031.9268 - val_mean_absolute_error: 93.6864\n",
      "Epoch 625/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8647.5410 - mean_absolute_error: 71.2986 - val_loss: 14772.7344 - val_mean_absolute_error: 107.9195\n",
      "Epoch 626/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12240.8975 - mean_absolute_error: 85.7133 - val_loss: 15594.6953 - val_mean_absolute_error: 112.0747\n",
      "Epoch 627/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14247.6738 - mean_absolute_error: 94.4229 - val_loss: 14192.7969 - val_mean_absolute_error: 104.9903\n",
      "Epoch 628/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11508.9102 - mean_absolute_error: 86.1527 - val_loss: 11170.4473 - val_mean_absolute_error: 94.0408\n",
      "Epoch 629/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8676.6504 - mean_absolute_error: 71.7876 - val_loss: 8761.0547 - val_mean_absolute_error: 85.0601\n",
      "Epoch 630/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12941.1992 - mean_absolute_error: 95.6547 - val_loss: 10706.8965 - val_mean_absolute_error: 92.4843\n",
      "Epoch 631/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9842.2803 - mean_absolute_error: 81.3644 - val_loss: 13281.0264 - val_mean_absolute_error: 100.2099\n",
      "Epoch 632/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9713.0000 - mean_absolute_error: 71.9970 - val_loss: 13613.3232 - val_mean_absolute_error: 102.0734\n",
      "Epoch 633/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8992.0283 - mean_absolute_error: 73.9314 - val_loss: 11051.7549 - val_mean_absolute_error: 93.5865\n",
      "Epoch 634/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8601.3350 - mean_absolute_error: 63.8358 - val_loss: 7841.4600 - val_mean_absolute_error: 80.4786\n",
      "Epoch 635/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10614.5479 - mean_absolute_error: 82.8969 - val_loss: 7098.3174 - val_mean_absolute_error: 75.5770\n",
      "Epoch 636/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11430.7637 - mean_absolute_error: 88.2625 - val_loss: 8752.3604 - val_mean_absolute_error: 85.0213\n",
      "Epoch 637/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11941.2148 - mean_absolute_error: 95.5368 - val_loss: 13210.1660 - val_mean_absolute_error: 100.0693\n",
      "Epoch 638/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12013.4971 - mean_absolute_error: 87.6364 - val_loss: 15985.0439 - val_mean_absolute_error: 114.3141\n",
      "Epoch 639/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11203.0449 - mean_absolute_error: 86.0398 - val_loss: 15801.7998 - val_mean_absolute_error: 113.4426\n",
      "Epoch 640/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12631.6279 - mean_absolute_error: 89.9552 - val_loss: 12639.5156 - val_mean_absolute_error: 98.1051\n",
      "Epoch 641/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8343.3809 - mean_absolute_error: 66.2030 - val_loss: 8907.4941 - val_mean_absolute_error: 85.6640\n",
      "Epoch 642/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7072.8398 - mean_absolute_error: 62.9198 - val_loss: 6225.3115 - val_mean_absolute_error: 64.2951\n",
      "Epoch 643/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9848.8008 - mean_absolute_error: 81.5911 - val_loss: 6273.0576 - val_mean_absolute_error: 63.0201\n",
      "Epoch 644/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14872.5703 - mean_absolute_error: 101.1847 - val_loss: 7221.4678 - val_mean_absolute_error: 76.9215\n",
      "Epoch 645/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11881.3672 - mean_absolute_error: 100.2415 - val_loss: 13727.1904 - val_mean_absolute_error: 103.1333\n",
      "Epoch 646/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8763.9277 - mean_absolute_error: 74.9147 - val_loss: 18705.3125 - val_mean_absolute_error: 126.4374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 647/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 14222.2715 - mean_absolute_error: 91.3886 - val_loss: 20110.5371 - val_mean_absolute_error: 132.1223\n",
      "Epoch 648/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16672.7344 - mean_absolute_error: 109.2732 - val_loss: 17856.9375 - val_mean_absolute_error: 122.8382\n",
      "Epoch 649/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 14674.2900 - mean_absolute_error: 96.9524 - val_loss: 13103.7139 - val_mean_absolute_error: 99.7214\n",
      "Epoch 650/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10482.6025 - mean_absolute_error: 78.8081 - val_loss: 7923.9858 - val_mean_absolute_error: 81.0912\n",
      "Epoch 651/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9742.0439 - mean_absolute_error: 72.8240 - val_loss: 6205.0142 - val_mean_absolute_error: 65.6797\n",
      "Epoch 652/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 12702.2100 - mean_absolute_error: 98.3349 - val_loss: 6728.6611 - val_mean_absolute_error: 73.3046\n",
      "Epoch 653/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 13816.0449 - mean_absolute_error: 93.8028 - val_loss: 11281.8848 - val_mean_absolute_error: 93.9848\n",
      "Epoch 654/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10585.4668 - mean_absolute_error: 82.9793 - val_loss: 16799.3789 - val_mean_absolute_error: 118.3948\n",
      "Epoch 655/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 15247.7500 - mean_absolute_error: 98.4134 - val_loss: 18671.1602 - val_mean_absolute_error: 126.5187\n",
      "Epoch 656/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 12394.6006 - mean_absolute_error: 93.4376 - val_loss: 17032.0566 - val_mean_absolute_error: 119.5295\n",
      "Epoch 657/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11280.5117 - mean_absolute_error: 85.3192 - val_loss: 12595.2676 - val_mean_absolute_error: 97.5628\n",
      "Epoch 658/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10873.4180 - mean_absolute_error: 84.7132 - val_loss: 7601.0752 - val_mean_absolute_error: 79.4593\n",
      "Epoch 659/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7568.7939 - mean_absolute_error: 66.1970 - val_loss: 5932.0264 - val_mean_absolute_error: 61.8167\n",
      "Epoch 660/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 18154.3125 - mean_absolute_error: 114.4152 - val_loss: 7188.0454 - val_mean_absolute_error: 77.1109\n",
      "Epoch 661/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 11194.0791 - mean_absolute_error: 86.7006 - val_loss: 12181.6143 - val_mean_absolute_error: 96.3077\n",
      "Epoch 662/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10763.3037 - mean_absolute_error: 82.6226 - val_loss: 15614.2188 - val_mean_absolute_error: 113.1286\n",
      "Epoch 663/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10888.2295 - mean_absolute_error: 79.9340 - val_loss: 16077.6582 - val_mean_absolute_error: 115.2958\n",
      "Epoch 664/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12231.3281 - mean_absolute_error: 82.6582 - val_loss: 14034.4248 - val_mean_absolute_error: 105.3605\n",
      "Epoch 665/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11986.6924 - mean_absolute_error: 89.2131 - val_loss: 9363.5586 - val_mean_absolute_error: 87.2570\n",
      "Epoch 666/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8822.7012 - mean_absolute_error: 78.7619 - val_loss: 6209.3096 - val_mean_absolute_error: 69.5530\n",
      "Epoch 667/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14102.7695 - mean_absolute_error: 105.4499 - val_loss: 7258.6318 - val_mean_absolute_error: 77.6721\n",
      "Epoch 668/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8250.8008 - mean_absolute_error: 76.8732 - val_loss: 9328.9932 - val_mean_absolute_error: 87.0964\n",
      "Epoch 669/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11071.3604 - mean_absolute_error: 81.3628 - val_loss: 11333.3447 - val_mean_absolute_error: 93.7247\n",
      "Epoch 670/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9770.6113 - mean_absolute_error: 71.0251 - val_loss: 12900.5059 - val_mean_absolute_error: 99.5679\n",
      "Epoch 671/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10158.8145 - mean_absolute_error: 75.6655 - val_loss: 12105.1758 - val_mean_absolute_error: 95.8642\n",
      "Epoch 672/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9200.5068 - mean_absolute_error: 75.0932 - val_loss: 10180.4834 - val_mean_absolute_error: 89.9659\n",
      "Epoch 673/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9282.1865 - mean_absolute_error: 67.4906 - val_loss: 8935.1504 - val_mean_absolute_error: 85.4501\n",
      "Epoch 674/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11143.3555 - mean_absolute_error: 91.2267 - val_loss: 10746.0215 - val_mean_absolute_error: 91.6777\n",
      "Epoch 675/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9762.2695 - mean_absolute_error: 74.8363 - val_loss: 12069.5986 - val_mean_absolute_error: 95.5134\n",
      "Epoch 676/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8617.5635 - mean_absolute_error: 68.4745 - val_loss: 13892.5234 - val_mean_absolute_error: 105.3094\n",
      "Epoch 677/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11030.3438 - mean_absolute_error: 84.6709 - val_loss: 13343.9268 - val_mean_absolute_error: 102.5842\n",
      "Epoch 678/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9136.1504 - mean_absolute_error: 70.7766 - val_loss: 10826.5469 - val_mean_absolute_error: 91.6857\n",
      "Epoch 679/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8984.7441 - mean_absolute_error: 72.3391 - val_loss: 8324.1943 - val_mean_absolute_error: 82.7576\n",
      "Epoch 680/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7984.8994 - mean_absolute_error: 70.8814 - val_loss: 6985.4468 - val_mean_absolute_error: 76.4424\n",
      "Epoch 681/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9099.9805 - mean_absolute_error: 79.8646 - val_loss: 7758.7764 - val_mean_absolute_error: 80.2665\n",
      "Epoch 682/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7843.4492 - mean_absolute_error: 69.5545 - val_loss: 8854.1699 - val_mean_absolute_error: 84.7115\n",
      "Epoch 683/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8732.6279 - mean_absolute_error: 70.5776 - val_loss: 10647.5283 - val_mean_absolute_error: 90.6948\n",
      "Epoch 684/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8558.8906 - mean_absolute_error: 76.3769 - val_loss: 11174.1816 - val_mean_absolute_error: 92.1797\n",
      "Epoch 685/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10840.9170 - mean_absolute_error: 82.6887 - val_loss: 10566.3457 - val_mean_absolute_error: 90.2384\n",
      "Epoch 686/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8250.4014 - mean_absolute_error: 64.2431 - val_loss: 8634.9951 - val_mean_absolute_error: 83.4633\n",
      "Epoch 687/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7595.1875 - mean_absolute_error: 64.3083 - val_loss: 6564.8018 - val_mean_absolute_error: 74.1992\n",
      "Epoch 688/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 11522.8320 - mean_absolute_error: 94.7096 - val_loss: 9830.3203 - val_mean_absolute_error: 87.5421\n",
      "Epoch 689/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7826.8096 - mean_absolute_error: 68.7765 - val_loss: 11935.6025 - val_mean_absolute_error: 96.5841\n",
      "Epoch 690/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8933.1494 - mean_absolute_error: 76.2412 - val_loss: 11363.4824 - val_mean_absolute_error: 93.4874\n",
      "Epoch 691/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10443.4531 - mean_absolute_error: 84.3404 - val_loss: 9000.3652 - val_mean_absolute_error: 84.4730\n",
      "Epoch 692/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8131.4570 - mean_absolute_error: 71.0903 - val_loss: 8700.6445 - val_mean_absolute_error: 83.4210\n",
      "Epoch 693/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8486.1016 - mean_absolute_error: 74.5668 - val_loss: 10851.1582 - val_mean_absolute_error: 90.7335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7024.4844 - mean_absolute_error: 67.4772 - val_loss: 12170.9531 - val_mean_absolute_error: 98.1210\n",
      "Epoch 695/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10192.3066 - mean_absolute_error: 77.2311 - val_loss: 10370.6201 - val_mean_absolute_error: 88.9408\n",
      "Epoch 696/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6229.7695 - mean_absolute_error: 65.5556 - val_loss: 7398.0498 - val_mean_absolute_error: 77.8563\n",
      "Epoch 697/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7027.6304 - mean_absolute_error: 66.5409 - val_loss: 6454.9121 - val_mean_absolute_error: 73.4262\n",
      "Epoch 698/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8607.0664 - mean_absolute_error: 76.2151 - val_loss: 8546.2666 - val_mean_absolute_error: 82.0275\n",
      "Epoch 699/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6568.5059 - mean_absolute_error: 63.0317 - val_loss: 9464.3721 - val_mean_absolute_error: 84.9595\n",
      "Epoch 700/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4817.3652 - mean_absolute_error: 59.2537 - val_loss: 9147.0557 - val_mean_absolute_error: 83.6534\n",
      "Epoch 701/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10079.5693 - mean_absolute_error: 83.1317 - val_loss: 10721.4033 - val_mean_absolute_error: 91.7466\n",
      "Epoch 702/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8309.0879 - mean_absolute_error: 71.3088 - val_loss: 11539.5498 - val_mean_absolute_error: 96.0879\n",
      "Epoch 703/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10501.0391 - mean_absolute_error: 76.8936 - val_loss: 9791.9883 - val_mean_absolute_error: 86.1566\n",
      "Epoch 704/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10535.4414 - mean_absolute_error: 86.6747 - val_loss: 7840.6689 - val_mean_absolute_error: 79.4502\n",
      "Epoch 705/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8835.1895 - mean_absolute_error: 79.4640 - val_loss: 6877.9580 - val_mean_absolute_error: 75.5384\n",
      "Epoch 706/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9166.9160 - mean_absolute_error: 78.7318 - val_loss: 9686.8711 - val_mean_absolute_error: 86.1304\n",
      "Epoch 707/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8975.2012 - mean_absolute_error: 79.8022 - val_loss: 13008.2793 - val_mean_absolute_error: 103.2472\n",
      "Epoch 708/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8327.8545 - mean_absolute_error: 76.9573 - val_loss: 14170.9355 - val_mean_absolute_error: 108.8185\n",
      "Epoch 709/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10974.9238 - mean_absolute_error: 80.9579 - val_loss: 11946.6406 - val_mean_absolute_error: 97.7852\n",
      "Epoch 710/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7544.1895 - mean_absolute_error: 64.9218 - val_loss: 8324.3652 - val_mean_absolute_error: 81.1888\n",
      "Epoch 711/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7236.4478 - mean_absolute_error: 66.7626 - val_loss: 6035.8691 - val_mean_absolute_error: 71.2417\n",
      "Epoch 712/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9641.5703 - mean_absolute_error: 80.6861 - val_loss: 4925.4263 - val_mean_absolute_error: 64.4186\n",
      "Epoch 713/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 11759.0342 - mean_absolute_error: 89.4815 - val_loss: 6418.4072 - val_mean_absolute_error: 73.1429\n",
      "Epoch 714/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8577.3721 - mean_absolute_error: 79.1637 - val_loss: 8518.9385 - val_mean_absolute_error: 82.0160\n",
      "Epoch 715/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8834.9961 - mean_absolute_error: 79.4017 - val_loss: 11301.4404 - val_mean_absolute_error: 94.1311\n",
      "Epoch 716/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7643.4199 - mean_absolute_error: 71.3948 - val_loss: 11485.1279 - val_mean_absolute_error: 94.8814\n",
      "Epoch 717/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8699.3750 - mean_absolute_error: 74.8369 - val_loss: 8643.2129 - val_mean_absolute_error: 82.9899\n",
      "Epoch 718/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9692.9297 - mean_absolute_error: 77.1764 - val_loss: 7788.4385 - val_mean_absolute_error: 79.7913\n",
      "Epoch 719/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5336.7231 - mean_absolute_error: 57.6101 - val_loss: 6179.7627 - val_mean_absolute_error: 72.4790\n",
      "Epoch 720/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7625.5127 - mean_absolute_error: 71.1723 - val_loss: 7527.4189 - val_mean_absolute_error: 78.5640\n",
      "Epoch 721/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5733.4077 - mean_absolute_error: 61.9966 - val_loss: 9108.9941 - val_mean_absolute_error: 84.5016\n",
      "Epoch 722/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7819.9902 - mean_absolute_error: 67.8649 - val_loss: 11207.8301 - val_mean_absolute_error: 93.6275\n",
      "Epoch 723/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6973.3916 - mean_absolute_error: 68.8030 - val_loss: 11173.4082 - val_mean_absolute_error: 93.7626\n",
      "Epoch 724/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5424.6934 - mean_absolute_error: 54.8037 - val_loss: 8444.5947 - val_mean_absolute_error: 81.1946\n",
      "Epoch 725/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9580.4277 - mean_absolute_error: 75.0928 - val_loss: 5040.2383 - val_mean_absolute_error: 65.5621\n",
      "Epoch 726/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9426.6094 - mean_absolute_error: 81.1325 - val_loss: 6364.6318 - val_mean_absolute_error: 72.8912\n",
      "Epoch 727/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8120.8550 - mean_absolute_error: 71.6101 - val_loss: 11223.6992 - val_mean_absolute_error: 94.1274\n",
      "Epoch 728/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5130.2661 - mean_absolute_error: 56.1694 - val_loss: 12924.6572 - val_mean_absolute_error: 102.8584\n",
      "Epoch 729/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7616.6113 - mean_absolute_error: 73.7415 - val_loss: 10218.8125 - val_mean_absolute_error: 88.0683\n",
      "Epoch 730/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7726.1626 - mean_absolute_error: 73.5981 - val_loss: 6669.3604 - val_mean_absolute_error: 74.5544\n",
      "Epoch 731/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8646.9834 - mean_absolute_error: 80.0302 - val_loss: 4612.3828 - val_mean_absolute_error: 62.1659\n",
      "Epoch 732/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 12146.7090 - mean_absolute_error: 92.3542 - val_loss: 8622.7344 - val_mean_absolute_error: 82.1253\n",
      "Epoch 733/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7658.5229 - mean_absolute_error: 72.7124 - val_loss: 13586.9268 - val_mean_absolute_error: 106.3703\n",
      "Epoch 734/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9914.2812 - mean_absolute_error: 77.7289 - val_loss: 14435.5264 - val_mean_absolute_error: 110.4599\n",
      "Epoch 735/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11401.9561 - mean_absolute_error: 89.3969 - val_loss: 11290.0332 - val_mean_absolute_error: 95.1503\n",
      "Epoch 736/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8678.4062 - mean_absolute_error: 75.8575 - val_loss: 6019.0244 - val_mean_absolute_error: 70.8263\n",
      "Epoch 737/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7158.2783 - mean_absolute_error: 68.6959 - val_loss: 3638.0422 - val_mean_absolute_error: 52.0188\n",
      "Epoch 738/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 10136.3613 - mean_absolute_error: 89.7089 - val_loss: 5509.9966 - val_mean_absolute_error: 68.2069\n",
      "Epoch 739/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6802.3438 - mean_absolute_error: 66.8691 - val_loss: 9796.1729 - val_mean_absolute_error: 86.5766\n",
      "Epoch 740/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5442.2412 - mean_absolute_error: 63.6009 - val_loss: 12085.8096 - val_mean_absolute_error: 99.1947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9090.2520 - mean_absolute_error: 69.6288 - val_loss: 11051.5771 - val_mean_absolute_error: 93.7246\n",
      "Epoch 742/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6637.1001 - mean_absolute_error: 59.3775 - val_loss: 6759.8672 - val_mean_absolute_error: 74.5211\n",
      "Epoch 743/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6318.2817 - mean_absolute_error: 69.2581 - val_loss: 4338.6343 - val_mean_absolute_error: 60.8813\n",
      "Epoch 744/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6212.2949 - mean_absolute_error: 65.2228 - val_loss: 4289.3149 - val_mean_absolute_error: 60.6121\n",
      "Epoch 745/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6395.9233 - mean_absolute_error: 67.8112 - val_loss: 7442.3330 - val_mean_absolute_error: 77.1718\n",
      "Epoch 746/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8429.2568 - mean_absolute_error: 72.4277 - val_loss: 11079.8389 - val_mean_absolute_error: 94.1719\n",
      "Epoch 747/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7383.0747 - mean_absolute_error: 66.8159 - val_loss: 11603.3848 - val_mean_absolute_error: 96.9241\n",
      "Epoch 748/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 10165.4238 - mean_absolute_error: 83.7537 - val_loss: 9588.7344 - val_mean_absolute_error: 85.7454\n",
      "Epoch 749/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6860.8926 - mean_absolute_error: 67.8587 - val_loss: 5535.8188 - val_mean_absolute_error: 68.5447\n",
      "Epoch 750/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7738.8164 - mean_absolute_error: 70.1228 - val_loss: 3278.4739 - val_mean_absolute_error: 50.7197\n",
      "Epoch 751/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8782.3115 - mean_absolute_error: 75.8749 - val_loss: 3954.0337 - val_mean_absolute_error: 58.3274\n",
      "Epoch 752/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 12562.9150 - mean_absolute_error: 83.8065 - val_loss: 9998.3291 - val_mean_absolute_error: 87.9654\n",
      "Epoch 753/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5796.5122 - mean_absolute_error: 59.0078 - val_loss: 14008.6426 - val_mean_absolute_error: 108.3678\n",
      "Epoch 754/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6656.1675 - mean_absolute_error: 66.0405 - val_loss: 13713.2549 - val_mean_absolute_error: 106.9153\n",
      "Epoch 755/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 8382.0391 - mean_absolute_error: 72.6829 - val_loss: 10180.1768 - val_mean_absolute_error: 88.5470\n",
      "Epoch 756/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8465.9639 - mean_absolute_error: 69.0731 - val_loss: 5411.6045 - val_mean_absolute_error: 68.1430\n",
      "Epoch 757/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3906.5054 - mean_absolute_error: 48.3027 - val_loss: 3263.2202 - val_mean_absolute_error: 50.0065\n",
      "Epoch 758/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 16042.5293 - mean_absolute_error: 98.1609 - val_loss: 5788.7212 - val_mean_absolute_error: 70.0705\n",
      "Epoch 759/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8933.3535 - mean_absolute_error: 82.6521 - val_loss: 13356.4404 - val_mean_absolute_error: 105.0720\n",
      "Epoch 760/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11486.9375 - mean_absolute_error: 90.8213 - val_loss: 16759.1602 - val_mean_absolute_error: 120.0972\n",
      "Epoch 761/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 14734.8896 - mean_absolute_error: 103.7992 - val_loss: 16836.7383 - val_mean_absolute_error: 120.3629\n",
      "Epoch 762/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 11263.6953 - mean_absolute_error: 90.3296 - val_loss: 14024.5859 - val_mean_absolute_error: 107.9161\n",
      "Epoch 763/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9789.8047 - mean_absolute_error: 80.0518 - val_loss: 9479.5420 - val_mean_absolute_error: 85.3395\n",
      "Epoch 764/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 9382.3594 - mean_absolute_error: 81.3708 - val_loss: 5654.8555 - val_mean_absolute_error: 69.6540\n",
      "Epoch 765/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8202.7812 - mean_absolute_error: 74.8724 - val_loss: 4867.8379 - val_mean_absolute_error: 64.8939\n",
      "Epoch 766/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 10739.3906 - mean_absolute_error: 87.8178 - val_loss: 6940.2803 - val_mean_absolute_error: 75.9633\n",
      "Epoch 767/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7398.6250 - mean_absolute_error: 73.3694 - val_loss: 9782.5156 - val_mean_absolute_error: 86.6070\n",
      "Epoch 768/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7050.5049 - mean_absolute_error: 66.2444 - val_loss: 11345.8711 - val_mean_absolute_error: 94.0517\n",
      "Epoch 769/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8043.2827 - mean_absolute_error: 69.9932 - val_loss: 10191.7871 - val_mean_absolute_error: 88.0380\n",
      "Epoch 770/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9057.5020 - mean_absolute_error: 80.1333 - val_loss: 8187.6665 - val_mean_absolute_error: 81.2098\n",
      "Epoch 771/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7463.0659 - mean_absolute_error: 67.0585 - val_loss: 6285.0850 - val_mean_absolute_error: 73.1621\n",
      "Epoch 772/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7530.1099 - mean_absolute_error: 68.3503 - val_loss: 5824.2822 - val_mean_absolute_error: 70.7604\n",
      "Epoch 773/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6221.4946 - mean_absolute_error: 70.6167 - val_loss: 5585.4858 - val_mean_absolute_error: 69.4503\n",
      "Epoch 774/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6433.4951 - mean_absolute_error: 63.3137 - val_loss: 6212.3525 - val_mean_absolute_error: 72.7619\n",
      "Epoch 775/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6244.8823 - mean_absolute_error: 69.9139 - val_loss: 8182.9233 - val_mean_absolute_error: 81.0199\n",
      "Epoch 776/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7802.3862 - mean_absolute_error: 69.3728 - val_loss: 9883.7930 - val_mean_absolute_error: 86.7255\n",
      "Epoch 777/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7599.8115 - mean_absolute_error: 67.1650 - val_loss: 10031.6787 - val_mean_absolute_error: 87.1712\n",
      "Epoch 778/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8350.1348 - mean_absolute_error: 66.7417 - val_loss: 8634.1396 - val_mean_absolute_error: 82.1033\n",
      "Epoch 779/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8457.3027 - mean_absolute_error: 70.3289 - val_loss: 7071.7422 - val_mean_absolute_error: 75.7401\n",
      "Epoch 780/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7124.3457 - mean_absolute_error: 66.6663 - val_loss: 6590.6650 - val_mean_absolute_error: 73.3306\n",
      "Epoch 781/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5753.9683 - mean_absolute_error: 58.3241 - val_loss: 6597.5986 - val_mean_absolute_error: 73.0913\n",
      "Epoch 782/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6241.8843 - mean_absolute_error: 57.4438 - val_loss: 6984.4365 - val_mean_absolute_error: 74.4343\n",
      "Epoch 783/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6757.8740 - mean_absolute_error: 66.4186 - val_loss: 7558.9595 - val_mean_absolute_error: 76.4122\n",
      "Epoch 784/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5017.7871 - mean_absolute_error: 62.3187 - val_loss: 7416.4546 - val_mean_absolute_error: 75.6214\n",
      "Epoch 785/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7458.4473 - mean_absolute_error: 74.8775 - val_loss: 6949.2930 - val_mean_absolute_error: 73.4933\n",
      "Epoch 786/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6665.7939 - mean_absolute_error: 67.0161 - val_loss: 7347.9937 - val_mean_absolute_error: 74.9540\n",
      "Epoch 787/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9114.4941 - mean_absolute_error: 72.9868 - val_loss: 7866.3843 - val_mean_absolute_error: 76.9884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 788/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6487.0029 - mean_absolute_error: 62.7635 - val_loss: 7835.5312 - val_mean_absolute_error: 77.0780\n",
      "Epoch 789/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4522.8643 - mean_absolute_error: 55.2880 - val_loss: 7135.3164 - val_mean_absolute_error: 74.5204\n",
      "Epoch 790/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5150.4468 - mean_absolute_error: 55.7183 - val_loss: 5735.3804 - val_mean_absolute_error: 68.5125\n",
      "Epoch 791/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4622.4434 - mean_absolute_error: 50.9260 - val_loss: 6128.8594 - val_mean_absolute_error: 70.5301\n",
      "Epoch 792/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7183.9014 - mean_absolute_error: 70.8683 - val_loss: 7949.6460 - val_mean_absolute_error: 78.0966\n",
      "Epoch 793/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7308.2051 - mean_absolute_error: 71.0727 - val_loss: 10081.4414 - val_mean_absolute_error: 88.7790\n",
      "Epoch 794/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9150.2910 - mean_absolute_error: 79.2425 - val_loss: 10514.8936 - val_mean_absolute_error: 90.8920\n",
      "Epoch 795/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7079.7466 - mean_absolute_error: 66.9714 - val_loss: 9313.1562 - val_mean_absolute_error: 83.8336\n",
      "Epoch 796/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7285.7461 - mean_absolute_error: 71.0286 - val_loss: 6266.0488 - val_mean_absolute_error: 72.1535\n",
      "Epoch 797/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6546.9629 - mean_absolute_error: 63.3103 - val_loss: 4971.0625 - val_mean_absolute_error: 65.6497\n",
      "Epoch 798/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6946.2100 - mean_absolute_error: 67.6580 - val_loss: 5770.3188 - val_mean_absolute_error: 69.8724\n",
      "Epoch 799/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9349.8926 - mean_absolute_error: 76.7581 - val_loss: 8068.7251 - val_mean_absolute_error: 79.5452\n",
      "Epoch 800/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5353.6147 - mean_absolute_error: 57.4185 - val_loss: 9909.7129 - val_mean_absolute_error: 86.9397\n",
      "Epoch 801/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7211.4229 - mean_absolute_error: 67.0957 - val_loss: 10200.3418 - val_mean_absolute_error: 88.6541\n",
      "Epoch 802/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7240.6738 - mean_absolute_error: 67.4858 - val_loss: 8266.1631 - val_mean_absolute_error: 80.1379\n",
      "Epoch 803/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6183.1650 - mean_absolute_error: 62.2716 - val_loss: 5751.5610 - val_mean_absolute_error: 69.7826\n",
      "Epoch 804/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 10291.3711 - mean_absolute_error: 83.0609 - val_loss: 4358.7358 - val_mean_absolute_error: 61.7268\n",
      "Epoch 805/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4664.9956 - mean_absolute_error: 57.8278 - val_loss: 4120.1455 - val_mean_absolute_error: 59.8892\n",
      "Epoch 806/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5742.1709 - mean_absolute_error: 63.2591 - val_loss: 5385.5244 - val_mean_absolute_error: 67.1832\n",
      "Epoch 807/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5405.7578 - mean_absolute_error: 57.2764 - val_loss: 6998.4365 - val_mean_absolute_error: 74.1608\n",
      "Epoch 808/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6980.3843 - mean_absolute_error: 69.0938 - val_loss: 7260.7983 - val_mean_absolute_error: 74.8214\n",
      "Epoch 809/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6513.5581 - mean_absolute_error: 67.4498 - val_loss: 6508.7295 - val_mean_absolute_error: 71.4735\n",
      "Epoch 810/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8544.8877 - mean_absolute_error: 71.9526 - val_loss: 5063.4795 - val_mean_absolute_error: 64.4805\n",
      "Epoch 811/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9052.2461 - mean_absolute_error: 79.1413 - val_loss: 5301.7334 - val_mean_absolute_error: 65.8326\n",
      "Epoch 812/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3478.1323 - mean_absolute_error: 47.3474 - val_loss: 6469.0103 - val_mean_absolute_error: 71.4410\n",
      "Epoch 813/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6399.7666 - mean_absolute_error: 65.0671 - val_loss: 7867.4795 - val_mean_absolute_error: 77.3490\n",
      "Epoch 814/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6831.0078 - mean_absolute_error: 67.0647 - val_loss: 8239.1465 - val_mean_absolute_error: 78.9474\n",
      "Epoch 815/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7839.9531 - mean_absolute_error: 76.5783 - val_loss: 7476.8281 - val_mean_absolute_error: 76.2893\n",
      "Epoch 816/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6857.9346 - mean_absolute_error: 60.5382 - val_loss: 6962.5063 - val_mean_absolute_error: 74.4225\n",
      "Epoch 817/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5105.8550 - mean_absolute_error: 62.8285 - val_loss: 5727.1846 - val_mean_absolute_error: 69.0963\n",
      "Epoch 818/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9030.3535 - mean_absolute_error: 70.6548 - val_loss: 5718.3755 - val_mean_absolute_error: 69.0513\n",
      "Epoch 819/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3414.3462 - mean_absolute_error: 40.8691 - val_loss: 5188.1021 - val_mean_absolute_error: 66.3124\n",
      "Epoch 820/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5480.9692 - mean_absolute_error: 55.2992 - val_loss: 4530.2759 - val_mean_absolute_error: 62.5347\n",
      "Epoch 821/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8301.0352 - mean_absolute_error: 75.2294 - val_loss: 5988.6396 - val_mean_absolute_error: 70.1628\n",
      "Epoch 822/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7410.2515 - mean_absolute_error: 70.7045 - val_loss: 8130.2485 - val_mean_absolute_error: 78.5494\n",
      "Epoch 823/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6872.1416 - mean_absolute_error: 64.2043 - val_loss: 9213.7305 - val_mean_absolute_error: 83.9655\n",
      "Epoch 824/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6982.0078 - mean_absolute_error: 69.1690 - val_loss: 8062.0596 - val_mean_absolute_error: 78.0859\n",
      "Epoch 825/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6172.2109 - mean_absolute_error: 69.9876 - val_loss: 5362.5132 - val_mean_absolute_error: 66.5397\n",
      "Epoch 826/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7910.7915 - mean_absolute_error: 72.4940 - val_loss: 3559.1743 - val_mean_absolute_error: 55.1876\n",
      "Epoch 827/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6094.3813 - mean_absolute_error: 64.3164 - val_loss: 3751.6499 - val_mean_absolute_error: 56.6666\n",
      "Epoch 828/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 8389.3926 - mean_absolute_error: 64.9153 - val_loss: 5457.1826 - val_mean_absolute_error: 67.0404\n",
      "Epoch 829/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5777.4556 - mean_absolute_error: 59.8827 - val_loss: 7718.4438 - val_mean_absolute_error: 76.9191\n",
      "Epoch 830/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8271.3359 - mean_absolute_error: 63.5648 - val_loss: 8367.0996 - val_mean_absolute_error: 79.4187\n",
      "Epoch 831/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4938.2979 - mean_absolute_error: 60.9141 - val_loss: 7947.1807 - val_mean_absolute_error: 78.0451\n",
      "Epoch 832/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7696.5850 - mean_absolute_error: 70.7630 - val_loss: 6787.3584 - val_mean_absolute_error: 73.7018\n",
      "Epoch 833/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 9362.8301 - mean_absolute_error: 80.2437 - val_loss: 5917.2622 - val_mean_absolute_error: 69.9798\n",
      "Epoch 834/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8116.3516 - mean_absolute_error: 75.4275 - val_loss: 6105.7549 - val_mean_absolute_error: 70.8924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6068.0654 - mean_absolute_error: 62.6150 - val_loss: 6011.4863 - val_mean_absolute_error: 70.4470\n",
      "Epoch 836/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4500.5522 - mean_absolute_error: 54.1787 - val_loss: 5366.3096 - val_mean_absolute_error: 67.2524\n",
      "Epoch 837/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4444.7368 - mean_absolute_error: 55.9307 - val_loss: 6008.6758 - val_mean_absolute_error: 70.1617\n",
      "Epoch 838/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5634.7559 - mean_absolute_error: 52.8766 - val_loss: 6279.1553 - val_mean_absolute_error: 71.1406\n",
      "Epoch 839/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3336.3721 - mean_absolute_error: 49.4900 - val_loss: 6410.0039 - val_mean_absolute_error: 71.3934\n",
      "Epoch 840/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4160.2388 - mean_absolute_error: 51.8682 - val_loss: 5456.2178 - val_mean_absolute_error: 66.4966\n",
      "Epoch 841/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5871.8975 - mean_absolute_error: 63.4843 - val_loss: 5358.0142 - val_mean_absolute_error: 65.5701\n",
      "Epoch 842/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5616.6641 - mean_absolute_error: 62.1789 - val_loss: 6210.1147 - val_mean_absolute_error: 69.2986\n",
      "Epoch 843/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5520.8188 - mean_absolute_error: 65.9715 - val_loss: 6363.8765 - val_mean_absolute_error: 69.8996\n",
      "Epoch 844/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4308.0981 - mean_absolute_error: 52.5388 - val_loss: 6619.5132 - val_mean_absolute_error: 71.0060\n",
      "Epoch 845/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7808.4629 - mean_absolute_error: 75.3597 - val_loss: 5947.7012 - val_mean_absolute_error: 68.1986\n",
      "Epoch 846/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6110.1440 - mean_absolute_error: 63.5970 - val_loss: 4701.3096 - val_mean_absolute_error: 61.9434\n",
      "Epoch 847/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4438.2314 - mean_absolute_error: 53.0355 - val_loss: 4153.0010 - val_mean_absolute_error: 58.7646\n",
      "Epoch 848/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3996.4878 - mean_absolute_error: 54.1194 - val_loss: 3944.9575 - val_mean_absolute_error: 57.4629\n",
      "Epoch 849/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7685.7056 - mean_absolute_error: 70.2602 - val_loss: 5396.5498 - val_mean_absolute_error: 65.9152\n",
      "Epoch 850/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6684.2949 - mean_absolute_error: 64.9472 - val_loss: 7089.6484 - val_mean_absolute_error: 73.4358\n",
      "Epoch 851/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4921.3472 - mean_absolute_error: 56.3775 - val_loss: 8580.5000 - val_mean_absolute_error: 80.2166\n",
      "Epoch 852/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7565.9746 - mean_absolute_error: 70.2070 - val_loss: 8755.9297 - val_mean_absolute_error: 81.0816\n",
      "Epoch 853/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7167.3125 - mean_absolute_error: 69.2788 - val_loss: 7699.2393 - val_mean_absolute_error: 76.1537\n",
      "Epoch 854/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5697.9434 - mean_absolute_error: 57.5868 - val_loss: 6455.9365 - val_mean_absolute_error: 71.4668\n",
      "Epoch 855/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5302.2041 - mean_absolute_error: 62.3594 - val_loss: 5143.1313 - val_mean_absolute_error: 65.4689\n",
      "Epoch 856/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5160.6851 - mean_absolute_error: 60.2774 - val_loss: 5128.9424 - val_mean_absolute_error: 65.3876\n",
      "Epoch 857/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8163.2856 - mean_absolute_error: 73.2445 - val_loss: 5266.0757 - val_mean_absolute_error: 66.1000\n",
      "Epoch 858/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7612.7891 - mean_absolute_error: 75.0088 - val_loss: 6185.8452 - val_mean_absolute_error: 70.3818\n",
      "Epoch 859/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6941.7476 - mean_absolute_error: 69.9286 - val_loss: 7498.2803 - val_mean_absolute_error: 75.6381\n",
      "Epoch 860/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5762.7964 - mean_absolute_error: 65.0590 - val_loss: 7689.5298 - val_mean_absolute_error: 76.2984\n",
      "Epoch 861/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9041.1035 - mean_absolute_error: 79.5161 - val_loss: 7905.4790 - val_mean_absolute_error: 77.0470\n",
      "Epoch 862/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5854.5737 - mean_absolute_error: 62.5501 - val_loss: 7518.7666 - val_mean_absolute_error: 75.5808\n",
      "Epoch 863/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4706.1704 - mean_absolute_error: 55.1361 - val_loss: 6652.7358 - val_mean_absolute_error: 72.0964\n",
      "Epoch 864/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6224.7480 - mean_absolute_error: 63.4211 - val_loss: 5449.2646 - val_mean_absolute_error: 66.5498\n",
      "Epoch 865/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5761.5239 - mean_absolute_error: 64.1409 - val_loss: 4988.8071 - val_mean_absolute_error: 64.0425\n",
      "Epoch 866/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5218.0322 - mean_absolute_error: 55.3757 - val_loss: 4818.8784 - val_mean_absolute_error: 63.0776\n",
      "Epoch 867/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5886.2334 - mean_absolute_error: 65.7997 - val_loss: 5763.0166 - val_mean_absolute_error: 67.8950\n",
      "Epoch 868/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4723.2480 - mean_absolute_error: 51.5479 - val_loss: 6055.9541 - val_mean_absolute_error: 69.2551\n",
      "Epoch 869/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4294.3330 - mean_absolute_error: 49.5558 - val_loss: 5493.8242 - val_mean_absolute_error: 66.5237\n",
      "Epoch 870/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5530.1621 - mean_absolute_error: 63.1336 - val_loss: 5715.2500 - val_mean_absolute_error: 67.4922\n",
      "Epoch 871/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7398.0527 - mean_absolute_error: 70.2293 - val_loss: 6280.7603 - val_mean_absolute_error: 69.9690\n",
      "Epoch 872/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7081.5840 - mean_absolute_error: 67.9233 - val_loss: 7176.7031 - val_mean_absolute_error: 73.6167\n",
      "Epoch 873/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7465.2065 - mean_absolute_error: 70.0326 - val_loss: 7271.9365 - val_mean_absolute_error: 73.9858\n",
      "Epoch 874/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4664.4585 - mean_absolute_error: 55.3889 - val_loss: 6435.3188 - val_mean_absolute_error: 70.5874\n",
      "Epoch 875/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4314.9966 - mean_absolute_error: 52.6701 - val_loss: 5203.4346 - val_mean_absolute_error: 64.8742\n",
      "Epoch 876/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7826.1699 - mean_absolute_error: 73.2415 - val_loss: 4808.1748 - val_mean_absolute_error: 62.8719\n",
      "Epoch 877/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5636.2627 - mean_absolute_error: 55.1552 - val_loss: 4697.8486 - val_mean_absolute_error: 62.1937\n",
      "Epoch 878/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7795.4233 - mean_absolute_error: 76.1127 - val_loss: 5134.3516 - val_mean_absolute_error: 64.4165\n",
      "Epoch 879/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6398.9766 - mean_absolute_error: 62.8871 - val_loss: 5318.2588 - val_mean_absolute_error: 65.3387\n",
      "Epoch 880/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5253.6641 - mean_absolute_error: 57.9898 - val_loss: 6144.7021 - val_mean_absolute_error: 68.9655\n",
      "Epoch 881/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5173.3442 - mean_absolute_error: 60.2989 - val_loss: 6253.6499 - val_mean_absolute_error: 69.3411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 882/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5264.3682 - mean_absolute_error: 58.1535 - val_loss: 5767.9233 - val_mean_absolute_error: 67.1151\n",
      "Epoch 883/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5870.9604 - mean_absolute_error: 59.9565 - val_loss: 4864.8940 - val_mean_absolute_error: 62.6186\n",
      "Epoch 884/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6955.2988 - mean_absolute_error: 68.7035 - val_loss: 4790.7549 - val_mean_absolute_error: 62.1879\n",
      "Epoch 885/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7539.1387 - mean_absolute_error: 64.9791 - val_loss: 4872.8018 - val_mean_absolute_error: 62.6849\n",
      "Epoch 886/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5556.7529 - mean_absolute_error: 64.4536 - val_loss: 5779.8330 - val_mean_absolute_error: 67.2177\n",
      "Epoch 887/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6944.7119 - mean_absolute_error: 68.5291 - val_loss: 6573.0469 - val_mean_absolute_error: 70.6856\n",
      "Epoch 888/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4541.3730 - mean_absolute_error: 52.7388 - val_loss: 6772.0156 - val_mean_absolute_error: 71.5083\n",
      "Epoch 889/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6353.9995 - mean_absolute_error: 59.7271 - val_loss: 6345.6138 - val_mean_absolute_error: 69.7723\n",
      "Epoch 890/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7459.3848 - mean_absolute_error: 76.2654 - val_loss: 5475.4990 - val_mean_absolute_error: 65.9381\n",
      "Epoch 891/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3195.1108 - mean_absolute_error: 50.3089 - val_loss: 4392.3062 - val_mean_absolute_error: 60.3116\n",
      "Epoch 892/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7199.6836 - mean_absolute_error: 65.7635 - val_loss: 4980.9072 - val_mean_absolute_error: 63.4765\n",
      "Epoch 893/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5969.3560 - mean_absolute_error: 61.2802 - val_loss: 5546.9321 - val_mean_absolute_error: 66.1661\n",
      "Epoch 894/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5629.9019 - mean_absolute_error: 66.2535 - val_loss: 5511.8003 - val_mean_absolute_error: 65.9713\n",
      "Epoch 895/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4667.1191 - mean_absolute_error: 52.1579 - val_loss: 5298.9170 - val_mean_absolute_error: 64.8981\n",
      "Epoch 896/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5378.0615 - mean_absolute_error: 57.0827 - val_loss: 5468.4370 - val_mean_absolute_error: 65.6071\n",
      "Epoch 897/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7657.0049 - mean_absolute_error: 73.1000 - val_loss: 5271.7563 - val_mean_absolute_error: 64.5675\n",
      "Epoch 898/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3222.4551 - mean_absolute_error: 47.5739 - val_loss: 5299.3315 - val_mean_absolute_error: 64.5490\n",
      "Epoch 899/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5720.6636 - mean_absolute_error: 63.2115 - val_loss: 5043.2959 - val_mean_absolute_error: 63.0803\n",
      "Epoch 900/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5032.5503 - mean_absolute_error: 61.2416 - val_loss: 4511.8862 - val_mean_absolute_error: 60.0919\n",
      "Epoch 901/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3377.1396 - mean_absolute_error: 47.7874 - val_loss: 4336.4888 - val_mean_absolute_error: 58.8035\n",
      "Epoch 902/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4483.6021 - mean_absolute_error: 54.9349 - val_loss: 4025.8618 - val_mean_absolute_error: 56.6297\n",
      "Epoch 903/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5398.8569 - mean_absolute_error: 57.4689 - val_loss: 4504.9209 - val_mean_absolute_error: 59.2248\n",
      "Epoch 904/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6809.8711 - mean_absolute_error: 66.9340 - val_loss: 6011.8550 - val_mean_absolute_error: 66.7738\n",
      "Epoch 905/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3745.9551 - mean_absolute_error: 48.4441 - val_loss: 7163.9907 - val_mean_absolute_error: 71.5722\n",
      "Epoch 906/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3224.6724 - mean_absolute_error: 44.0063 - val_loss: 7090.8018 - val_mean_absolute_error: 71.2806\n",
      "Epoch 907/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5750.6221 - mean_absolute_error: 61.8359 - val_loss: 6397.2900 - val_mean_absolute_error: 68.4823\n",
      "Epoch 908/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5215.5176 - mean_absolute_error: 59.6320 - val_loss: 4857.3555 - val_mean_absolute_error: 61.3575\n",
      "Epoch 909/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6274.7446 - mean_absolute_error: 65.2842 - val_loss: 3649.1001 - val_mean_absolute_error: 54.1909\n",
      "Epoch 910/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5914.8398 - mean_absolute_error: 63.0281 - val_loss: 3125.6104 - val_mean_absolute_error: 50.4102\n",
      "Epoch 911/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5997.5254 - mean_absolute_error: 61.9207 - val_loss: 4162.1421 - val_mean_absolute_error: 57.7187\n",
      "Epoch 912/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3961.1980 - mean_absolute_error: 50.4424 - val_loss: 5394.9683 - val_mean_absolute_error: 64.3150\n",
      "Epoch 913/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7171.4424 - mean_absolute_error: 69.1301 - val_loss: 6432.1221 - val_mean_absolute_error: 69.0888\n",
      "Epoch 914/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 6615.2832 - mean_absolute_error: 60.9829 - val_loss: 7031.1108 - val_mean_absolute_error: 71.7943\n",
      "Epoch 915/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4099.0908 - mean_absolute_error: 46.2204 - val_loss: 6917.7554 - val_mean_absolute_error: 71.5352\n",
      "Epoch 916/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6507.5352 - mean_absolute_error: 64.4442 - val_loss: 6189.7173 - val_mean_absolute_error: 68.6866\n",
      "Epoch 917/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4480.1895 - mean_absolute_error: 50.4132 - val_loss: 5579.3081 - val_mean_absolute_error: 66.0044\n",
      "Epoch 918/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5535.9897 - mean_absolute_error: 57.6961 - val_loss: 5465.8540 - val_mean_absolute_error: 65.3977\n",
      "Epoch 919/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5914.1934 - mean_absolute_error: 64.6229 - val_loss: 5341.9858 - val_mean_absolute_error: 64.6470\n",
      "Epoch 920/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2935.2368 - mean_absolute_error: 45.6249 - val_loss: 5079.9463 - val_mean_absolute_error: 63.1297\n",
      "Epoch 921/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3785.1206 - mean_absolute_error: 53.1102 - val_loss: 4983.4541 - val_mean_absolute_error: 62.3915\n",
      "Epoch 922/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5047.2612 - mean_absolute_error: 60.0278 - val_loss: 4889.0225 - val_mean_absolute_error: 61.5694\n",
      "Epoch 923/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5294.4785 - mean_absolute_error: 63.2794 - val_loss: 4780.8345 - val_mean_absolute_error: 60.5948\n",
      "Epoch 924/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7810.9487 - mean_absolute_error: 69.6969 - val_loss: 4908.7603 - val_mean_absolute_error: 60.8435\n",
      "Epoch 925/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3722.5610 - mean_absolute_error: 49.8282 - val_loss: 4773.3740 - val_mean_absolute_error: 59.7350\n",
      "Epoch 926/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5908.1406 - mean_absolute_error: 64.0232 - val_loss: 5057.7207 - val_mean_absolute_error: 60.9726\n",
      "Epoch 927/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5320.4429 - mean_absolute_error: 64.1133 - val_loss: 4841.3638 - val_mean_absolute_error: 59.6033\n",
      "Epoch 928/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7998.2192 - mean_absolute_error: 71.2127 - val_loss: 4274.7822 - val_mean_absolute_error: 56.2597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 929/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5116.2979 - mean_absolute_error: 56.4999 - val_loss: 3631.0789 - val_mean_absolute_error: 52.1044\n",
      "Epoch 930/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7069.4492 - mean_absolute_error: 66.0872 - val_loss: 3523.9797 - val_mean_absolute_error: 51.5951\n",
      "Epoch 931/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4324.4668 - mean_absolute_error: 57.1970 - val_loss: 4034.7864 - val_mean_absolute_error: 55.3208\n",
      "Epoch 932/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5791.3354 - mean_absolute_error: 67.4896 - val_loss: 5232.2979 - val_mean_absolute_error: 62.1909\n",
      "Epoch 933/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5977.1416 - mean_absolute_error: 61.3924 - val_loss: 6677.9126 - val_mean_absolute_error: 68.8842\n",
      "Epoch 934/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5390.7754 - mean_absolute_error: 58.5396 - val_loss: 7332.7524 - val_mean_absolute_error: 71.7292\n",
      "Epoch 935/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5036.1680 - mean_absolute_error: 61.5692 - val_loss: 7109.1201 - val_mean_absolute_error: 71.0198\n",
      "Epoch 936/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4679.9878 - mean_absolute_error: 59.4880 - val_loss: 6457.4448 - val_mean_absolute_error: 68.5748\n",
      "Epoch 937/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5471.4009 - mean_absolute_error: 61.9897 - val_loss: 5570.4326 - val_mean_absolute_error: 64.7926\n",
      "Epoch 938/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5569.6655 - mean_absolute_error: 62.9587 - val_loss: 4700.0254 - val_mean_absolute_error: 60.4603\n",
      "Epoch 939/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6029.9395 - mean_absolute_error: 61.8845 - val_loss: 4099.0430 - val_mean_absolute_error: 56.8404\n",
      "Epoch 940/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6185.3232 - mean_absolute_error: 67.9934 - val_loss: 3823.0464 - val_mean_absolute_error: 54.8617\n",
      "Epoch 941/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3975.8872 - mean_absolute_error: 54.5217 - val_loss: 4019.8652 - val_mean_absolute_error: 56.1626\n",
      "Epoch 942/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5891.4395 - mean_absolute_error: 62.7172 - val_loss: 4391.9014 - val_mean_absolute_error: 58.4253\n",
      "Epoch 943/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3515.2749 - mean_absolute_error: 45.4329 - val_loss: 5048.0146 - val_mean_absolute_error: 61.9261\n",
      "Epoch 944/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5538.7666 - mean_absolute_error: 62.2377 - val_loss: 5739.3623 - val_mean_absolute_error: 65.1272\n",
      "Epoch 945/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6466.5723 - mean_absolute_error: 60.9215 - val_loss: 5921.2021 - val_mean_absolute_error: 65.8617\n",
      "Epoch 946/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3494.9485 - mean_absolute_error: 47.4366 - val_loss: 5575.4902 - val_mean_absolute_error: 64.1664\n",
      "Epoch 947/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3562.9302 - mean_absolute_error: 49.6371 - val_loss: 5100.9429 - val_mean_absolute_error: 61.7049\n",
      "Epoch 948/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2816.7087 - mean_absolute_error: 41.0488 - val_loss: 4487.3042 - val_mean_absolute_error: 58.2222\n",
      "Epoch 949/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4511.8521 - mean_absolute_error: 53.7413 - val_loss: 4003.2312 - val_mean_absolute_error: 55.1034\n",
      "Epoch 950/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8015.1123 - mean_absolute_error: 70.1572 - val_loss: 3811.6145 - val_mean_absolute_error: 53.6391\n",
      "Epoch 951/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5361.7944 - mean_absolute_error: 54.1262 - val_loss: 3673.9282 - val_mean_absolute_error: 52.5495\n",
      "Epoch 952/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4623.3140 - mean_absolute_error: 58.1628 - val_loss: 3616.1167 - val_mean_absolute_error: 51.9554\n",
      "Epoch 953/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6714.4692 - mean_absolute_error: 69.1465 - val_loss: 4065.6594 - val_mean_absolute_error: 54.8402\n",
      "Epoch 954/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4808.6025 - mean_absolute_error: 62.0811 - val_loss: 4189.6826 - val_mean_absolute_error: 55.5399\n",
      "Epoch 955/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6419.1299 - mean_absolute_error: 62.1784 - val_loss: 4089.8684 - val_mean_absolute_error: 54.8340\n",
      "Epoch 956/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4025.8777 - mean_absolute_error: 53.1981 - val_loss: 4479.8076 - val_mean_absolute_error: 57.0783\n",
      "Epoch 957/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 6663.7759 - mean_absolute_error: 67.9276 - val_loss: 4855.5352 - val_mean_absolute_error: 59.0581\n",
      "Epoch 958/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5951.8086 - mean_absolute_error: 66.5144 - val_loss: 4998.0889 - val_mean_absolute_error: 59.8239\n",
      "Epoch 959/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6066.3013 - mean_absolute_error: 65.4803 - val_loss: 4966.8018 - val_mean_absolute_error: 59.6711\n",
      "Epoch 960/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3326.6611 - mean_absolute_error: 48.0728 - val_loss: 4813.4346 - val_mean_absolute_error: 58.8864\n",
      "Epoch 961/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7533.9023 - mean_absolute_error: 68.9431 - val_loss: 4179.8057 - val_mean_absolute_error: 55.3155\n",
      "Epoch 962/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5223.6421 - mean_absolute_error: 61.1854 - val_loss: 3696.5110 - val_mean_absolute_error: 52.1811\n",
      "Epoch 963/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4915.5933 - mean_absolute_error: 53.9952 - val_loss: 3552.1816 - val_mean_absolute_error: 51.1864\n",
      "Epoch 964/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4956.9326 - mean_absolute_error: 59.0447 - val_loss: 3745.5063 - val_mean_absolute_error: 52.6148\n",
      "Epoch 965/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3966.3167 - mean_absolute_error: 50.1938 - val_loss: 3815.7285 - val_mean_absolute_error: 53.1483\n",
      "Epoch 966/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5013.1709 - mean_absolute_error: 57.0119 - val_loss: 3804.8391 - val_mean_absolute_error: 53.0807\n",
      "Epoch 967/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5679.7183 - mean_absolute_error: 57.0246 - val_loss: 3636.1399 - val_mean_absolute_error: 51.8501\n",
      "Epoch 968/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7145.0752 - mean_absolute_error: 61.9544 - val_loss: 3888.0054 - val_mean_absolute_error: 53.5451\n",
      "Epoch 969/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5425.1904 - mean_absolute_error: 61.9815 - val_loss: 4098.2266 - val_mean_absolute_error: 54.8052\n",
      "Epoch 970/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4903.2129 - mean_absolute_error: 63.5234 - val_loss: 4408.6494 - val_mean_absolute_error: 56.6376\n",
      "Epoch 971/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4630.2627 - mean_absolute_error: 55.6665 - val_loss: 4806.5947 - val_mean_absolute_error: 58.8712\n",
      "Epoch 972/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5429.6597 - mean_absolute_error: 64.4854 - val_loss: 5199.7515 - val_mean_absolute_error: 60.8610\n",
      "Epoch 973/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4189.7949 - mean_absolute_error: 55.0562 - val_loss: 5095.7280 - val_mean_absolute_error: 60.3745\n",
      "Epoch 974/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5472.3086 - mean_absolute_error: 65.1629 - val_loss: 5124.7642 - val_mean_absolute_error: 60.5117\n",
      "Epoch 975/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6899.7051 - mean_absolute_error: 74.5512 - val_loss: 4829.3081 - val_mean_absolute_error: 58.9417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 976/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6230.3799 - mean_absolute_error: 66.3430 - val_loss: 4384.4946 - val_mean_absolute_error: 56.5133\n",
      "Epoch 977/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3956.9824 - mean_absolute_error: 49.8041 - val_loss: 3986.2234 - val_mean_absolute_error: 54.0494\n",
      "Epoch 978/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4233.1631 - mean_absolute_error: 50.6856 - val_loss: 3963.8169 - val_mean_absolute_error: 53.8346\n",
      "Epoch 979/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4842.0635 - mean_absolute_error: 62.6529 - val_loss: 4209.6494 - val_mean_absolute_error: 55.2976\n",
      "Epoch 980/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5593.9048 - mean_absolute_error: 60.6848 - val_loss: 4582.0596 - val_mean_absolute_error: 57.3336\n",
      "Epoch 981/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 6187.6191 - mean_absolute_error: 62.7021 - val_loss: 4763.3174 - val_mean_absolute_error: 58.2566\n",
      "Epoch 982/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5591.8203 - mean_absolute_error: 64.6324 - val_loss: 4908.8291 - val_mean_absolute_error: 59.0180\n",
      "Epoch 983/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5143.5337 - mean_absolute_error: 64.1867 - val_loss: 4683.1553 - val_mean_absolute_error: 57.7813\n",
      "Epoch 984/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5205.5200 - mean_absolute_error: 61.2012 - val_loss: 4295.6084 - val_mean_absolute_error: 55.5278\n",
      "Epoch 985/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2968.0654 - mean_absolute_error: 45.3804 - val_loss: 3826.3071 - val_mean_absolute_error: 52.4776\n",
      "Epoch 986/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4459.4697 - mean_absolute_error: 52.7367 - val_loss: 3211.7712 - val_mean_absolute_error: 47.7602\n",
      "Epoch 987/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6801.0947 - mean_absolute_error: 66.9790 - val_loss: 3251.1169 - val_mean_absolute_error: 48.0686\n",
      "Epoch 988/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6842.2046 - mean_absolute_error: 60.5965 - val_loss: 3742.0110 - val_mean_absolute_error: 51.8040\n",
      "Epoch 989/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2314.9492 - mean_absolute_error: 40.5899 - val_loss: 4058.3835 - val_mean_absolute_error: 53.8650\n",
      "Epoch 990/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3048.8774 - mean_absolute_error: 43.2199 - val_loss: 4285.3950 - val_mean_absolute_error: 55.2398\n",
      "Epoch 991/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5080.2095 - mean_absolute_error: 58.2315 - val_loss: 4338.3037 - val_mean_absolute_error: 55.5525\n",
      "Epoch 992/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3585.1772 - mean_absolute_error: 46.7948 - val_loss: 4105.3350 - val_mean_absolute_error: 54.1057\n",
      "Epoch 993/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4105.8076 - mean_absolute_error: 53.2287 - val_loss: 3723.6733 - val_mean_absolute_error: 51.5371\n",
      "Epoch 994/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4467.2998 - mean_absolute_error: 53.3210 - val_loss: 3419.6289 - val_mean_absolute_error: 49.2658\n",
      "Epoch 995/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4780.0864 - mean_absolute_error: 58.1578 - val_loss: 3186.2625 - val_mean_absolute_error: 47.2949\n",
      "Epoch 996/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7510.6592 - mean_absolute_error: 75.0515 - val_loss: 2991.3533 - val_mean_absolute_error: 45.5253\n",
      "Epoch 997/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6964.6304 - mean_absolute_error: 67.3982 - val_loss: 3178.2585 - val_mean_absolute_error: 47.3464\n",
      "Epoch 998/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7360.0801 - mean_absolute_error: 75.0801 - val_loss: 3956.1985 - val_mean_absolute_error: 53.1969\n",
      "Epoch 999/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5567.2026 - mean_absolute_error: 55.8458 - val_loss: 4711.4438 - val_mean_absolute_error: 57.7787\n",
      "Epoch 1000/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3691.8372 - mean_absolute_error: 51.9416 - val_loss: 5065.4136 - val_mean_absolute_error: 59.6935\n",
      "Epoch 1001/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3924.5625 - mean_absolute_error: 54.0365 - val_loss: 5230.5928 - val_mean_absolute_error: 60.6085\n",
      "Epoch 1002/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4245.5146 - mean_absolute_error: 51.8017 - val_loss: 4981.0137 - val_mean_absolute_error: 59.4207\n",
      "Epoch 1003/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5008.2944 - mean_absolute_error: 52.3558 - val_loss: 4285.0747 - val_mean_absolute_error: 55.5770\n",
      "Epoch 1004/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3472.3672 - mean_absolute_error: 50.9518 - val_loss: 3450.0112 - val_mean_absolute_error: 49.8945\n",
      "Epoch 1005/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5070.2881 - mean_absolute_error: 54.1603 - val_loss: 3058.1426 - val_mean_absolute_error: 46.3751\n",
      "Epoch 1006/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4298.0850 - mean_absolute_error: 51.6365 - val_loss: 2848.9124 - val_mean_absolute_error: 44.0900\n",
      "Epoch 1007/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4589.0371 - mean_absolute_error: 58.7946 - val_loss: 3043.8582 - val_mean_absolute_error: 46.0574\n",
      "Epoch 1008/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4707.1230 - mean_absolute_error: 59.3699 - val_loss: 3613.2434 - val_mean_absolute_error: 50.6763\n",
      "Epoch 1009/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 4726.6162 - mean_absolute_error: 58.1255 - val_loss: 4185.0508 - val_mean_absolute_error: 54.3387\n",
      "Epoch 1010/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6836.6030 - mean_absolute_error: 66.3592 - val_loss: 4381.6860 - val_mean_absolute_error: 55.4420\n",
      "Epoch 1011/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5285.0083 - mean_absolute_error: 64.3571 - val_loss: 4373.8350 - val_mean_absolute_error: 55.3497\n",
      "Epoch 1012/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3835.8638 - mean_absolute_error: 45.5235 - val_loss: 4387.7402 - val_mean_absolute_error: 55.3689\n",
      "Epoch 1013/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4648.9766 - mean_absolute_error: 58.9616 - val_loss: 4531.7290 - val_mean_absolute_error: 56.1827\n",
      "Epoch 1014/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5131.5225 - mean_absolute_error: 60.4010 - val_loss: 4491.7959 - val_mean_absolute_error: 55.9247\n",
      "Epoch 1015/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6655.7520 - mean_absolute_error: 62.6095 - val_loss: 4488.2080 - val_mean_absolute_error: 55.8715\n",
      "Epoch 1016/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5923.0078 - mean_absolute_error: 60.4003 - val_loss: 4231.6831 - val_mean_absolute_error: 54.3313\n",
      "Epoch 1017/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5979.8584 - mean_absolute_error: 70.6705 - val_loss: 4013.3911 - val_mean_absolute_error: 53.0048\n",
      "Epoch 1018/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5462.9268 - mean_absolute_error: 63.9599 - val_loss: 3687.8325 - val_mean_absolute_error: 50.8237\n",
      "Epoch 1019/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6092.5269 - mean_absolute_error: 67.9974 - val_loss: 3585.6055 - val_mean_absolute_error: 50.1412\n",
      "Epoch 1020/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5425.1191 - mean_absolute_error: 65.3095 - val_loss: 3945.0344 - val_mean_absolute_error: 52.6991\n",
      "Epoch 1021/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 7199.4736 - mean_absolute_error: 72.4026 - val_loss: 4730.1084 - val_mean_absolute_error: 57.3568\n",
      "Epoch 1022/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4936.1289 - mean_absolute_error: 57.2534 - val_loss: 5567.5269 - val_mean_absolute_error: 61.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1023/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6888.2642 - mean_absolute_error: 60.2184 - val_loss: 5470.3037 - val_mean_absolute_error: 61.2675\n",
      "Epoch 1024/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4215.8516 - mean_absolute_error: 52.2111 - val_loss: 5086.7993 - val_mean_absolute_error: 59.4634\n",
      "Epoch 1025/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4355.6641 - mean_absolute_error: 54.6294 - val_loss: 4524.9829 - val_mean_absolute_error: 56.5530\n",
      "Epoch 1026/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3829.5845 - mean_absolute_error: 49.3178 - val_loss: 3747.4644 - val_mean_absolute_error: 51.6789\n",
      "Epoch 1027/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4654.2549 - mean_absolute_error: 55.2338 - val_loss: 3055.3020 - val_mean_absolute_error: 45.9894\n",
      "Epoch 1028/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3547.9150 - mean_absolute_error: 49.7215 - val_loss: 2564.9219 - val_mean_absolute_error: 40.2526\n",
      "Epoch 1029/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2561.9014 - mean_absolute_error: 42.1224 - val_loss: 2352.0337 - val_mean_absolute_error: 39.2722\n",
      "Epoch 1030/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6769.8540 - mean_absolute_error: 69.2009 - val_loss: 2617.1216 - val_mean_absolute_error: 40.9305\n",
      "Epoch 1031/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3868.9216 - mean_absolute_error: 45.1291 - val_loss: 3461.9263 - val_mean_absolute_error: 49.0307\n",
      "Epoch 1032/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5323.1367 - mean_absolute_error: 60.5739 - val_loss: 4880.8423 - val_mean_absolute_error: 57.7326\n",
      "Epoch 1033/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6407.9775 - mean_absolute_error: 69.1491 - val_loss: 6151.1064 - val_mean_absolute_error: 63.7698\n",
      "Epoch 1034/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5737.3262 - mean_absolute_error: 58.9879 - val_loss: 6952.6304 - val_mean_absolute_error: 67.1631\n",
      "Epoch 1035/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7781.1455 - mean_absolute_error: 66.7962 - val_loss: 6979.1016 - val_mean_absolute_error: 67.2412\n",
      "Epoch 1036/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5320.3896 - mean_absolute_error: 59.6881 - val_loss: 6493.6411 - val_mean_absolute_error: 65.1517\n",
      "Epoch 1037/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 9019.9004 - mean_absolute_error: 72.2483 - val_loss: 5675.5752 - val_mean_absolute_error: 61.3378\n",
      "Epoch 1038/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5244.1934 - mean_absolute_error: 60.0841 - val_loss: 4766.3271 - val_mean_absolute_error: 56.5503\n",
      "Epoch 1039/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7054.7900 - mean_absolute_error: 71.9335 - val_loss: 3865.2102 - val_mean_absolute_error: 50.9144\n",
      "Epoch 1040/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4505.0332 - mean_absolute_error: 54.4347 - val_loss: 3301.7012 - val_mean_absolute_error: 46.6315\n",
      "Epoch 1041/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5105.0732 - mean_absolute_error: 56.8657 - val_loss: 3089.0725 - val_mean_absolute_error: 44.7587\n",
      "Epoch 1042/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3306.0679 - mean_absolute_error: 46.9883 - val_loss: 3085.9790 - val_mean_absolute_error: 44.6265\n",
      "Epoch 1043/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5620.4238 - mean_absolute_error: 65.6943 - val_loss: 3346.6399 - val_mean_absolute_error: 46.8729\n",
      "Epoch 1044/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3737.0952 - mean_absolute_error: 47.1220 - val_loss: 3691.6301 - val_mean_absolute_error: 49.6852\n",
      "Epoch 1045/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6079.8730 - mean_absolute_error: 64.4144 - val_loss: 4238.8037 - val_mean_absolute_error: 53.5129\n",
      "Epoch 1046/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8536.8750 - mean_absolute_error: 72.3847 - val_loss: 4813.0518 - val_mean_absolute_error: 56.9597\n",
      "Epoch 1047/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5673.0547 - mean_absolute_error: 62.9973 - val_loss: 5100.0596 - val_mean_absolute_error: 58.5456\n",
      "Epoch 1048/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3926.1155 - mean_absolute_error: 55.0265 - val_loss: 5077.6367 - val_mean_absolute_error: 58.4647\n",
      "Epoch 1049/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5245.7910 - mean_absolute_error: 55.7194 - val_loss: 4735.4150 - val_mean_absolute_error: 56.6211\n",
      "Epoch 1050/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5593.1226 - mean_absolute_error: 66.8037 - val_loss: 4271.9736 - val_mean_absolute_error: 53.8868\n",
      "Epoch 1051/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5856.3330 - mean_absolute_error: 63.9482 - val_loss: 3865.4087 - val_mean_absolute_error: 51.1412\n",
      "Epoch 1052/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5438.8477 - mean_absolute_error: 64.4191 - val_loss: 3525.5286 - val_mean_absolute_error: 48.5060\n",
      "Epoch 1053/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6831.8516 - mean_absolute_error: 67.7486 - val_loss: 3494.7500 - val_mean_absolute_error: 48.2787\n",
      "Epoch 1054/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4848.2754 - mean_absolute_error: 57.0328 - val_loss: 3411.7200 - val_mean_absolute_error: 47.5824\n",
      "Epoch 1055/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4784.3672 - mean_absolute_error: 51.3514 - val_loss: 3409.9062 - val_mean_absolute_error: 47.5632\n",
      "Epoch 1056/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6980.1777 - mean_absolute_error: 65.9358 - val_loss: 3721.4448 - val_mean_absolute_error: 50.1218\n",
      "Epoch 1057/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7157.2163 - mean_absolute_error: 67.6750 - val_loss: 4094.0906 - val_mean_absolute_error: 52.7464\n",
      "Epoch 1058/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5776.3105 - mean_absolute_error: 61.3736 - val_loss: 4467.3252 - val_mean_absolute_error: 55.0818\n",
      "Epoch 1059/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6399.2178 - mean_absolute_error: 67.2133 - val_loss: 4565.9209 - val_mean_absolute_error: 55.6825\n",
      "Epoch 1060/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5393.8496 - mean_absolute_error: 63.9884 - val_loss: 4521.3154 - val_mean_absolute_error: 55.4401\n",
      "Epoch 1061/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5780.8984 - mean_absolute_error: 62.7694 - val_loss: 4381.4761 - val_mean_absolute_error: 54.6361\n",
      "Epoch 1062/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5058.9590 - mean_absolute_error: 61.3928 - val_loss: 4115.9844 - val_mean_absolute_error: 52.9596\n",
      "Epoch 1063/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4542.7871 - mean_absolute_error: 55.6460 - val_loss: 3843.1392 - val_mean_absolute_error: 51.1742\n",
      "Epoch 1064/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4243.3984 - mean_absolute_error: 57.7561 - val_loss: 3713.7573 - val_mean_absolute_error: 50.2271\n",
      "Epoch 1065/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5144.7114 - mean_absolute_error: 55.0839 - val_loss: 3324.2944 - val_mean_absolute_error: 46.9872\n",
      "Epoch 1066/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3896.4536 - mean_absolute_error: 50.1927 - val_loss: 2996.8020 - val_mean_absolute_error: 43.7240\n",
      "Epoch 1067/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5484.3989 - mean_absolute_error: 67.1962 - val_loss: 3061.5500 - val_mean_absolute_error: 44.4133\n",
      "Epoch 1068/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4358.5557 - mean_absolute_error: 56.0939 - val_loss: 3523.7922 - val_mean_absolute_error: 48.5842\n",
      "Epoch 1069/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4806.9688 - mean_absolute_error: 62.6665 - val_loss: 4303.3530 - val_mean_absolute_error: 54.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1070/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4238.7026 - mean_absolute_error: 49.3048 - val_loss: 4784.3545 - val_mean_absolute_error: 56.8021\n",
      "Epoch 1071/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7195.5674 - mean_absolute_error: 62.7699 - val_loss: 4905.1587 - val_mean_absolute_error: 57.4319\n",
      "Epoch 1072/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6289.2578 - mean_absolute_error: 63.3848 - val_loss: 4814.9014 - val_mean_absolute_error: 56.9499\n",
      "Epoch 1073/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7025.8262 - mean_absolute_error: 70.2215 - val_loss: 4681.6162 - val_mean_absolute_error: 56.2819\n",
      "Epoch 1074/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5992.8574 - mean_absolute_error: 65.1556 - val_loss: 4401.0771 - val_mean_absolute_error: 54.7445\n",
      "Epoch 1075/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3040.1611 - mean_absolute_error: 49.0181 - val_loss: 4101.9224 - val_mean_absolute_error: 52.9003\n",
      "Epoch 1076/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5713.9229 - mean_absolute_error: 64.5779 - val_loss: 4090.4836 - val_mean_absolute_error: 52.8233\n",
      "Epoch 1077/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5650.5625 - mean_absolute_error: 63.0723 - val_loss: 4011.6492 - val_mean_absolute_error: 52.3530\n",
      "Epoch 1078/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5134.0176 - mean_absolute_error: 55.6306 - val_loss: 3857.1440 - val_mean_absolute_error: 51.4256\n",
      "Epoch 1079/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4127.2266 - mean_absolute_error: 55.1970 - val_loss: 3650.6772 - val_mean_absolute_error: 50.0193\n",
      "Epoch 1080/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4055.3030 - mean_absolute_error: 54.5264 - val_loss: 3523.8442 - val_mean_absolute_error: 49.0872\n",
      "Epoch 1081/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4385.7856 - mean_absolute_error: 54.7987 - val_loss: 3457.9258 - val_mean_absolute_error: 48.6574\n",
      "Epoch 1082/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3139.3643 - mean_absolute_error: 40.5603 - val_loss: 3519.0977 - val_mean_absolute_error: 49.1152\n",
      "Epoch 1083/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3930.8433 - mean_absolute_error: 52.9634 - val_loss: 3672.7200 - val_mean_absolute_error: 50.1799\n",
      "Epoch 1084/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4194.0425 - mean_absolute_error: 49.3971 - val_loss: 3874.2827 - val_mean_absolute_error: 51.4483\n",
      "Epoch 1085/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4326.9556 - mean_absolute_error: 47.7675 - val_loss: 4357.8691 - val_mean_absolute_error: 54.5341\n",
      "Epoch 1086/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5429.2988 - mean_absolute_error: 61.6375 - val_loss: 5097.8643 - val_mean_absolute_error: 58.6412\n",
      "Epoch 1087/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5265.7510 - mean_absolute_error: 56.7070 - val_loss: 5163.6260 - val_mean_absolute_error: 59.0217\n",
      "Epoch 1088/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3270.4546 - mean_absolute_error: 50.5955 - val_loss: 5032.8691 - val_mean_absolute_error: 58.3831\n",
      "Epoch 1089/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6062.5801 - mean_absolute_error: 64.9716 - val_loss: 4300.9175 - val_mean_absolute_error: 54.3536\n",
      "Epoch 1090/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7467.2451 - mean_absolute_error: 68.6557 - val_loss: 3664.9185 - val_mean_absolute_error: 50.0453\n",
      "Epoch 1091/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6817.8618 - mean_absolute_error: 74.5706 - val_loss: 3539.4727 - val_mean_absolute_error: 49.0483\n",
      "Epoch 1092/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4157.8994 - mean_absolute_error: 48.9089 - val_loss: 3310.3745 - val_mean_absolute_error: 47.3013\n",
      "Epoch 1093/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5295.9082 - mean_absolute_error: 60.1986 - val_loss: 2968.2761 - val_mean_absolute_error: 44.2569\n",
      "Epoch 1094/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2702.0537 - mean_absolute_error: 45.0992 - val_loss: 2701.8486 - val_mean_absolute_error: 41.2126\n",
      "Epoch 1095/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5593.3862 - mean_absolute_error: 61.8804 - val_loss: 3249.9946 - val_mean_absolute_error: 46.7664\n",
      "Epoch 1096/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5304.1606 - mean_absolute_error: 51.6891 - val_loss: 4448.3867 - val_mean_absolute_error: 54.9461\n",
      "Epoch 1097/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4385.2773 - mean_absolute_error: 57.8170 - val_loss: 5572.3999 - val_mean_absolute_error: 60.7919\n",
      "Epoch 1098/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5476.1172 - mean_absolute_error: 57.5935 - val_loss: 6090.0566 - val_mean_absolute_error: 63.1528\n",
      "Epoch 1099/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5433.7261 - mean_absolute_error: 65.2192 - val_loss: 6147.7349 - val_mean_absolute_error: 63.3550\n",
      "Epoch 1100/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4262.4004 - mean_absolute_error: 49.6498 - val_loss: 5813.0449 - val_mean_absolute_error: 61.7004\n",
      "Epoch 1101/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5367.5723 - mean_absolute_error: 58.7074 - val_loss: 5189.5815 - val_mean_absolute_error: 58.5082\n",
      "Epoch 1102/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7175.9624 - mean_absolute_error: 66.1929 - val_loss: 4338.7549 - val_mean_absolute_error: 53.5771\n",
      "Epoch 1103/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6057.5107 - mean_absolute_error: 65.2027 - val_loss: 3648.2012 - val_mean_absolute_error: 48.6905\n",
      "Epoch 1104/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5393.4990 - mean_absolute_error: 61.3749 - val_loss: 3051.4426 - val_mean_absolute_error: 43.3240\n",
      "Epoch 1105/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5412.9336 - mean_absolute_error: 57.8308 - val_loss: 2754.4487 - val_mean_absolute_error: 40.9944\n",
      "Epoch 1106/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6191.1641 - mean_absolute_error: 68.0012 - val_loss: 2742.2739 - val_mean_absolute_error: 40.8743\n",
      "Epoch 1107/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6672.2578 - mean_absolute_error: 69.2383 - val_loss: 2867.1218 - val_mean_absolute_error: 41.5074\n",
      "Epoch 1108/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5489.9810 - mean_absolute_error: 60.4300 - val_loss: 3367.7749 - val_mean_absolute_error: 46.6424\n",
      "Epoch 1109/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3413.5034 - mean_absolute_error: 52.8538 - val_loss: 3899.0554 - val_mean_absolute_error: 51.1129\n",
      "Epoch 1110/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4086.3967 - mean_absolute_error: 46.9797 - val_loss: 4397.0405 - val_mean_absolute_error: 54.6854\n",
      "Epoch 1111/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5726.3169 - mean_absolute_error: 66.0119 - val_loss: 4762.7275 - val_mean_absolute_error: 57.1277\n",
      "Epoch 1112/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3702.8171 - mean_absolute_error: 50.6483 - val_loss: 4856.4531 - val_mean_absolute_error: 57.8693\n",
      "Epoch 1113/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7406.1948 - mean_absolute_error: 74.5996 - val_loss: 4563.3936 - val_mean_absolute_error: 56.4466\n",
      "Epoch 1114/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5400.3784 - mean_absolute_error: 65.1477 - val_loss: 4243.2744 - val_mean_absolute_error: 54.6883\n",
      "Epoch 1115/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5865.0835 - mean_absolute_error: 63.9679 - val_loss: 3897.3503 - val_mean_absolute_error: 52.5401\n",
      "Epoch 1116/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7716.4121 - mean_absolute_error: 73.1150 - val_loss: 3594.3945 - val_mean_absolute_error: 50.3524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1117/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5450.9487 - mean_absolute_error: 59.3863 - val_loss: 3485.3276 - val_mean_absolute_error: 49.4620\n",
      "Epoch 1118/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8075.0034 - mean_absolute_error: 70.3368 - val_loss: 3532.7488 - val_mean_absolute_error: 49.8522\n",
      "Epoch 1119/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4158.8320 - mean_absolute_error: 54.7376 - val_loss: 3762.8159 - val_mean_absolute_error: 51.6001\n",
      "Epoch 1120/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4334.8232 - mean_absolute_error: 57.7278 - val_loss: 4213.4536 - val_mean_absolute_error: 54.5597\n",
      "Epoch 1121/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6245.2148 - mean_absolute_error: 62.4049 - val_loss: 4524.7388 - val_mean_absolute_error: 56.3302\n",
      "Epoch 1122/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3643.9893 - mean_absolute_error: 51.2623 - val_loss: 4541.0410 - val_mean_absolute_error: 56.3255\n",
      "Epoch 1123/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3639.4961 - mean_absolute_error: 50.8728 - val_loss: 4365.3428 - val_mean_absolute_error: 55.0973\n",
      "Epoch 1124/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7046.3652 - mean_absolute_error: 72.1544 - val_loss: 4025.1750 - val_mean_absolute_error: 52.6853\n",
      "Epoch 1125/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6914.6001 - mean_absolute_error: 60.4618 - val_loss: 3659.2942 - val_mean_absolute_error: 49.8229\n",
      "Epoch 1126/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3067.0933 - mean_absolute_error: 45.9915 - val_loss: 3366.6589 - val_mean_absolute_error: 47.2608\n",
      "Epoch 1127/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4675.6045 - mean_absolute_error: 58.0668 - val_loss: 3368.3413 - val_mean_absolute_error: 47.0128\n",
      "Epoch 1128/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4433.7607 - mean_absolute_error: 56.5484 - val_loss: 3444.3628 - val_mean_absolute_error: 47.3374\n",
      "Epoch 1129/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3277.1240 - mean_absolute_error: 41.6697 - val_loss: 3515.4556 - val_mean_absolute_error: 47.6137\n",
      "Epoch 1130/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3708.6328 - mean_absolute_error: 48.3222 - val_loss: 4055.8267 - val_mean_absolute_error: 51.8984\n",
      "Epoch 1131/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4354.7769 - mean_absolute_error: 50.1537 - val_loss: 4356.4502 - val_mean_absolute_error: 54.1138\n",
      "Epoch 1132/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3934.1504 - mean_absolute_error: 50.2750 - val_loss: 4068.4890 - val_mean_absolute_error: 52.5636\n",
      "Epoch 1133/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4793.5156 - mean_absolute_error: 58.4919 - val_loss: 3847.7864 - val_mean_absolute_error: 51.1919\n",
      "Epoch 1134/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5441.0972 - mean_absolute_error: 59.9215 - val_loss: 3586.8105 - val_mean_absolute_error: 49.2220\n",
      "Epoch 1135/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3060.8345 - mean_absolute_error: 47.1950 - val_loss: 3524.3828 - val_mean_absolute_error: 48.5770\n",
      "Epoch 1136/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3457.2451 - mean_absolute_error: 49.4027 - val_loss: 3530.7051 - val_mean_absolute_error: 48.4769\n",
      "Epoch 1137/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4772.3325 - mean_absolute_error: 51.8507 - val_loss: 3953.0891 - val_mean_absolute_error: 51.4536\n",
      "Epoch 1138/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4473.7021 - mean_absolute_error: 56.4425 - val_loss: 4373.3496 - val_mean_absolute_error: 54.2064\n",
      "Epoch 1139/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5641.1504 - mean_absolute_error: 56.8950 - val_loss: 4770.6387 - val_mean_absolute_error: 56.8103\n",
      "Epoch 1140/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7399.0381 - mean_absolute_error: 73.9326 - val_loss: 4748.3862 - val_mean_absolute_error: 56.9526\n",
      "Epoch 1141/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2805.0984 - mean_absolute_error: 43.8308 - val_loss: 4485.6128 - val_mean_absolute_error: 55.7333\n",
      "Epoch 1142/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3233.5518 - mean_absolute_error: 44.0785 - val_loss: 4138.4692 - val_mean_absolute_error: 54.0196\n",
      "Epoch 1143/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3696.2998 - mean_absolute_error: 50.4366 - val_loss: 3559.0110 - val_mean_absolute_error: 50.0354\n",
      "Epoch 1144/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4516.5093 - mean_absolute_error: 57.6932 - val_loss: 3411.6199 - val_mean_absolute_error: 48.8519\n",
      "Epoch 1145/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3278.2078 - mean_absolute_error: 45.2921 - val_loss: 3253.0347 - val_mean_absolute_error: 47.2647\n",
      "Epoch 1146/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6430.0103 - mean_absolute_error: 66.7679 - val_loss: 3546.7739 - val_mean_absolute_error: 50.0116\n",
      "Epoch 1147/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5665.6543 - mean_absolute_error: 66.9033 - val_loss: 4155.7847 - val_mean_absolute_error: 54.3126\n",
      "Epoch 1148/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6414.4717 - mean_absolute_error: 67.6377 - val_loss: 4920.2119 - val_mean_absolute_error: 58.6742\n",
      "Epoch 1149/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4646.7363 - mean_absolute_error: 55.2832 - val_loss: 5542.1230 - val_mean_absolute_error: 61.6439\n",
      "Epoch 1150/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5121.5830 - mean_absolute_error: 64.2263 - val_loss: 5722.7329 - val_mean_absolute_error: 62.2829\n",
      "Epoch 1151/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5404.7959 - mean_absolute_error: 60.4246 - val_loss: 5341.2729 - val_mean_absolute_error: 59.9776\n",
      "Epoch 1152/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4987.2422 - mean_absolute_error: 57.1007 - val_loss: 4551.9614 - val_mean_absolute_error: 54.9757\n",
      "Epoch 1153/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3667.8086 - mean_absolute_error: 46.5040 - val_loss: 3650.5840 - val_mean_absolute_error: 48.0040\n",
      "Epoch 1154/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5422.9746 - mean_absolute_error: 60.7275 - val_loss: 3004.2769 - val_mean_absolute_error: 42.6686\n",
      "Epoch 1155/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6099.1792 - mean_absolute_error: 49.0206 - val_loss: 2895.5308 - val_mean_absolute_error: 42.2122\n",
      "Epoch 1156/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4684.9746 - mean_absolute_error: 58.0749 - val_loss: 3253.7356 - val_mean_absolute_error: 45.7091\n",
      "Epoch 1157/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4763.4971 - mean_absolute_error: 59.0440 - val_loss: 3792.8992 - val_mean_absolute_error: 51.3858\n",
      "Epoch 1158/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7622.1768 - mean_absolute_error: 73.4292 - val_loss: 4013.5352 - val_mean_absolute_error: 53.8922\n",
      "Epoch 1159/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3971.5762 - mean_absolute_error: 50.0179 - val_loss: 4388.5830 - val_mean_absolute_error: 56.7859\n",
      "Epoch 1160/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3650.4414 - mean_absolute_error: 48.7683 - val_loss: 4696.7275 - val_mean_absolute_error: 58.8450\n",
      "Epoch 1161/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5559.3569 - mean_absolute_error: 58.9509 - val_loss: 4889.9497 - val_mean_absolute_error: 60.0530\n",
      "Epoch 1162/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4934.0000 - mean_absolute_error: 58.4149 - val_loss: 4852.5103 - val_mean_absolute_error: 59.9239\n",
      "Epoch 1163/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4554.8765 - mean_absolute_error: 51.3509 - val_loss: 4377.9722 - val_mean_absolute_error: 57.2349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1164/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6611.0566 - mean_absolute_error: 62.8155 - val_loss: 3886.0164 - val_mean_absolute_error: 53.8256\n",
      "Epoch 1165/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2813.0881 - mean_absolute_error: 44.0078 - val_loss: 3505.3145 - val_mean_absolute_error: 50.4165\n",
      "Epoch 1166/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4631.7080 - mean_absolute_error: 55.3896 - val_loss: 3339.5737 - val_mean_absolute_error: 48.6621\n",
      "Epoch 1167/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3792.8196 - mean_absolute_error: 55.2031 - val_loss: 3043.0637 - val_mean_absolute_error: 45.5496\n",
      "Epoch 1168/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5779.7866 - mean_absolute_error: 61.3105 - val_loss: 2858.0991 - val_mean_absolute_error: 43.2716\n",
      "Epoch 1169/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5561.9253 - mean_absolute_error: 55.0557 - val_loss: 3261.2161 - val_mean_absolute_error: 46.6356\n",
      "Epoch 1170/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2919.2012 - mean_absolute_error: 42.5089 - val_loss: 3631.7886 - val_mean_absolute_error: 49.4657\n",
      "Epoch 1171/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3593.3491 - mean_absolute_error: 48.8844 - val_loss: 4288.8169 - val_mean_absolute_error: 53.8606\n",
      "Epoch 1172/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4253.8950 - mean_absolute_error: 54.2422 - val_loss: 4526.6865 - val_mean_absolute_error: 55.5436\n",
      "Epoch 1173/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3506.2256 - mean_absolute_error: 50.3559 - val_loss: 4568.7578 - val_mean_absolute_error: 56.0672\n",
      "Epoch 1174/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5536.7520 - mean_absolute_error: 64.9770 - val_loss: 4289.8472 - val_mean_absolute_error: 54.7462\n",
      "Epoch 1175/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6857.6250 - mean_absolute_error: 69.4870 - val_loss: 3866.6782 - val_mean_absolute_error: 52.2029\n",
      "Epoch 1176/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4361.7222 - mean_absolute_error: 57.3111 - val_loss: 3441.5464 - val_mean_absolute_error: 48.9901\n",
      "Epoch 1177/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5127.2051 - mean_absolute_error: 58.0467 - val_loss: 3344.3501 - val_mean_absolute_error: 48.1132\n",
      "Epoch 1178/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4371.2500 - mean_absolute_error: 56.4773 - val_loss: 3353.7981 - val_mean_absolute_error: 48.1806\n",
      "Epoch 1179/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4450.5298 - mean_absolute_error: 53.7393 - val_loss: 3696.6992 - val_mean_absolute_error: 51.2193\n",
      "Epoch 1180/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4081.6707 - mean_absolute_error: 57.0552 - val_loss: 4035.6582 - val_mean_absolute_error: 53.5193\n",
      "Epoch 1181/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3313.5234 - mean_absolute_error: 45.5103 - val_loss: 4451.5581 - val_mean_absolute_error: 55.5442\n",
      "Epoch 1182/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3517.5518 - mean_absolute_error: 46.8429 - val_loss: 4769.0459 - val_mean_absolute_error: 56.4781\n",
      "Epoch 1183/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5613.2378 - mean_absolute_error: 59.4037 - val_loss: 4851.2397 - val_mean_absolute_error: 56.3002\n",
      "Epoch 1184/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2730.6450 - mean_absolute_error: 41.1020 - val_loss: 4634.0991 - val_mean_absolute_error: 54.4815\n",
      "Epoch 1185/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3510.4478 - mean_absolute_error: 48.2324 - val_loss: 4418.3477 - val_mean_absolute_error: 53.0998\n",
      "Epoch 1186/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 1752.8452 - mean_absolute_error: 33.9849 - val_loss: 3968.0698 - val_mean_absolute_error: 50.2463\n",
      "Epoch 1187/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 8068.8560 - mean_absolute_error: 65.1827 - val_loss: 3399.8784 - val_mean_absolute_error: 46.2859\n",
      "Epoch 1188/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5072.5845 - mean_absolute_error: 48.1139 - val_loss: 3631.9949 - val_mean_absolute_error: 49.0809\n",
      "Epoch 1189/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3731.0374 - mean_absolute_error: 50.6676 - val_loss: 4067.6714 - val_mean_absolute_error: 53.2444\n",
      "Epoch 1190/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2584.6133 - mean_absolute_error: 41.5244 - val_loss: 4348.3550 - val_mean_absolute_error: 55.8644\n",
      "Epoch 1191/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6379.2803 - mean_absolute_error: 67.6459 - val_loss: 4244.4922 - val_mean_absolute_error: 55.7334\n",
      "Epoch 1192/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5569.3750 - mean_absolute_error: 63.1323 - val_loss: 4219.3179 - val_mean_absolute_error: 55.7890\n",
      "Epoch 1193/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4726.3970 - mean_absolute_error: 56.5333 - val_loss: 4154.7754 - val_mean_absolute_error: 55.4740\n",
      "Epoch 1194/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3294.5996 - mean_absolute_error: 48.7820 - val_loss: 4260.5459 - val_mean_absolute_error: 56.1405\n",
      "Epoch 1195/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3760.2212 - mean_absolute_error: 51.8180 - val_loss: 4416.7612 - val_mean_absolute_error: 56.8318\n",
      "Epoch 1196/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4246.9209 - mean_absolute_error: 56.4367 - val_loss: 4384.5542 - val_mean_absolute_error: 56.2411\n",
      "Epoch 1197/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6948.1738 - mean_absolute_error: 65.7961 - val_loss: 4104.6899 - val_mean_absolute_error: 53.5731\n",
      "Epoch 1198/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4948.0352 - mean_absolute_error: 54.4135 - val_loss: 3695.2476 - val_mean_absolute_error: 49.6566\n",
      "Epoch 1199/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3629.9690 - mean_absolute_error: 44.5795 - val_loss: 3435.7681 - val_mean_absolute_error: 47.2003\n",
      "Epoch 1200/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3020.7800 - mean_absolute_error: 42.1735 - val_loss: 3448.2827 - val_mean_absolute_error: 47.0850\n",
      "Epoch 1201/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5820.1875 - mean_absolute_error: 60.2726 - val_loss: 3987.1411 - val_mean_absolute_error: 51.7871\n",
      "Epoch 1202/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5345.0435 - mean_absolute_error: 61.9603 - val_loss: 4206.0537 - val_mean_absolute_error: 53.5745\n",
      "Epoch 1203/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4951.5713 - mean_absolute_error: 55.5761 - val_loss: 4022.4910 - val_mean_absolute_error: 52.5010\n",
      "Epoch 1204/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4032.8408 - mean_absolute_error: 53.2036 - val_loss: 3993.0476 - val_mean_absolute_error: 53.0359\n",
      "Epoch 1205/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5958.4849 - mean_absolute_error: 64.3324 - val_loss: 3967.7637 - val_mean_absolute_error: 53.3027\n",
      "Epoch 1206/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4869.2021 - mean_absolute_error: 57.0997 - val_loss: 4051.8738 - val_mean_absolute_error: 54.2285\n",
      "Epoch 1207/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4572.1519 - mean_absolute_error: 56.7978 - val_loss: 3984.3608 - val_mean_absolute_error: 53.7217\n",
      "Epoch 1208/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5274.6816 - mean_absolute_error: 53.4639 - val_loss: 3846.1919 - val_mean_absolute_error: 52.3565\n",
      "Epoch 1209/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4834.5547 - mean_absolute_error: 63.6625 - val_loss: 3847.5161 - val_mean_absolute_error: 52.4237\n",
      "Epoch 1210/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6345.9287 - mean_absolute_error: 62.3420 - val_loss: 3592.9019 - val_mean_absolute_error: 50.7572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1211/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3550.0039 - mean_absolute_error: 49.1532 - val_loss: 3552.2271 - val_mean_absolute_error: 50.7215\n",
      "Epoch 1212/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5186.5957 - mean_absolute_error: 54.4604 - val_loss: 4398.1069 - val_mean_absolute_error: 56.9798\n",
      "Epoch 1213/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4486.1221 - mean_absolute_error: 55.3627 - val_loss: 4812.9688 - val_mean_absolute_error: 59.9048\n",
      "Epoch 1214/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4886.7305 - mean_absolute_error: 57.1871 - val_loss: 5222.6162 - val_mean_absolute_error: 62.3088\n",
      "Epoch 1215/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3673.7544 - mean_absolute_error: 46.5914 - val_loss: 5476.5703 - val_mean_absolute_error: 63.5145\n",
      "Epoch 1216/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4892.9072 - mean_absolute_error: 56.7272 - val_loss: 5304.8076 - val_mean_absolute_error: 62.7181\n",
      "Epoch 1217/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4624.4121 - mean_absolute_error: 54.8071 - val_loss: 4764.4385 - val_mean_absolute_error: 60.1403\n",
      "Epoch 1218/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5849.2451 - mean_absolute_error: 60.8342 - val_loss: 4331.8672 - val_mean_absolute_error: 57.3518\n",
      "Epoch 1219/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4409.8965 - mean_absolute_error: 57.0103 - val_loss: 4343.4888 - val_mean_absolute_error: 56.9782\n",
      "Epoch 1220/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6362.8203 - mean_absolute_error: 62.4502 - val_loss: 4628.0132 - val_mean_absolute_error: 58.5237\n",
      "Epoch 1221/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3846.5225 - mean_absolute_error: 51.9287 - val_loss: 4681.6304 - val_mean_absolute_error: 58.4983\n",
      "Epoch 1222/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5134.2310 - mean_absolute_error: 53.7717 - val_loss: 4887.1108 - val_mean_absolute_error: 59.4413\n",
      "Epoch 1223/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3291.9548 - mean_absolute_error: 45.4473 - val_loss: 5243.5049 - val_mean_absolute_error: 61.2010\n",
      "Epoch 1224/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5455.8193 - mean_absolute_error: 61.3222 - val_loss: 5562.2388 - val_mean_absolute_error: 62.6078\n",
      "Epoch 1225/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4083.8047 - mean_absolute_error: 51.7424 - val_loss: 5547.4033 - val_mean_absolute_error: 62.4698\n",
      "Epoch 1226/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3402.7351 - mean_absolute_error: 46.5674 - val_loss: 5306.5146 - val_mean_absolute_error: 61.9061\n",
      "Epoch 1227/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4300.4277 - mean_absolute_error: 49.6440 - val_loss: 4652.7134 - val_mean_absolute_error: 58.4136\n",
      "Epoch 1228/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2585.8589 - mean_absolute_error: 42.8373 - val_loss: 4110.6992 - val_mean_absolute_error: 54.5563\n",
      "Epoch 1229/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2160.6270 - mean_absolute_error: 36.3740 - val_loss: 3390.6587 - val_mean_absolute_error: 48.9145\n",
      "Epoch 1230/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6638.6938 - mean_absolute_error: 58.9904 - val_loss: 3815.1750 - val_mean_absolute_error: 52.9827\n",
      "Epoch 1231/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3228.8433 - mean_absolute_error: 51.1583 - val_loss: 4331.2803 - val_mean_absolute_error: 57.3492\n",
      "Epoch 1232/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3148.7498 - mean_absolute_error: 44.0075 - val_loss: 4899.7515 - val_mean_absolute_error: 60.7833\n",
      "Epoch 1233/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5687.0664 - mean_absolute_error: 64.5267 - val_loss: 5362.7368 - val_mean_absolute_error: 63.3637\n",
      "Epoch 1234/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3107.6833 - mean_absolute_error: 47.3695 - val_loss: 5706.7778 - val_mean_absolute_error: 64.8265\n",
      "Epoch 1235/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4698.6699 - mean_absolute_error: 49.8130 - val_loss: 5774.1152 - val_mean_absolute_error: 64.2131\n",
      "Epoch 1236/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5552.7686 - mean_absolute_error: 62.5123 - val_loss: 5241.5098 - val_mean_absolute_error: 59.9149\n",
      "Epoch 1237/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2553.0425 - mean_absolute_error: 41.6673 - val_loss: 4323.8267 - val_mean_absolute_error: 52.3712\n",
      "Epoch 1238/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3396.6855 - mean_absolute_error: 48.4006 - val_loss: 3698.0938 - val_mean_absolute_error: 48.5565\n",
      "Epoch 1239/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3681.8003 - mean_absolute_error: 47.2611 - val_loss: 3893.5925 - val_mean_absolute_error: 51.5768\n",
      "Epoch 1240/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 1991.6632 - mean_absolute_error: 38.4977 - val_loss: 3901.2261 - val_mean_absolute_error: 52.7419\n",
      "Epoch 1241/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 5262.9370 - mean_absolute_error: 61.7617 - val_loss: 4000.5259 - val_mean_absolute_error: 54.0035\n",
      "Epoch 1242/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5659.9443 - mean_absolute_error: 59.2721 - val_loss: 4192.6753 - val_mean_absolute_error: 55.6442\n",
      "Epoch 1243/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5485.1621 - mean_absolute_error: 60.8039 - val_loss: 4634.8799 - val_mean_absolute_error: 58.2931\n",
      "Epoch 1244/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2965.8369 - mean_absolute_error: 43.2656 - val_loss: 4959.8223 - val_mean_absolute_error: 61.4141\n",
      "Epoch 1245/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7686.0259 - mean_absolute_error: 66.7168 - val_loss: 5031.7515 - val_mean_absolute_error: 61.9181\n",
      "Epoch 1246/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2149.3059 - mean_absolute_error: 37.8035 - val_loss: 5048.0020 - val_mean_absolute_error: 62.0095\n",
      "Epoch 1247/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5744.9355 - mean_absolute_error: 63.6348 - val_loss: 4945.6597 - val_mean_absolute_error: 61.3787\n",
      "Epoch 1248/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1641.6068 - mean_absolute_error: 33.9035 - val_loss: 4934.9409 - val_mean_absolute_error: 60.4818\n",
      "Epoch 1249/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3306.2683 - mean_absolute_error: 45.7915 - val_loss: 4491.7979 - val_mean_absolute_error: 56.8518\n",
      "Epoch 1250/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2902.5020 - mean_absolute_error: 43.5718 - val_loss: 3974.1543 - val_mean_absolute_error: 53.0813\n",
      "Epoch 1251/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3999.0068 - mean_absolute_error: 53.0800 - val_loss: 3416.9023 - val_mean_absolute_error: 48.6196\n",
      "Epoch 1252/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3245.1008 - mean_absolute_error: 50.3074 - val_loss: 3591.4395 - val_mean_absolute_error: 51.5700\n",
      "Epoch 1253/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4755.3770 - mean_absolute_error: 58.4514 - val_loss: 4416.2114 - val_mean_absolute_error: 58.5438\n",
      "Epoch 1254/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5049.7920 - mean_absolute_error: 57.4816 - val_loss: 5443.0308 - val_mean_absolute_error: 64.4466\n",
      "Epoch 1255/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6808.5195 - mean_absolute_error: 65.6483 - val_loss: 6584.7866 - val_mean_absolute_error: 68.8501\n",
      "Epoch 1256/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5197.9028 - mean_absolute_error: 62.7782 - val_loss: 7163.9521 - val_mean_absolute_error: 70.0997\n",
      "Epoch 1257/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5586.6021 - mean_absolute_error: 58.9941 - val_loss: 6902.9180 - val_mean_absolute_error: 65.3212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1258/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6920.4463 - mean_absolute_error: 69.0400 - val_loss: 5280.2559 - val_mean_absolute_error: 54.1626\n",
      "Epoch 1259/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3710.3862 - mean_absolute_error: 47.7403 - val_loss: 3967.6128 - val_mean_absolute_error: 46.3529\n",
      "Epoch 1260/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4564.4180 - mean_absolute_error: 59.4036 - val_loss: 3516.9355 - val_mean_absolute_error: 46.2736\n",
      "Epoch 1261/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2933.0071 - mean_absolute_error: 46.4012 - val_loss: 3694.0254 - val_mean_absolute_error: 48.6353\n",
      "Epoch 1262/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4789.0273 - mean_absolute_error: 58.7013 - val_loss: 4467.5967 - val_mean_absolute_error: 57.0647\n",
      "Epoch 1263/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3492.7910 - mean_absolute_error: 47.9276 - val_loss: 4850.7588 - val_mean_absolute_error: 60.6123\n",
      "Epoch 1264/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 5093.8643 - mean_absolute_error: 57.5800 - val_loss: 5077.4648 - val_mean_absolute_error: 61.8124\n",
      "Epoch 1265/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3339.0503 - mean_absolute_error: 51.2913 - val_loss: 5209.3145 - val_mean_absolute_error: 62.0798\n",
      "Epoch 1266/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2918.3374 - mean_absolute_error: 44.2765 - val_loss: 4946.7988 - val_mean_absolute_error: 59.8837\n",
      "Epoch 1267/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5714.0869 - mean_absolute_error: 66.7502 - val_loss: 4945.3096 - val_mean_absolute_error: 60.1687\n",
      "Epoch 1268/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5075.8115 - mean_absolute_error: 57.1082 - val_loss: 5196.9756 - val_mean_absolute_error: 62.1387\n",
      "Epoch 1269/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1837.0322 - mean_absolute_error: 35.4727 - val_loss: 5115.9819 - val_mean_absolute_error: 62.3514\n",
      "Epoch 1270/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4924.4287 - mean_absolute_error: 57.6670 - val_loss: 5130.4180 - val_mean_absolute_error: 61.7993\n",
      "Epoch 1271/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3846.4624 - mean_absolute_error: 52.7411 - val_loss: 5100.6270 - val_mean_absolute_error: 62.1240\n",
      "Epoch 1272/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3265.5771 - mean_absolute_error: 42.8374 - val_loss: 4988.7686 - val_mean_absolute_error: 61.9788\n",
      "Epoch 1273/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6055.2822 - mean_absolute_error: 64.3465 - val_loss: 4479.7163 - val_mean_absolute_error: 59.0837\n",
      "Epoch 1274/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2789.8955 - mean_absolute_error: 46.0058 - val_loss: 4030.4224 - val_mean_absolute_error: 55.7781\n",
      "Epoch 1275/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3792.9670 - mean_absolute_error: 52.9411 - val_loss: 3772.6763 - val_mean_absolute_error: 52.6686\n",
      "Epoch 1276/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3185.4160 - mean_absolute_error: 45.9551 - val_loss: 3482.1187 - val_mean_absolute_error: 49.4784\n",
      "Epoch 1277/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4020.8467 - mean_absolute_error: 55.1487 - val_loss: 3844.1191 - val_mean_absolute_error: 52.8121\n",
      "Epoch 1278/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4818.8174 - mean_absolute_error: 54.1657 - val_loss: 4974.4082 - val_mean_absolute_error: 58.0751\n",
      "Epoch 1279/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3654.2432 - mean_absolute_error: 50.3291 - val_loss: 5875.1299 - val_mean_absolute_error: 65.7723\n",
      "Epoch 1280/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5025.9443 - mean_absolute_error: 53.4299 - val_loss: 6370.0117 - val_mean_absolute_error: 68.8829\n",
      "Epoch 1281/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3582.7119 - mean_absolute_error: 51.8094 - val_loss: 6532.4121 - val_mean_absolute_error: 69.6165\n",
      "Epoch 1282/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4071.0581 - mean_absolute_error: 50.2390 - val_loss: 6384.2314 - val_mean_absolute_error: 68.7781\n",
      "Epoch 1283/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5168.4893 - mean_absolute_error: 59.0189 - val_loss: 5923.0913 - val_mean_absolute_error: 65.3337\n",
      "Epoch 1284/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2402.2224 - mean_absolute_error: 41.3287 - val_loss: 5082.0352 - val_mean_absolute_error: 56.7461\n",
      "Epoch 1285/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3681.6194 - mean_absolute_error: 51.9928 - val_loss: 4332.7144 - val_mean_absolute_error: 57.3104\n",
      "Epoch 1286/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3230.3022 - mean_absolute_error: 47.5561 - val_loss: 4530.6836 - val_mean_absolute_error: 59.8721\n",
      "Epoch 1287/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3544.5405 - mean_absolute_error: 51.4704 - val_loss: 5743.4834 - val_mean_absolute_error: 63.9558\n",
      "Epoch 1288/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6473.4717 - mean_absolute_error: 56.5030 - val_loss: 6403.1606 - val_mean_absolute_error: 65.1252\n",
      "Epoch 1289/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3247.9170 - mean_absolute_error: 41.8321 - val_loss: 6072.5312 - val_mean_absolute_error: 60.3066\n",
      "Epoch 1290/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3334.6831 - mean_absolute_error: 48.5241 - val_loss: 6133.0430 - val_mean_absolute_error: 61.9343\n",
      "Epoch 1291/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2387.6018 - mean_absolute_error: 39.7506 - val_loss: 6083.8950 - val_mean_absolute_error: 63.9424\n",
      "Epoch 1292/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 1772.4641 - mean_absolute_error: 35.9261 - val_loss: 5851.4302 - val_mean_absolute_error: 65.2872\n",
      "Epoch 1293/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5351.6816 - mean_absolute_error: 60.2623 - val_loss: 5525.0752 - val_mean_absolute_error: 64.1061\n",
      "Epoch 1294/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3674.1582 - mean_absolute_error: 48.7736 - val_loss: 4809.4854 - val_mean_absolute_error: 60.8086\n",
      "Epoch 1295/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4513.7783 - mean_absolute_error: 51.9664 - val_loss: 4259.8721 - val_mean_absolute_error: 57.3945\n",
      "Epoch 1296/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4633.2358 - mean_absolute_error: 56.8546 - val_loss: 3894.2056 - val_mean_absolute_error: 50.8332\n",
      "Epoch 1297/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2892.4097 - mean_absolute_error: 42.2248 - val_loss: 3987.8867 - val_mean_absolute_error: 54.9150\n",
      "Epoch 1298/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2511.2407 - mean_absolute_error: 42.3633 - val_loss: 4020.1699 - val_mean_absolute_error: 55.3262\n",
      "Epoch 1299/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2940.6460 - mean_absolute_error: 35.7796 - val_loss: 4184.6733 - val_mean_absolute_error: 53.6448\n",
      "Epoch 1300/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6874.1621 - mean_absolute_error: 62.7173 - val_loss: 3725.0132 - val_mean_absolute_error: 53.1906\n",
      "Epoch 1301/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 8080.0698 - mean_absolute_error: 74.3481 - val_loss: 4094.6641 - val_mean_absolute_error: 56.6022\n",
      "Epoch 1302/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4127.2354 - mean_absolute_error: 53.5443 - val_loss: 4747.4551 - val_mean_absolute_error: 60.6276\n",
      "Epoch 1303/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6793.9229 - mean_absolute_error: 68.0527 - val_loss: 5330.7231 - val_mean_absolute_error: 63.4867\n",
      "Epoch 1304/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5997.0020 - mean_absolute_error: 57.9912 - val_loss: 5418.2549 - val_mean_absolute_error: 63.8129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1305/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5782.1587 - mean_absolute_error: 57.6036 - val_loss: 5208.6504 - val_mean_absolute_error: 62.6635\n",
      "Epoch 1306/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5596.8193 - mean_absolute_error: 56.2431 - val_loss: 4763.9053 - val_mean_absolute_error: 60.2483\n",
      "Epoch 1307/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5541.9658 - mean_absolute_error: 63.5129 - val_loss: 4104.7979 - val_mean_absolute_error: 56.1459\n",
      "Epoch 1308/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7034.9585 - mean_absolute_error: 70.6678 - val_loss: 3831.1387 - val_mean_absolute_error: 53.9518\n",
      "Epoch 1309/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5282.3857 - mean_absolute_error: 61.8017 - val_loss: 3887.6479 - val_mean_absolute_error: 54.1492\n",
      "Epoch 1310/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4981.8086 - mean_absolute_error: 57.7740 - val_loss: 3983.0356 - val_mean_absolute_error: 54.5593\n",
      "Epoch 1311/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 6499.2900 - mean_absolute_error: 58.9446 - val_loss: 4066.5000 - val_mean_absolute_error: 54.8249\n",
      "Epoch 1312/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5485.2441 - mean_absolute_error: 55.3480 - val_loss: 4111.4263 - val_mean_absolute_error: 54.8494\n",
      "Epoch 1313/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4424.8721 - mean_absolute_error: 52.9988 - val_loss: 4288.0854 - val_mean_absolute_error: 55.6230\n",
      "Epoch 1314/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4584.9980 - mean_absolute_error: 48.2900 - val_loss: 4252.6670 - val_mean_absolute_error: 55.0421\n",
      "Epoch 1315/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5284.3711 - mean_absolute_error: 57.2534 - val_loss: 4104.3462 - val_mean_absolute_error: 53.7242\n",
      "Epoch 1316/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5973.2451 - mean_absolute_error: 65.3289 - val_loss: 3961.5630 - val_mean_absolute_error: 52.4375\n",
      "Epoch 1317/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5972.0781 - mean_absolute_error: 60.3764 - val_loss: 3803.5144 - val_mean_absolute_error: 51.0196\n",
      "Epoch 1318/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3462.7402 - mean_absolute_error: 54.6226 - val_loss: 3496.7922 - val_mean_absolute_error: 48.3887\n",
      "Epoch 1319/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5204.0513 - mean_absolute_error: 60.3740 - val_loss: 3130.2515 - val_mean_absolute_error: 44.7393\n",
      "Epoch 1320/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4878.4277 - mean_absolute_error: 56.5655 - val_loss: 3063.8552 - val_mean_absolute_error: 43.6938\n",
      "Epoch 1321/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4981.6396 - mean_absolute_error: 58.7188 - val_loss: 3173.9026 - val_mean_absolute_error: 44.4989\n",
      "Epoch 1322/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7375.2285 - mean_absolute_error: 67.9631 - val_loss: 3236.4290 - val_mean_absolute_error: 44.7169\n",
      "Epoch 1323/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6391.5542 - mean_absolute_error: 61.6914 - val_loss: 3401.0825 - val_mean_absolute_error: 45.8909\n",
      "Epoch 1324/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7602.4165 - mean_absolute_error: 77.0947 - val_loss: 3809.0718 - val_mean_absolute_error: 48.8501\n",
      "Epoch 1325/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7281.1748 - mean_absolute_error: 73.2758 - val_loss: 4067.1523 - val_mean_absolute_error: 50.5723\n",
      "Epoch 1326/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3342.6819 - mean_absolute_error: 48.4494 - val_loss: 4075.0190 - val_mean_absolute_error: 50.3909\n",
      "Epoch 1327/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4285.7891 - mean_absolute_error: 50.5945 - val_loss: 3821.6685 - val_mean_absolute_error: 48.2301\n",
      "Epoch 1328/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4979.7715 - mean_absolute_error: 58.3799 - val_loss: 3540.5032 - val_mean_absolute_error: 45.8327\n",
      "Epoch 1329/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5836.3921 - mean_absolute_error: 65.0130 - val_loss: 3408.6301 - val_mean_absolute_error: 44.9859\n",
      "Epoch 1330/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5246.9258 - mean_absolute_error: 56.4938 - val_loss: 3393.2456 - val_mean_absolute_error: 44.9164\n",
      "Epoch 1331/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2544.5635 - mean_absolute_error: 43.6008 - val_loss: 3357.2942 - val_mean_absolute_error: 44.7147\n",
      "Epoch 1332/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4602.2622 - mean_absolute_error: 53.3673 - val_loss: 3334.7852 - val_mean_absolute_error: 44.9238\n",
      "Epoch 1333/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5724.7427 - mean_absolute_error: 57.4735 - val_loss: 3225.6821 - val_mean_absolute_error: 44.3029\n",
      "Epoch 1334/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5970.3706 - mean_absolute_error: 71.9532 - val_loss: 3107.4453 - val_mean_absolute_error: 43.3442\n",
      "Epoch 1335/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4579.3779 - mean_absolute_error: 62.6282 - val_loss: 3387.9993 - val_mean_absolute_error: 46.1270\n",
      "Epoch 1336/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3549.7815 - mean_absolute_error: 48.0749 - val_loss: 3590.6296 - val_mean_absolute_error: 47.8050\n",
      "Epoch 1337/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4325.0957 - mean_absolute_error: 50.6468 - val_loss: 3695.5715 - val_mean_absolute_error: 48.4297\n",
      "Epoch 1338/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3805.8608 - mean_absolute_error: 52.4918 - val_loss: 3742.8931 - val_mean_absolute_error: 48.5937\n",
      "Epoch 1339/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3377.5410 - mean_absolute_error: 52.0488 - val_loss: 3718.5691 - val_mean_absolute_error: 48.0619\n",
      "Epoch 1340/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2884.0449 - mean_absolute_error: 42.6455 - val_loss: 3523.0464 - val_mean_absolute_error: 46.0582\n",
      "Epoch 1341/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6542.4409 - mean_absolute_error: 64.2943 - val_loss: 3296.3098 - val_mean_absolute_error: 45.5521\n",
      "Epoch 1342/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5974.1016 - mean_absolute_error: 62.1741 - val_loss: 3154.5500 - val_mean_absolute_error: 44.6029\n",
      "Epoch 1343/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5028.1870 - mean_absolute_error: 57.5776 - val_loss: 3242.9880 - val_mean_absolute_error: 45.1943\n",
      "Epoch 1344/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5167.4214 - mean_absolute_error: 59.0517 - val_loss: 3497.0723 - val_mean_absolute_error: 46.7244\n",
      "Epoch 1345/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5779.4854 - mean_absolute_error: 64.8185 - val_loss: 3913.8933 - val_mean_absolute_error: 51.0581\n",
      "Epoch 1346/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2392.9443 - mean_absolute_error: 40.2658 - val_loss: 4120.7861 - val_mean_absolute_error: 52.9792\n",
      "Epoch 1347/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3444.5139 - mean_absolute_error: 47.4962 - val_loss: 3949.5933 - val_mean_absolute_error: 52.1720\n",
      "Epoch 1348/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3139.8467 - mean_absolute_error: 37.5120 - val_loss: 3892.2749 - val_mean_absolute_error: 52.0190\n",
      "Epoch 1349/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4406.3755 - mean_absolute_error: 54.5340 - val_loss: 4009.4856 - val_mean_absolute_error: 51.8735\n",
      "Epoch 1350/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3660.8511 - mean_absolute_error: 50.0218 - val_loss: 3820.6472 - val_mean_absolute_error: 50.3904\n",
      "Epoch 1351/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2472.5664 - mean_absolute_error: 42.7205 - val_loss: 3589.9629 - val_mean_absolute_error: 48.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1352/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 6075.5547 - mean_absolute_error: 60.8486 - val_loss: 3569.5747 - val_mean_absolute_error: 48.9617\n",
      "Epoch 1353/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3597.5825 - mean_absolute_error: 51.4281 - val_loss: 4004.1577 - val_mean_absolute_error: 50.1032\n",
      "Epoch 1354/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3359.3706 - mean_absolute_error: 43.4532 - val_loss: 4495.4688 - val_mean_absolute_error: 53.9625\n",
      "Epoch 1355/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3741.3137 - mean_absolute_error: 47.5443 - val_loss: 5158.4844 - val_mean_absolute_error: 61.3378\n",
      "Epoch 1356/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4622.7051 - mean_absolute_error: 56.3441 - val_loss: 4906.1782 - val_mean_absolute_error: 60.7066\n",
      "Epoch 1357/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5646.6841 - mean_absolute_error: 64.8809 - val_loss: 4592.0000 - val_mean_absolute_error: 59.3047\n",
      "Epoch 1358/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3694.9766 - mean_absolute_error: 44.3723 - val_loss: 4617.9678 - val_mean_absolute_error: 59.4516\n",
      "Epoch 1359/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5139.4229 - mean_absolute_error: 62.7776 - val_loss: 4099.4282 - val_mean_absolute_error: 55.3740\n",
      "Epoch 1360/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3724.7063 - mean_absolute_error: 50.0801 - val_loss: 3573.0757 - val_mean_absolute_error: 50.2531\n",
      "Epoch 1361/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3742.3726 - mean_absolute_error: 48.5680 - val_loss: 3460.5151 - val_mean_absolute_error: 49.4447\n",
      "Epoch 1362/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4633.5435 - mean_absolute_error: 49.0370 - val_loss: 3970.9395 - val_mean_absolute_error: 54.2309\n",
      "Epoch 1363/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2645.0286 - mean_absolute_error: 40.1114 - val_loss: 4994.9141 - val_mean_absolute_error: 60.2568\n",
      "Epoch 1364/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5321.8525 - mean_absolute_error: 52.7415 - val_loss: 5625.5801 - val_mean_absolute_error: 61.9403\n",
      "Epoch 1365/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7973.1772 - mean_absolute_error: 67.3309 - val_loss: 5781.6382 - val_mean_absolute_error: 61.7872\n",
      "Epoch 1366/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2397.7292 - mean_absolute_error: 37.2844 - val_loss: 5615.2715 - val_mean_absolute_error: 60.1220\n",
      "Epoch 1367/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 1519.8667 - mean_absolute_error: 30.2424 - val_loss: 5319.1748 - val_mean_absolute_error: 58.1828\n",
      "Epoch 1368/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3800.5317 - mean_absolute_error: 48.0408 - val_loss: 4722.0444 - val_mean_absolute_error: 55.8038\n",
      "Epoch 1369/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3516.8267 - mean_absolute_error: 50.9910 - val_loss: 4040.8848 - val_mean_absolute_error: 51.9996\n",
      "Epoch 1370/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2539.9731 - mean_absolute_error: 41.5840 - val_loss: 3931.6382 - val_mean_absolute_error: 52.7333\n",
      "Epoch 1371/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3117.3687 - mean_absolute_error: 41.9914 - val_loss: 4019.7637 - val_mean_absolute_error: 54.2180\n",
      "Epoch 1372/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3238.9717 - mean_absolute_error: 50.3427 - val_loss: 4333.5576 - val_mean_absolute_error: 53.9661\n",
      "Epoch 1373/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2983.6675 - mean_absolute_error: 46.2957 - val_loss: 4666.0366 - val_mean_absolute_error: 54.2190\n",
      "Epoch 1374/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4780.1338 - mean_absolute_error: 54.5904 - val_loss: 5404.1064 - val_mean_absolute_error: 61.6299\n",
      "Epoch 1375/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 1891.1517 - mean_absolute_error: 35.3911 - val_loss: 6143.0908 - val_mean_absolute_error: 66.0163\n",
      "Epoch 1376/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3839.0544 - mean_absolute_error: 44.9734 - val_loss: 6243.2451 - val_mean_absolute_error: 66.7969\n",
      "Epoch 1377/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4548.6738 - mean_absolute_error: 55.5633 - val_loss: 5990.1030 - val_mean_absolute_error: 65.5843\n",
      "Epoch 1378/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6109.3389 - mean_absolute_error: 65.7116 - val_loss: 4976.4736 - val_mean_absolute_error: 59.9508\n",
      "Epoch 1379/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3835.3071 - mean_absolute_error: 48.3950 - val_loss: 3833.1206 - val_mean_absolute_error: 52.4429\n",
      "Epoch 1380/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4527.7451 - mean_absolute_error: 56.9705 - val_loss: 3278.2175 - val_mean_absolute_error: 46.3096\n",
      "Epoch 1381/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6960.9492 - mean_absolute_error: 72.9392 - val_loss: 3469.4304 - val_mean_absolute_error: 47.8258\n",
      "Epoch 1382/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4046.3313 - mean_absolute_error: 54.9484 - val_loss: 4295.9004 - val_mean_absolute_error: 52.9222\n",
      "Epoch 1383/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4913.8545 - mean_absolute_error: 57.0569 - val_loss: 5210.9863 - val_mean_absolute_error: 57.2575\n",
      "Epoch 1384/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5091.5449 - mean_absolute_error: 55.4673 - val_loss: 5723.1299 - val_mean_absolute_error: 57.4705\n",
      "Epoch 1385/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4655.5532 - mean_absolute_error: 50.0410 - val_loss: 5516.8140 - val_mean_absolute_error: 56.2782\n",
      "Epoch 1386/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4267.6689 - mean_absolute_error: 53.7770 - val_loss: 5759.6582 - val_mean_absolute_error: 56.9996\n",
      "Epoch 1387/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7405.9048 - mean_absolute_error: 71.4129 - val_loss: 6966.5322 - val_mean_absolute_error: 67.9830\n",
      "Epoch 1388/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4875.5229 - mean_absolute_error: 53.4602 - val_loss: 6887.3765 - val_mean_absolute_error: 68.8274\n",
      "Epoch 1389/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5701.5610 - mean_absolute_error: 65.5008 - val_loss: 6423.7910 - val_mean_absolute_error: 67.2108\n",
      "Epoch 1390/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5509.0713 - mean_absolute_error: 57.1204 - val_loss: 5534.7856 - val_mean_absolute_error: 63.5095\n",
      "Epoch 1391/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4110.6987 - mean_absolute_error: 50.7025 - val_loss: 4702.2627 - val_mean_absolute_error: 58.5267\n",
      "Epoch 1392/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3617.6196 - mean_absolute_error: 47.8295 - val_loss: 3662.3374 - val_mean_absolute_error: 49.1228\n",
      "Epoch 1393/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4787.7695 - mean_absolute_error: 58.5514 - val_loss: 2943.3254 - val_mean_absolute_error: 46.7698\n",
      "Epoch 1394/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4957.2168 - mean_absolute_error: 61.8238 - val_loss: 3329.1543 - val_mean_absolute_error: 47.9135\n",
      "Epoch 1395/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4107.0645 - mean_absolute_error: 48.8099 - val_loss: 4175.3892 - val_mean_absolute_error: 53.3091\n",
      "Epoch 1396/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2767.5239 - mean_absolute_error: 44.0282 - val_loss: 5328.6514 - val_mean_absolute_error: 61.1655\n",
      "Epoch 1397/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4321.5737 - mean_absolute_error: 52.0415 - val_loss: 6134.7451 - val_mean_absolute_error: 65.2712\n",
      "Epoch 1398/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4625.5967 - mean_absolute_error: 52.6393 - val_loss: 6377.9365 - val_mean_absolute_error: 66.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1399/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5216.7891 - mean_absolute_error: 54.2876 - val_loss: 6094.7671 - val_mean_absolute_error: 65.1479\n",
      "Epoch 1400/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 9676.8867 - mean_absolute_error: 73.1126 - val_loss: 5423.7139 - val_mean_absolute_error: 61.7075\n",
      "Epoch 1401/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4415.9434 - mean_absolute_error: 54.8693 - val_loss: 4339.6499 - val_mean_absolute_error: 54.7569\n",
      "Epoch 1402/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2707.1304 - mean_absolute_error: 44.2099 - val_loss: 3316.4585 - val_mean_absolute_error: 47.1838\n",
      "Epoch 1403/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3446.2656 - mean_absolute_error: 50.9153 - val_loss: 2725.7163 - val_mean_absolute_error: 44.5884\n",
      "Epoch 1404/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 5425.6855 - mean_absolute_error: 58.2811 - val_loss: 2931.9626 - val_mean_absolute_error: 47.0260\n",
      "Epoch 1405/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3174.5522 - mean_absolute_error: 46.8594 - val_loss: 3898.4192 - val_mean_absolute_error: 50.4031\n",
      "Epoch 1406/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5439.4351 - mean_absolute_error: 56.7516 - val_loss: 5169.9590 - val_mean_absolute_error: 59.3138\n",
      "Epoch 1407/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5256.0508 - mean_absolute_error: 55.5688 - val_loss: 6153.4956 - val_mean_absolute_error: 65.7362\n",
      "Epoch 1408/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3913.2012 - mean_absolute_error: 48.1524 - val_loss: 6420.0039 - val_mean_absolute_error: 67.1986\n",
      "Epoch 1409/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5292.2119 - mean_absolute_error: 62.6379 - val_loss: 6272.5498 - val_mean_absolute_error: 66.6933\n",
      "Epoch 1410/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 6513.7246 - mean_absolute_error: 70.6942 - val_loss: 5772.3599 - val_mean_absolute_error: 64.6005\n",
      "Epoch 1411/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7813.0854 - mean_absolute_error: 69.0942 - val_loss: 4982.4531 - val_mean_absolute_error: 60.8257\n",
      "Epoch 1412/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7922.0166 - mean_absolute_error: 68.3004 - val_loss: 4078.6118 - val_mean_absolute_error: 55.4408\n",
      "Epoch 1413/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5995.5869 - mean_absolute_error: 64.2041 - val_loss: 3505.9409 - val_mean_absolute_error: 50.4450\n",
      "Epoch 1414/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2791.9756 - mean_absolute_error: 40.9428 - val_loss: 3183.3091 - val_mean_absolute_error: 45.8316\n",
      "Epoch 1415/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5153.2627 - mean_absolute_error: 59.6047 - val_loss: 3196.5146 - val_mean_absolute_error: 44.9902\n",
      "Epoch 1416/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3875.6553 - mean_absolute_error: 51.8736 - val_loss: 3383.5767 - val_mean_absolute_error: 50.1956\n",
      "Epoch 1417/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4368.3789 - mean_absolute_error: 50.1077 - val_loss: 3642.1519 - val_mean_absolute_error: 51.1870\n",
      "Epoch 1418/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3743.6582 - mean_absolute_error: 45.5474 - val_loss: 3846.0942 - val_mean_absolute_error: 51.3882\n",
      "Epoch 1419/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1842.0273 - mean_absolute_error: 35.3855 - val_loss: 4232.0029 - val_mean_absolute_error: 53.0497\n",
      "Epoch 1420/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3409.5068 - mean_absolute_error: 46.7444 - val_loss: 4471.8936 - val_mean_absolute_error: 55.3780\n",
      "Epoch 1421/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 7096.2266 - mean_absolute_error: 72.9059 - val_loss: 5049.2061 - val_mean_absolute_error: 59.3865\n",
      "Epoch 1422/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5598.7983 - mean_absolute_error: 59.6571 - val_loss: 5564.0244 - val_mean_absolute_error: 62.0019\n",
      "Epoch 1423/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 7641.8789 - mean_absolute_error: 70.5507 - val_loss: 5741.9756 - val_mean_absolute_error: 62.4093\n",
      "Epoch 1424/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3201.4167 - mean_absolute_error: 45.5093 - val_loss: 5710.0718 - val_mean_absolute_error: 61.2821\n",
      "Epoch 1425/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3450.5979 - mean_absolute_error: 52.9998 - val_loss: 5159.7812 - val_mean_absolute_error: 55.4664\n",
      "Epoch 1426/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 2414.4966 - mean_absolute_error: 41.5294 - val_loss: 4104.5654 - val_mean_absolute_error: 51.5637\n",
      "Epoch 1427/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3860.3298 - mean_absolute_error: 48.8507 - val_loss: 4265.6313 - val_mean_absolute_error: 52.3603\n",
      "Epoch 1428/1500\n",
      "16/16 [==============================] - 0s 938us/step - loss: 7460.9473 - mean_absolute_error: 67.2698 - val_loss: 4715.4302 - val_mean_absolute_error: 53.5187\n",
      "Epoch 1429/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3012.9946 - mean_absolute_error: 45.2482 - val_loss: 4652.6245 - val_mean_absolute_error: 56.0908\n",
      "Epoch 1430/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4217.4072 - mean_absolute_error: 51.5957 - val_loss: 4561.8018 - val_mean_absolute_error: 57.3660\n",
      "Epoch 1431/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5138.9756 - mean_absolute_error: 54.3780 - val_loss: 4237.1367 - val_mean_absolute_error: 56.1069\n",
      "Epoch 1432/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3207.3096 - mean_absolute_error: 47.1648 - val_loss: 3976.5906 - val_mean_absolute_error: 54.4443\n",
      "Epoch 1433/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3025.7944 - mean_absolute_error: 46.9983 - val_loss: 3914.6694 - val_mean_absolute_error: 53.6736\n",
      "Epoch 1434/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4056.5881 - mean_absolute_error: 50.7988 - val_loss: 4113.0288 - val_mean_absolute_error: 52.3620\n",
      "Epoch 1435/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3794.0540 - mean_absolute_error: 50.7841 - val_loss: 4399.7271 - val_mean_absolute_error: 53.1075\n",
      "Epoch 1436/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3919.3901 - mean_absolute_error: 55.4613 - val_loss: 4704.5850 - val_mean_absolute_error: 56.1224\n",
      "Epoch 1437/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4966.6377 - mean_absolute_error: 58.0300 - val_loss: 4989.2192 - val_mean_absolute_error: 58.9765\n",
      "Epoch 1438/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 3804.2324 - mean_absolute_error: 48.8813 - val_loss: 4980.4360 - val_mean_absolute_error: 60.5941\n",
      "Epoch 1439/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 1738.3992 - mean_absolute_error: 34.5211 - val_loss: 4839.9502 - val_mean_absolute_error: 60.3013\n",
      "Epoch 1440/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4387.0376 - mean_absolute_error: 53.6964 - val_loss: 4581.6211 - val_mean_absolute_error: 58.9233\n",
      "Epoch 1441/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4005.7729 - mean_absolute_error: 54.5779 - val_loss: 4333.1475 - val_mean_absolute_error: 56.9176\n",
      "Epoch 1442/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 6856.7432 - mean_absolute_error: 67.9758 - val_loss: 4082.1343 - val_mean_absolute_error: 52.8371\n",
      "Epoch 1443/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2738.1545 - mean_absolute_error: 39.2600 - val_loss: 3901.9512 - val_mean_absolute_error: 51.4510\n",
      "Epoch 1444/1500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3538.1904 - mean_absolute_error: 50.7715 - val_loss: 3438.2808 - val_mean_absolute_error: 50.2133\n",
      "Epoch 1445/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 3151.0977 - mean_absolute_error: 44.3709 - val_loss: 3443.1594 - val_mean_absolute_error: 50.4484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1446/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4430.3613 - mean_absolute_error: 51.8715 - val_loss: 4168.8271 - val_mean_absolute_error: 52.7562\n",
      "Epoch 1447/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4076.5703 - mean_absolute_error: 48.8110 - val_loss: 4491.0679 - val_mean_absolute_error: 56.9036\n",
      "Epoch 1448/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2866.0464 - mean_absolute_error: 42.6088 - val_loss: 4721.3477 - val_mean_absolute_error: 59.7314\n",
      "Epoch 1449/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4376.0752 - mean_absolute_error: 58.7412 - val_loss: 4768.3345 - val_mean_absolute_error: 60.2878\n",
      "Epoch 1450/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3819.6975 - mean_absolute_error: 49.4194 - val_loss: 4822.7666 - val_mean_absolute_error: 59.2904\n",
      "Epoch 1451/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4406.6919 - mean_absolute_error: 56.3159 - val_loss: 4573.2061 - val_mean_absolute_error: 56.5936\n",
      "Epoch 1452/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3534.9604 - mean_absolute_error: 48.2177 - val_loss: 3864.8203 - val_mean_absolute_error: 50.3159\n",
      "Epoch 1453/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 4713.6357 - mean_absolute_error: 56.2747 - val_loss: 3543.5879 - val_mean_absolute_error: 49.3714\n",
      "Epoch 1454/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3114.4792 - mean_absolute_error: 43.9965 - val_loss: 3487.5422 - val_mean_absolute_error: 49.7525\n",
      "Epoch 1455/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5086.9551 - mean_absolute_error: 54.7839 - val_loss: 4229.0918 - val_mean_absolute_error: 54.6191\n",
      "Epoch 1456/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 1742.4419 - mean_absolute_error: 33.8847 - val_loss: 4475.8711 - val_mean_absolute_error: 58.6333\n",
      "Epoch 1457/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 6365.0039 - mean_absolute_error: 61.7371 - val_loss: 5099.2808 - val_mean_absolute_error: 62.4816\n",
      "Epoch 1458/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 5240.7021 - mean_absolute_error: 52.7992 - val_loss: 5742.6553 - val_mean_absolute_error: 64.3301\n",
      "Epoch 1459/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3522.1797 - mean_absolute_error: 44.7690 - val_loss: 5793.5767 - val_mean_absolute_error: 61.5937\n",
      "Epoch 1460/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3114.9097 - mean_absolute_error: 41.5920 - val_loss: 5382.5498 - val_mean_absolute_error: 57.6249\n",
      "Epoch 1461/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3216.6404 - mean_absolute_error: 48.1963 - val_loss: 4838.8662 - val_mean_absolute_error: 53.2099\n",
      "Epoch 1462/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2550.7988 - mean_absolute_error: 39.2367 - val_loss: 4412.1021 - val_mean_absolute_error: 52.7185\n",
      "Epoch 1463/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3078.5171 - mean_absolute_error: 43.2872 - val_loss: 3825.2056 - val_mean_absolute_error: 52.0879\n",
      "Epoch 1464/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2460.8987 - mean_absolute_error: 37.2068 - val_loss: 4258.6997 - val_mean_absolute_error: 53.6902\n",
      "Epoch 1465/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2131.3027 - mean_absolute_error: 38.0849 - val_loss: 4085.4263 - val_mean_absolute_error: 52.9617\n",
      "Epoch 1466/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2225.9014 - mean_absolute_error: 37.2778 - val_loss: 4235.7803 - val_mean_absolute_error: 53.9759\n",
      "Epoch 1467/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2713.4219 - mean_absolute_error: 40.0619 - val_loss: 4590.0527 - val_mean_absolute_error: 57.1353\n",
      "Epoch 1468/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2978.9392 - mean_absolute_error: 42.0223 - val_loss: 5053.8569 - val_mean_absolute_error: 60.9627\n",
      "Epoch 1469/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2138.9849 - mean_absolute_error: 36.5595 - val_loss: 5516.0024 - val_mean_absolute_error: 63.9599\n",
      "Epoch 1470/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4996.1113 - mean_absolute_error: 58.2774 - val_loss: 5559.7344 - val_mean_absolute_error: 63.9045\n",
      "Epoch 1471/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2495.0640 - mean_absolute_error: 43.1920 - val_loss: 5165.0098 - val_mean_absolute_error: 61.9948\n",
      "Epoch 1472/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 2566.3403 - mean_absolute_error: 42.9010 - val_loss: 5160.4229 - val_mean_absolute_error: 60.7956\n",
      "Epoch 1473/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3765.9683 - mean_absolute_error: 49.9791 - val_loss: 4823.4507 - val_mean_absolute_error: 57.9192\n",
      "Epoch 1474/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3710.1387 - mean_absolute_error: 50.0294 - val_loss: 4532.3125 - val_mean_absolute_error: 55.8550\n",
      "Epoch 1475/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1579.7014 - mean_absolute_error: 31.6898 - val_loss: 4341.9463 - val_mean_absolute_error: 54.4850\n",
      "Epoch 1476/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5545.2046 - mean_absolute_error: 62.0770 - val_loss: 5079.7031 - val_mean_absolute_error: 57.8515\n",
      "Epoch 1477/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 5135.6294 - mean_absolute_error: 59.3383 - val_loss: 5902.6064 - val_mean_absolute_error: 61.7678\n",
      "Epoch 1478/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4331.1133 - mean_absolute_error: 54.3552 - val_loss: 6048.2393 - val_mean_absolute_error: 68.0642\n",
      "Epoch 1479/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 4194.7568 - mean_absolute_error: 50.1905 - val_loss: 5711.2881 - val_mean_absolute_error: 67.3532\n",
      "Epoch 1480/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3997.3823 - mean_absolute_error: 49.5528 - val_loss: 5269.8452 - val_mean_absolute_error: 65.1064\n",
      "Epoch 1481/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3664.6328 - mean_absolute_error: 52.1434 - val_loss: 4963.7432 - val_mean_absolute_error: 63.2399\n",
      "Epoch 1482/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 2029.9327 - mean_absolute_error: 34.8036 - val_loss: 4866.9463 - val_mean_absolute_error: 62.0105\n",
      "Epoch 1483/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 1911.8577 - mean_absolute_error: 37.1387 - val_loss: 4673.6885 - val_mean_absolute_error: 57.4924\n",
      "Epoch 1484/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3438.5737 - mean_absolute_error: 43.8346 - val_loss: 3677.3809 - val_mean_absolute_error: 52.6954\n",
      "Epoch 1485/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5351.6270 - mean_absolute_error: 58.4979 - val_loss: 3764.8809 - val_mean_absolute_error: 49.8753\n",
      "Epoch 1486/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3905.8987 - mean_absolute_error: 47.4400 - val_loss: 4653.9795 - val_mean_absolute_error: 56.9986\n",
      "Epoch 1487/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 4796.5444 - mean_absolute_error: 56.2271 - val_loss: 5928.9868 - val_mean_absolute_error: 66.0355\n",
      "Epoch 1488/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 3501.0103 - mean_absolute_error: 50.0227 - val_loss: 5574.9136 - val_mean_absolute_error: 66.3484\n",
      "Epoch 1489/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 7413.6416 - mean_absolute_error: 72.0381 - val_loss: 5447.1460 - val_mean_absolute_error: 65.7878\n",
      "Epoch 1490/1500\n",
      "16/16 [==============================] - 0s 813us/step - loss: 4311.7393 - mean_absolute_error: 50.9127 - val_loss: 5614.2822 - val_mean_absolute_error: 66.3384\n",
      "Epoch 1491/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 3908.0752 - mean_absolute_error: 51.6488 - val_loss: 5658.7588 - val_mean_absolute_error: 61.8411\n",
      "Epoch 1492/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3832.8311 - mean_absolute_error: 42.7613 - val_loss: 4648.0049 - val_mean_absolute_error: 57.7498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1493/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3368.3508 - mean_absolute_error: 44.8907 - val_loss: 3650.1550 - val_mean_absolute_error: 51.8333\n",
      "Epoch 1494/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 7655.0254 - mean_absolute_error: 67.9797 - val_loss: 4906.6221 - val_mean_absolute_error: 57.4287\n",
      "Epoch 1495/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 4139.6987 - mean_absolute_error: 53.1527 - val_loss: 4893.4512 - val_mean_absolute_error: 60.8827\n",
      "Epoch 1496/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 2462.6543 - mean_absolute_error: 41.7448 - val_loss: 4973.6343 - val_mean_absolute_error: 63.0461\n",
      "Epoch 1497/1500\n",
      "16/16 [==============================] - 0s 625us/step - loss: 5579.2383 - mean_absolute_error: 61.9960 - val_loss: 5505.2588 - val_mean_absolute_error: 65.8446\n",
      "Epoch 1498/1500\n",
      "16/16 [==============================] - 0s 688us/step - loss: 3912.3076 - mean_absolute_error: 53.1440 - val_loss: 6658.8477 - val_mean_absolute_error: 70.4998\n",
      "Epoch 1499/1500\n",
      "16/16 [==============================] - 0s 750us/step - loss: 2243.8987 - mean_absolute_error: 38.5814 - val_loss: 7323.0132 - val_mean_absolute_error: 71.2224\n",
      "Epoch 1500/1500\n",
      "16/16 [==============================] - 0s 875us/step - loss: 5005.8569 - mean_absolute_error: 59.7850 - val_loss: 7128.6611 - val_mean_absolute_error: 67.5522\n"
     ]
    }
   ],
   "source": [
    "#訓練開始 xx為feature Y為label  batch_size為每次放多少進去 epochs為處理幾輪 validation_split為抽多少樣本來驗證 verbose=1為每次顯示\n",
    "train_history=model.fit(XX_train,YY_train,batch_size=batch_size,epochs=epochs,validation_split=0.2,verbose=1)\n",
    "# train_history=model.fit(xx,Y,batch_size=batch_size,epochs=epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e593ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#劃出準確度歷程\n",
    "import matplotlib.pyplot as plt\n",
    "def show_tarin_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title(\"Train History\")\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(['train','validation'],loc=\"upper left\")\n",
    "    plt.show()\n",
    "show_tarin_history(train_history,'loss','val_loss')\n",
    "# show_tarin_history(train_history,'loss','loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.64290030367559"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=model.predict(XX_train).reshape([len(XX_train)])-np.array(YY_train)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.60695844757177"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=model.predict(XX_test).reshape([len(XX_test)])-np.array(YY_test)\n",
    "np.average(error**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSepkQEiSOv1/vQSWBCwy1tAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e593630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\no/4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25b06b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict=model.predict(XX_train)\n",
    "plotPaint(predict,YY_train,title=\"train\")\n",
    "predict=model.predict(XX_test)\n",
    "plotPaint(predict,YY_test,title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict=model.predict(newDataxxG)\n",
    "# plotPaint(predict,YG,R=1)\n",
    "# predict=model.predict(newDataxxB)\n",
    "# plotPaint(predict,YB,R=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted_sales = model.predict(newDataxxG)\n",
    "# predicted_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predicted_sales = model.predict(newDataxxB)\n",
    "# predicted_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================預測類型==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試跑SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 直接將資料分7成訓練集、3成測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#將資料分成訓練、測試集 其中測試集佔三成\n",
    "xx_train, xx_test, Y_train, Y_test =train_test_split(xx,typeY,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 14) (9, 14) (21,) (9,)\n"
     ]
    }
   ],
   "source": [
    "print(xx_train.shape,xx_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#正確率function\n",
    "def GorB(someModel,xx_train=xx_train,Y_train=Y_train,xx_test=xx_test,Y_test=Y_test):\n",
    "    predicted = someModel((xx_train)) #預測結果\n",
    "    accuracy=list(predicted==Y_train)#回傳true or false的list\n",
    "    print(\"訓練集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率\n",
    "\n",
    "\n",
    "    predicted = someModel((xx_test)) #預測結果\n",
    "    accuracy=list(predicted==Y_test)#回傳true or false的list\n",
    "    print(\"測試集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線性分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集正確率：0.7142857142857143\n",
      "測試集正確率：0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(xx_train, Y_train)\n",
    "\n",
    "# predicted = clf1.predict((xx_train)) #預測結果\n",
    "# accuracy=list(predicted==Y_train)#回傳true or false的list\n",
    "# print(\"訓練集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率\n",
    "\n",
    "\n",
    "# predicted = clf1.predict((xx_test)) #預測結果\n",
    "# accuracy=list(predicted==Y_test)#回傳true or false的list\n",
    "# print(\"測試集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率\n",
    "\n",
    "GorB(clf1.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高斯單純貝氏分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集正確率：0.6666666666666666\n",
      "測試集正確率：0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf=GaussianNB().fit(xx_train, Y_train)\n",
    "GorB(clf.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集正確率：0.7142857142857143\n",
      "測試集正確率：0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf=KNeighborsClassifier().fit(xx_train, Y_train)\n",
    "GorB(clf.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隨機森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集正確率：1.0\n",
      "測試集正確率：0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier().fit(xx_train, Y_train)\n",
    "GorB(clf.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 10\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 100\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 1000\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1e-05 10000\n",
      "訓練集正確率：0.6666666666666666\n",
      "測試集正確率：0.7777777777777778\n",
      "1e-05 100000\n",
      "訓練集正確率：0.9047619047619048\n",
      "測試集正確率：0.6666666666666666\n",
      "1e-05 1000000\n",
      "訓練集正確率：0.9523809523809523\n",
      "測試集正確率：0.6666666666666666\n",
      "1e-05 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.0001 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 10\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 100\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.0001 1000\n",
      "訓練集正確率：0.6666666666666666\n",
      "測試集正確率：0.8888888888888888\n",
      "0.0001 10000\n",
      "訓練集正確率：0.9047619047619048\n",
      "測試集正確率：0.6666666666666666\n",
      "0.0001 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.0001 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.0001 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.001 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 10\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.001 100\n",
      "訓練集正確率：0.7142857142857143\n",
      "測試集正確率：1.0\n",
      "0.001 1000\n",
      "訓練集正確率：0.9047619047619048\n",
      "測試集正確率：0.6666666666666666\n",
      "0.001 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.001 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.001 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.001 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.7777777777777778\n",
      "0.01 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.01 10\n",
      "訓練集正確率：0.8095238095238095\n",
      "測試集正確率：1.0\n",
      "0.01 100\n",
      "訓練集正確率：0.9523809523809523\n",
      "測試集正確率：0.7777777777777778\n",
      "0.01 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.01 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.01 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.01 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.01 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "0.1 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.1 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.1 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.1 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.1 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "0.1 1\n",
      "訓練集正確率：0.8095238095238095\n",
      "測試集正確率：0.4444444444444444\n",
      "0.1 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "0.1 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.5555555555555556\n",
      "1 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "100000 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "1000000 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 1e-05\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 0.0001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 0.001\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 0.01\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 0.1\n",
      "訓練集正確率：0.47619047619047616\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 1\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 10\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 100\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 1000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 10000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 100000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 1000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n",
      "10000000 10000000\n",
      "訓練集正確率：1.0\n",
      "測試集正確率：0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# 產生SVC分類器 \n",
    "#C(誤差容忍，越高，说明越不能容忍出现误差) \n",
    "#gamma(隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度)\n",
    "#如果gamma设的太大，高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近\n",
    "\n",
    "n=13\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        i2=10**(i-5)\n",
    "        j2=10**(j-5)\n",
    "\n",
    "\n",
    "        classifier = svm.SVC(gamma=i2, C=j2,kernel=\"rbf\")\n",
    "        # classifier = svm.SVC(gamma=20, C=1,kernel=\"rbf\")\n",
    "        # classifier = svm.SVC(gamma=1000, C=1000,kernel=\"linear\")\n",
    "        #訓練\n",
    "        print(i2,j2)\n",
    "        classifier.fit(xx_train, Y_train)\n",
    "        GorB(classifier.predict)\n",
    "        \n",
    "\n",
    "# predicted = classifier.predict((xx_train)) #預測結果\n",
    "# accuracy=list(predicted==Y_train)#回傳true or false的list\n",
    "# print(\"訓練集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率\n",
    "\n",
    "\n",
    "# predicted = classifier.predict((xx_test)) #預測結果\n",
    "# accuracy=list(predicted==Y_test)#回傳true or false的list\n",
    "# print(\"測試集正確率：{}\".format(accuracy.count(True)/len(accuracy))) #看正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集正確率：1.0\n",
      "測試集正確率：0.6666666666666666\n",
      "訓練集正確率：0.9523809523809523\n",
      "測試集正確率：0.7777777777777778\n",
      "訓練集正確率：0.8095238095238095\n",
      "測試集正確率：1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(gamma=0.00001, C=10000000,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=20, C=1,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=1000, C=1000,kernel=\"linear\")\n",
    "#訓練\n",
    "classifier.fit(xx_train, Y_train)\n",
    "GorB(classifier.predict)\n",
    "\n",
    "\n",
    "classifier = svm.SVC(gamma=0.01, C=100,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=20, C=1,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=1000, C=1000,kernel=\"linear\")\n",
    "#訓練\n",
    "classifier.fit(xx_train, Y_train)\n",
    "GorB(classifier.predict)\n",
    "\n",
    "\n",
    "classifier = svm.SVC(gamma=0.01, C=10,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=20, C=1,kernel=\"rbf\")\n",
    "# classifier = svm.SVC(gamma=1000, C=1000,kernel=\"linear\")\n",
    "#訓練\n",
    "classifier.fit(xx_train, Y_train)\n",
    "GorB(classifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300, 500, 700, 300, 500),\n",
       "       learning_rate='constant', learning_rate_init=0.001, max_iter=1000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#多層類神經網路分類器 ()\n",
    "    #random_state=1初始亂數值設定永遠相同 \n",
    "    #hidden_layer_sizes=(200,100)有兩層隱藏層，分別有200跟100個神經元 預設單層100\n",
    "    #activation='identity', 'logistic', 'tanh', 'relu' 啟動函數有四種 預設為'relu'\n",
    "        #'relu'預設，f(x)=max(0,x) 79.8%\n",
    "        #'logistic'f(x)=1/(1+exp(x)) 對事件的機率有興趣時使用 46%\n",
    "        #'identity'f(x)=x 48% \n",
    "        #'tanh'??? 46%\n",
    "    #max_iter=500跌代次數，重複訓練的次數 預設為200\n",
    "# mlp=MLPClassifier(random_state=1,hidden_layer_sizes=(200,500,300),activation=\"relu\",max_iter=500)\n",
    "mlp=MLPClassifier(random_state=1,hidden_layer_sizes=(300,500,700,300,500),activation=\"relu\",max_iter=1000)\n",
    "mlp.fit(xx_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集 1.0\n",
      "測試集 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "print(\"訓練集\",len([i for i in mlp.predict(xx_train)==Y_train if i==True])/len(Y_train))\n",
    "print(\"測試集\",len([i for i in mlp.predict(xx_test)==Y_test if i==True])/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試跑Keras DNN分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes=3\n",
    "#one-hot\n",
    "Y_trainO=np_utils.to_categorical(Y_train,classes)\n",
    "Y_testO=np_utils.to_categorical(Y_test,classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.]]), array([[0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trainO[:5],Y_testO[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()  #定義model\n",
    "input_size=len(xx_train[0])#feature數量\n",
    "batch_size=50#每批樣本大小\n",
    "epochs=2000#處理幾輪\n",
    "\n",
    "model.add(Dense(100,input_dim=input_size)) \n",
    "model.add(Activation('relu')) #啟動函數\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200)) \n",
    "model.add(Activation('relu')) #啟動函數\n",
    "model.add(Dense(200)) \n",
    "model.add(Activation('relu')) #啟動函數\n",
    "model.add(Dense(100)) \n",
    "model.add(Activation('relu')) #啟動函數\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# for i in range(5):\n",
    "#     model.add(Dense(100-i*10)) \n",
    "#     model.add(Activation('relu')) #啟動函數\n",
    "\n",
    "\n",
    "# model.add(Dense(20)) \n",
    "# model.add(Activation('relu')) #啟動函數\n",
    "\n",
    "model.add(Dense(10,activation=\"sigmoid\")) \n",
    "# model.add(Dense(10,activation=\"sigmoid\")) \n",
    "\n",
    "model.add(Dense(3))  #加入層(緊密層) 產出個數10.輸入個數100 次元\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 100)               1500      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 3)                 33        \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 83,043\n",
      "Trainable params: 83,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#optimizer最佳化工具sgd(隨機梯度下降法) loss成本函數(交叉熵)   metrics性能評估方法()\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "# model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "# model.compile(loss=\"MSE\",metrics=['accuracy'],optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18 samples, validate on 3 samples\n",
      "Epoch 1/2000\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 1.0738 - acc: 0.5556 - val_loss: 1.1952 - val_acc: 0.0000e+00\n",
      "Epoch 2/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0647 - acc: 0.5000 - val_loss: 1.2057 - val_acc: 0.0000e+00\n",
      "Epoch 3/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0651 - acc: 0.5000 - val_loss: 1.2191 - val_acc: 0.0000e+00\n",
      "Epoch 4/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0576 - acc: 0.5000 - val_loss: 1.2348 - val_acc: 0.0000e+00\n",
      "Epoch 5/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0478 - acc: 0.5556 - val_loss: 1.2527 - val_acc: 0.0000e+00\n",
      "Epoch 6/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0313 - acc: 0.5556 - val_loss: 1.2720 - val_acc: 0.0000e+00\n",
      "Epoch 7/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 1.0488 - acc: 0.5556 - val_loss: 1.2931 - val_acc: 0.0000e+00\n",
      "Epoch 8/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0236 - acc: 0.5556 - val_loss: 1.3148 - val_acc: 0.0000e+00\n",
      "Epoch 9/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0211 - acc: 0.5556 - val_loss: 1.3370 - val_acc: 0.0000e+00\n",
      "Epoch 10/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 1.0087 - acc: 0.5556 - val_loss: 1.3593 - val_acc: 0.0000e+00\n",
      "Epoch 11/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 1.0021 - acc: 0.5556 - val_loss: 1.3814 - val_acc: 0.0000e+00\n",
      "Epoch 12/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0087 - acc: 0.5556 - val_loss: 1.4028 - val_acc: 0.0000e+00\n",
      "Epoch 13/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 1.0037 - acc: 0.5556 - val_loss: 1.4235 - val_acc: 0.0000e+00\n",
      "Epoch 14/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9974 - acc: 0.5556 - val_loss: 1.4432 - val_acc: 0.0000e+00\n",
      "Epoch 15/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 1.0042 - acc: 0.5556 - val_loss: 1.4614 - val_acc: 0.0000e+00\n",
      "Epoch 16/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9884 - acc: 0.5556 - val_loss: 1.4781 - val_acc: 0.0000e+00\n",
      "Epoch 17/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9811 - acc: 0.5556 - val_loss: 1.4928 - val_acc: 0.0000e+00\n",
      "Epoch 18/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9844 - acc: 0.5556 - val_loss: 1.5056 - val_acc: 0.0000e+00\n",
      "Epoch 19/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.9824 - acc: 0.5556 - val_loss: 1.5165 - val_acc: 0.0000e+00\n",
      "Epoch 20/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9807 - acc: 0.5556 - val_loss: 1.5253 - val_acc: 0.0000e+00\n",
      "Epoch 21/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9703 - acc: 0.5556 - val_loss: 1.5322 - val_acc: 0.0000e+00\n",
      "Epoch 22/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9831 - acc: 0.5556 - val_loss: 1.5369 - val_acc: 0.0000e+00\n",
      "Epoch 23/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9817 - acc: 0.5556 - val_loss: 1.5397 - val_acc: 0.0000e+00\n",
      "Epoch 24/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9835 - acc: 0.5556 - val_loss: 1.5408 - val_acc: 0.0000e+00\n",
      "Epoch 25/2000\n",
      "18/18 [==============================] - 0s 167us/step - loss: 0.9836 - acc: 0.5556 - val_loss: 1.5400 - val_acc: 0.0000e+00\n",
      "Epoch 26/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9840 - acc: 0.5556 - val_loss: 1.5376 - val_acc: 0.0000e+00\n",
      "Epoch 27/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9763 - acc: 0.5556 - val_loss: 1.5340 - val_acc: 0.0000e+00\n",
      "Epoch 28/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9779 - acc: 0.5556 - val_loss: 1.5285 - val_acc: 0.0000e+00\n",
      "Epoch 29/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.9526 - acc: 0.5556 - val_loss: 1.5229 - val_acc: 0.0000e+00\n",
      "Epoch 30/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9630 - acc: 0.5556 - val_loss: 1.5165 - val_acc: 0.0000e+00\n",
      "Epoch 31/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9631 - acc: 0.5556 - val_loss: 1.5094 - val_acc: 0.0000e+00\n",
      "Epoch 32/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9595 - acc: 0.5556 - val_loss: 1.5021 - val_acc: 0.0000e+00\n",
      "Epoch 33/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9524 - acc: 0.5556 - val_loss: 1.4942 - val_acc: 0.0000e+00\n",
      "Epoch 34/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9602 - acc: 0.5556 - val_loss: 1.4858 - val_acc: 0.0000e+00\n",
      "Epoch 35/2000\n",
      "18/18 [==============================] - 0s 167us/step - loss: 0.9478 - acc: 0.5556 - val_loss: 1.4768 - val_acc: 0.0000e+00\n",
      "Epoch 36/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9621 - acc: 0.5556 - val_loss: 1.4680 - val_acc: 0.0000e+00\n",
      "Epoch 37/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9504 - acc: 0.5556 - val_loss: 1.4593 - val_acc: 0.0000e+00\n",
      "Epoch 38/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9549 - acc: 0.5556 - val_loss: 1.4505 - val_acc: 0.0000e+00\n",
      "Epoch 39/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9376 - acc: 0.5556 - val_loss: 1.4418 - val_acc: 0.0000e+00\n",
      "Epoch 40/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.9440 - acc: 0.5556 - val_loss: 1.4337 - val_acc: 0.0000e+00\n",
      "Epoch 41/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9432 - acc: 0.5556 - val_loss: 1.4258 - val_acc: 0.0000e+00\n",
      "Epoch 42/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9586 - acc: 0.5556 - val_loss: 1.4183 - val_acc: 0.0000e+00\n",
      "Epoch 43/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9258 - acc: 0.5556 - val_loss: 1.4111 - val_acc: 0.0000e+00\n",
      "Epoch 44/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9417 - acc: 0.5556 - val_loss: 1.4046 - val_acc: 0.0000e+00\n",
      "Epoch 45/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9384 - acc: 0.5556 - val_loss: 1.3987 - val_acc: 0.0000e+00\n",
      "Epoch 46/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9334 - acc: 0.5556 - val_loss: 1.3926 - val_acc: 0.0000e+00\n",
      "Epoch 47/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9309 - acc: 0.5556 - val_loss: 1.3874 - val_acc: 0.0000e+00\n",
      "Epoch 48/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9362 - acc: 0.5556 - val_loss: 1.3827 - val_acc: 0.0000e+00\n",
      "Epoch 49/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9187 - acc: 0.5556 - val_loss: 1.3786 - val_acc: 0.0000e+00\n",
      "Epoch 50/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9215 - acc: 0.5556 - val_loss: 1.3747 - val_acc: 0.0000e+00\n",
      "Epoch 51/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9090 - acc: 0.5556 - val_loss: 1.3713 - val_acc: 0.0000e+00\n",
      "Epoch 52/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9251 - acc: 0.5556 - val_loss: 1.3683 - val_acc: 0.0000e+00\n",
      "Epoch 53/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9120 - acc: 0.5556 - val_loss: 1.3650 - val_acc: 0.0000e+00\n",
      "Epoch 54/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9051 - acc: 0.5556 - val_loss: 1.3612 - val_acc: 0.0000e+00\n",
      "Epoch 55/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.9133 - acc: 0.5556 - val_loss: 1.3585 - val_acc: 0.0000e+00\n",
      "Epoch 56/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9158 - acc: 0.5556 - val_loss: 1.3554 - val_acc: 0.0000e+00\n",
      "Epoch 57/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.9042 - acc: 0.5556 - val_loss: 1.3522 - val_acc: 0.0000e+00\n",
      "Epoch 58/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9380 - acc: 0.5556 - val_loss: 1.3500 - val_acc: 0.0000e+00\n",
      "Epoch 59/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9027 - acc: 0.5556 - val_loss: 1.3477 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8962 - acc: 0.5556 - val_loss: 1.3448 - val_acc: 0.0000e+00\n",
      "Epoch 61/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8872 - acc: 0.5556 - val_loss: 1.3418 - val_acc: 0.0000e+00\n",
      "Epoch 62/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9034 - acc: 0.5556 - val_loss: 1.3393 - val_acc: 0.0000e+00\n",
      "Epoch 63/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.9038 - acc: 0.5556 - val_loss: 1.3362 - val_acc: 0.0000e+00\n",
      "Epoch 64/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8781 - acc: 0.5556 - val_loss: 1.3332 - val_acc: 0.0000e+00\n",
      "Epoch 65/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8855 - acc: 0.5556 - val_loss: 1.3304 - val_acc: 0.0000e+00\n",
      "Epoch 66/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8857 - acc: 0.5556 - val_loss: 1.3279 - val_acc: 0.0000e+00\n",
      "Epoch 67/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8664 - acc: 0.5556 - val_loss: 1.3251 - val_acc: 0.0000e+00\n",
      "Epoch 68/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.8900 - acc: 0.5556 - val_loss: 1.3226 - val_acc: 0.0000e+00\n",
      "Epoch 69/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8680 - acc: 0.5556 - val_loss: 1.3197 - val_acc: 0.0000e+00\n",
      "Epoch 70/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8856 - acc: 0.5556 - val_loss: 1.3167 - val_acc: 0.0000e+00\n",
      "Epoch 71/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8474 - acc: 0.6111 - val_loss: 1.3142 - val_acc: 0.0000e+00\n",
      "Epoch 72/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8410 - acc: 0.6111 - val_loss: 1.3114 - val_acc: 0.0000e+00\n",
      "Epoch 73/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8544 - acc: 0.5556 - val_loss: 1.3089 - val_acc: 0.0000e+00\n",
      "Epoch 74/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8755 - acc: 0.6111 - val_loss: 1.3063 - val_acc: 0.0000e+00\n",
      "Epoch 75/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8672 - acc: 0.6111 - val_loss: 1.3046 - val_acc: 0.3333\n",
      "Epoch 76/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8421 - acc: 0.6111 - val_loss: 1.3027 - val_acc: 0.3333\n",
      "Epoch 77/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8256 - acc: 0.6667 - val_loss: 1.3016 - val_acc: 0.3333\n",
      "Epoch 78/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.8305 - acc: 0.6111 - val_loss: 1.3008 - val_acc: 0.3333\n",
      "Epoch 79/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8400 - acc: 0.6111 - val_loss: 1.3003 - val_acc: 0.3333\n",
      "Epoch 80/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8405 - acc: 0.6111 - val_loss: 1.2990 - val_acc: 0.3333\n",
      "Epoch 81/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8223 - acc: 0.6111 - val_loss: 1.2972 - val_acc: 0.3333\n",
      "Epoch 82/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8231 - acc: 0.6667 - val_loss: 1.2967 - val_acc: 0.3333\n",
      "Epoch 83/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7991 - acc: 0.6667 - val_loss: 1.2952 - val_acc: 0.3333\n",
      "Epoch 84/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8527 - acc: 0.6667 - val_loss: 1.2940 - val_acc: 0.3333\n",
      "Epoch 85/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7881 - acc: 0.6667 - val_loss: 1.2928 - val_acc: 0.3333\n",
      "Epoch 86/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7937 - acc: 0.6667 - val_loss: 1.2907 - val_acc: 0.3333\n",
      "Epoch 87/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.7851 - acc: 0.6667 - val_loss: 1.2873 - val_acc: 0.3333\n",
      "Epoch 88/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.7705 - acc: 0.6667 - val_loss: 1.2853 - val_acc: 0.3333\n",
      "Epoch 89/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7892 - acc: 0.6667 - val_loss: 1.2840 - val_acc: 0.3333\n",
      "Epoch 90/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7761 - acc: 0.6667 - val_loss: 1.2836 - val_acc: 0.3333\n",
      "Epoch 91/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7906 - acc: 0.6667 - val_loss: 1.2810 - val_acc: 0.3333\n",
      "Epoch 92/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7821 - acc: 0.6111 - val_loss: 1.2782 - val_acc: 0.3333\n",
      "Epoch 93/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7793 - acc: 0.6111 - val_loss: 1.2758 - val_acc: 0.3333\n",
      "Epoch 94/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.8120 - acc: 0.6667 - val_loss: 1.2758 - val_acc: 0.3333\n",
      "Epoch 95/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7796 - acc: 0.6111 - val_loss: 1.2750 - val_acc: 0.3333\n",
      "Epoch 96/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7517 - acc: 0.7222 - val_loss: 1.2739 - val_acc: 0.3333\n",
      "Epoch 97/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7596 - acc: 0.6667 - val_loss: 1.2739 - val_acc: 0.3333\n",
      "Epoch 98/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7219 - acc: 0.7222 - val_loss: 1.2742 - val_acc: 0.3333\n",
      "Epoch 99/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7867 - acc: 0.6667 - val_loss: 1.2739 - val_acc: 0.3333\n",
      "Epoch 100/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7387 - acc: 0.6667 - val_loss: 1.2757 - val_acc: 0.3333\n",
      "Epoch 101/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7069 - acc: 0.7222 - val_loss: 1.2767 - val_acc: 0.3333\n",
      "Epoch 102/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.7579 - acc: 0.7222 - val_loss: 1.2789 - val_acc: 0.3333\n",
      "Epoch 103/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7121 - acc: 0.7222 - val_loss: 1.2813 - val_acc: 0.3333\n",
      "Epoch 104/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6882 - acc: 0.7222 - val_loss: 1.2828 - val_acc: 0.3333\n",
      "Epoch 105/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6957 - acc: 0.7222 - val_loss: 1.2870 - val_acc: 0.3333\n",
      "Epoch 106/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7195 - acc: 0.7222 - val_loss: 1.2898 - val_acc: 0.3333\n",
      "Epoch 107/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.6776 - acc: 0.7222 - val_loss: 1.2956 - val_acc: 0.3333\n",
      "Epoch 108/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6693 - acc: 0.7222 - val_loss: 1.3013 - val_acc: 0.3333\n",
      "Epoch 109/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.7090 - acc: 0.6667 - val_loss: 1.3046 - val_acc: 0.3333\n",
      "Epoch 110/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6502 - acc: 0.7222 - val_loss: 1.3094 - val_acc: 0.3333\n",
      "Epoch 111/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6603 - acc: 0.7222 - val_loss: 1.3122 - val_acc: 0.3333\n",
      "Epoch 112/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6803 - acc: 0.6667 - val_loss: 1.3145 - val_acc: 0.3333\n",
      "Epoch 113/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6684 - acc: 0.7222 - val_loss: 1.3195 - val_acc: 0.3333\n",
      "Epoch 114/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6526 - acc: 0.7222 - val_loss: 1.3279 - val_acc: 0.3333\n",
      "Epoch 115/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6823 - acc: 0.7222 - val_loss: 1.3314 - val_acc: 0.3333\n",
      "Epoch 116/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6772 - acc: 0.6667 - val_loss: 1.3337 - val_acc: 0.3333\n",
      "Epoch 117/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.6283 - acc: 0.7222 - val_loss: 1.3377 - val_acc: 0.3333\n",
      "Epoch 118/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6062 - acc: 0.7222 - val_loss: 1.3399 - val_acc: 0.3333\n",
      "Epoch 119/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.6096 - acc: 0.7222 - val_loss: 1.3423 - val_acc: 0.3333\n",
      "Epoch 120/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 278us/step - loss: 0.6460 - acc: 0.7222 - val_loss: 1.3465 - val_acc: 0.3333\n",
      "Epoch 121/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.6133 - acc: 0.7222 - val_loss: 1.3518 - val_acc: 0.3333\n",
      "Epoch 122/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.6277 - acc: 0.7222 - val_loss: 1.3538 - val_acc: 0.3333\n",
      "Epoch 123/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.6408 - acc: 0.6667 - val_loss: 1.3537 - val_acc: 0.3333\n",
      "Epoch 124/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5845 - acc: 0.7778 - val_loss: 1.3549 - val_acc: 0.3333\n",
      "Epoch 125/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.5524 - acc: 0.7778 - val_loss: 1.3589 - val_acc: 0.3333\n",
      "Epoch 126/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5761 - acc: 0.7778 - val_loss: 1.3650 - val_acc: 0.3333\n",
      "Epoch 127/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5425 - acc: 0.7778 - val_loss: 1.3700 - val_acc: 0.3333\n",
      "Epoch 128/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5557 - acc: 0.7778 - val_loss: 1.3750 - val_acc: 0.3333\n",
      "Epoch 129/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.5696 - acc: 0.7222 - val_loss: 1.3818 - val_acc: 0.3333\n",
      "Epoch 130/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5835 - acc: 0.7778 - val_loss: 1.3889 - val_acc: 0.3333\n",
      "Epoch 131/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.5471 - acc: 0.7778 - val_loss: 1.3955 - val_acc: 0.3333\n",
      "Epoch 132/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.5322 - acc: 0.7778 - val_loss: 1.4013 - val_acc: 0.3333\n",
      "Epoch 133/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5604 - acc: 0.7778 - val_loss: 1.4070 - val_acc: 0.3333\n",
      "Epoch 134/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5200 - acc: 0.7778 - val_loss: 1.4135 - val_acc: 0.3333\n",
      "Epoch 135/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5414 - acc: 0.7778 - val_loss: 1.4195 - val_acc: 0.3333\n",
      "Epoch 136/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.5121 - acc: 0.7778 - val_loss: 1.4280 - val_acc: 0.3333\n",
      "Epoch 137/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4979 - acc: 0.7778 - val_loss: 1.4363 - val_acc: 0.3333\n",
      "Epoch 138/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4795 - acc: 0.7778 - val_loss: 1.4426 - val_acc: 0.3333\n",
      "Epoch 139/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4801 - acc: 0.7778 - val_loss: 1.4488 - val_acc: 0.3333\n",
      "Epoch 140/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4741 - acc: 0.7778 - val_loss: 1.4530 - val_acc: 0.3333\n",
      "Epoch 141/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.4881 - acc: 0.7778 - val_loss: 1.4578 - val_acc: 0.3333\n",
      "Epoch 142/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4641 - acc: 0.7778 - val_loss: 1.4636 - val_acc: 0.3333\n",
      "Epoch 143/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 1.4697 - val_acc: 0.3333\n",
      "Epoch 144/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4959 - acc: 0.7778 - val_loss: 1.4747 - val_acc: 0.3333\n",
      "Epoch 145/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4438 - acc: 0.7778 - val_loss: 1.4797 - val_acc: 0.3333\n",
      "Epoch 146/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4431 - acc: 0.7778 - val_loss: 1.4836 - val_acc: 0.3333\n",
      "Epoch 147/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4386 - acc: 0.7778 - val_loss: 1.4884 - val_acc: 0.3333\n",
      "Epoch 148/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4393 - acc: 0.7778 - val_loss: 1.4920 - val_acc: 0.3333\n",
      "Epoch 149/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4348 - acc: 0.7778 - val_loss: 1.4965 - val_acc: 0.3333\n",
      "Epoch 150/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4256 - acc: 0.7778 - val_loss: 1.5017 - val_acc: 0.3333\n",
      "Epoch 151/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4033 - acc: 0.7778 - val_loss: 1.5073 - val_acc: 0.3333\n",
      "Epoch 152/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4131 - acc: 0.8333 - val_loss: 1.5131 - val_acc: 0.3333\n",
      "Epoch 153/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4227 - acc: 0.7778 - val_loss: 1.5167 - val_acc: 0.3333\n",
      "Epoch 154/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3907 - acc: 0.7778 - val_loss: 1.5220 - val_acc: 0.3333\n",
      "Epoch 155/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4338 - acc: 0.7778 - val_loss: 1.5254 - val_acc: 0.3333\n",
      "Epoch 156/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4185 - acc: 0.8889 - val_loss: 1.5295 - val_acc: 0.3333\n",
      "Epoch 157/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.4235 - acc: 0.7778 - val_loss: 1.5320 - val_acc: 0.3333\n",
      "Epoch 158/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.3881 - acc: 0.8889 - val_loss: 1.5354 - val_acc: 0.3333\n",
      "Epoch 159/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3623 - acc: 0.8333 - val_loss: 1.5402 - val_acc: 0.3333\n",
      "Epoch 160/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3964 - acc: 0.8333 - val_loss: 1.5445 - val_acc: 0.3333\n",
      "Epoch 161/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.4037 - acc: 0.7778 - val_loss: 1.5494 - val_acc: 0.3333\n",
      "Epoch 162/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.3655 - acc: 0.7778 - val_loss: 1.5546 - val_acc: 0.3333\n",
      "Epoch 163/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.3524 - acc: 0.8889 - val_loss: 1.5595 - val_acc: 0.3333\n",
      "Epoch 164/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3559 - acc: 0.8333 - val_loss: 1.5639 - val_acc: 0.3333\n",
      "Epoch 165/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3293 - acc: 0.8889 - val_loss: 1.5695 - val_acc: 0.3333\n",
      "Epoch 166/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3531 - acc: 0.8889 - val_loss: 1.5754 - val_acc: 0.3333\n",
      "Epoch 167/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3202 - acc: 0.8889 - val_loss: 1.5805 - val_acc: 0.3333\n",
      "Epoch 168/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.3404 - acc: 0.9444 - val_loss: 1.5867 - val_acc: 0.3333\n",
      "Epoch 169/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3225 - acc: 0.9444 - val_loss: 1.5925 - val_acc: 0.3333\n",
      "Epoch 170/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.3108 - acc: 0.9444 - val_loss: 1.6006 - val_acc: 0.3333\n",
      "Epoch 171/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2843 - acc: 1.0000 - val_loss: 1.6076 - val_acc: 0.3333\n",
      "Epoch 172/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3241 - acc: 0.8889 - val_loss: 1.6147 - val_acc: 0.3333\n",
      "Epoch 173/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3356 - acc: 0.9444 - val_loss: 1.6222 - val_acc: 0.3333\n",
      "Epoch 174/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2710 - acc: 1.0000 - val_loss: 1.6296 - val_acc: 0.3333\n",
      "Epoch 175/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.3314 - acc: 0.8333 - val_loss: 1.6300 - val_acc: 0.3333\n",
      "Epoch 176/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3075 - acc: 0.9444 - val_loss: 1.6296 - val_acc: 0.3333\n",
      "Epoch 177/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3010 - acc: 1.0000 - val_loss: 1.6349 - val_acc: 0.3333\n",
      "Epoch 178/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2919 - acc: 0.9444 - val_loss: 1.6391 - val_acc: 0.3333\n",
      "Epoch 179/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2682 - acc: 1.0000 - val_loss: 1.6462 - val_acc: 0.3333\n",
      "Epoch 180/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.3010 - acc: 0.8889 - val_loss: 1.6486 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2712 - acc: 1.0000 - val_loss: 1.6554 - val_acc: 0.3333\n",
      "Epoch 182/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2798 - acc: 0.9444 - val_loss: 1.6553 - val_acc: 0.3333\n",
      "Epoch 183/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2159 - acc: 1.0000 - val_loss: 1.6563 - val_acc: 0.3333\n",
      "Epoch 184/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2602 - acc: 1.0000 - val_loss: 1.6584 - val_acc: 0.3333\n",
      "Epoch 185/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.2485 - acc: 1.0000 - val_loss: 1.6608 - val_acc: 0.3333\n",
      "Epoch 186/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2180 - acc: 0.9444 - val_loss: 1.6616 - val_acc: 0.3333\n",
      "Epoch 187/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.2111 - acc: 1.0000 - val_loss: 1.6619 - val_acc: 0.3333\n",
      "Epoch 188/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2208 - acc: 1.0000 - val_loss: 1.6667 - val_acc: 0.3333\n",
      "Epoch 189/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2381 - acc: 1.0000 - val_loss: 1.6737 - val_acc: 0.3333\n",
      "Epoch 190/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2262 - acc: 1.0000 - val_loss: 1.6811 - val_acc: 0.3333\n",
      "Epoch 191/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2061 - acc: 1.0000 - val_loss: 1.6892 - val_acc: 0.3333\n",
      "Epoch 192/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2081 - acc: 1.0000 - val_loss: 1.7016 - val_acc: 0.3333\n",
      "Epoch 193/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.2194 - acc: 1.0000 - val_loss: 1.7116 - val_acc: 0.3333\n",
      "Epoch 194/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2096 - acc: 1.0000 - val_loss: 1.7186 - val_acc: 0.3333\n",
      "Epoch 195/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.2057 - acc: 1.0000 - val_loss: 1.7270 - val_acc: 0.3333\n",
      "Epoch 196/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1959 - acc: 1.0000 - val_loss: 1.7311 - val_acc: 0.3333\n",
      "Epoch 197/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.2065 - acc: 1.0000 - val_loss: 1.7359 - val_acc: 0.3333\n",
      "Epoch 198/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.2078 - acc: 1.0000 - val_loss: 1.7420 - val_acc: 0.3333\n",
      "Epoch 199/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.2111 - acc: 0.9444 - val_loss: 1.7430 - val_acc: 0.3333\n",
      "Epoch 200/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1677 - acc: 1.0000 - val_loss: 1.7534 - val_acc: 0.3333\n",
      "Epoch 201/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1801 - acc: 1.0000 - val_loss: 1.7597 - val_acc: 0.3333\n",
      "Epoch 202/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1797 - acc: 1.0000 - val_loss: 1.7703 - val_acc: 0.3333\n",
      "Epoch 203/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1597 - acc: 1.0000 - val_loss: 1.7794 - val_acc: 0.3333\n",
      "Epoch 204/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1637 - acc: 1.0000 - val_loss: 1.7886 - val_acc: 0.3333\n",
      "Epoch 205/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1631 - acc: 1.0000 - val_loss: 1.7862 - val_acc: 0.3333\n",
      "Epoch 206/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1742 - acc: 1.0000 - val_loss: 1.7952 - val_acc: 0.3333\n",
      "Epoch 207/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1747 - acc: 1.0000 - val_loss: 1.7945 - val_acc: 0.3333\n",
      "Epoch 208/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1309 - acc: 1.0000 - val_loss: 1.8009 - val_acc: 0.3333\n",
      "Epoch 209/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1496 - acc: 1.0000 - val_loss: 1.8062 - val_acc: 0.3333\n",
      "Epoch 210/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1311 - acc: 1.0000 - val_loss: 1.8125 - val_acc: 0.3333\n",
      "Epoch 211/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1568 - acc: 1.0000 - val_loss: 1.8144 - val_acc: 0.3333\n",
      "Epoch 212/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.2115 - acc: 0.9444 - val_loss: 1.8136 - val_acc: 0.3333\n",
      "Epoch 213/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1376 - acc: 1.0000 - val_loss: 1.8125 - val_acc: 0.3333\n",
      "Epoch 214/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1683 - acc: 1.0000 - val_loss: 1.8263 - val_acc: 0.3333\n",
      "Epoch 215/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1590 - acc: 1.0000 - val_loss: 1.8290 - val_acc: 0.3333\n",
      "Epoch 216/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1210 - acc: 1.0000 - val_loss: 1.8318 - val_acc: 0.3333\n",
      "Epoch 217/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1157 - acc: 1.0000 - val_loss: 1.8372 - val_acc: 0.3333\n",
      "Epoch 218/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1172 - acc: 1.0000 - val_loss: 1.8425 - val_acc: 0.3333\n",
      "Epoch 219/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1442 - acc: 1.0000 - val_loss: 1.8488 - val_acc: 0.3333\n",
      "Epoch 220/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1396 - acc: 1.0000 - val_loss: 1.8547 - val_acc: 0.3333\n",
      "Epoch 221/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1286 - acc: 1.0000 - val_loss: 1.8578 - val_acc: 0.3333\n",
      "Epoch 222/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1356 - acc: 1.0000 - val_loss: 1.8614 - val_acc: 0.3333\n",
      "Epoch 223/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0998 - acc: 1.0000 - val_loss: 1.8656 - val_acc: 0.3333\n",
      "Epoch 224/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1118 - acc: 1.0000 - val_loss: 1.8693 - val_acc: 0.3333\n",
      "Epoch 225/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1155 - acc: 1.0000 - val_loss: 1.8765 - val_acc: 0.3333\n",
      "Epoch 226/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1010 - acc: 1.0000 - val_loss: 1.8819 - val_acc: 0.3333\n",
      "Epoch 227/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0902 - acc: 1.0000 - val_loss: 1.8878 - val_acc: 0.3333\n",
      "Epoch 228/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1074 - acc: 1.0000 - val_loss: 1.8935 - val_acc: 0.3333\n",
      "Epoch 229/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1290 - acc: 1.0000 - val_loss: 1.8831 - val_acc: 0.3333\n",
      "Epoch 230/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0933 - acc: 1.0000 - val_loss: 1.8807 - val_acc: 0.3333\n",
      "Epoch 231/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1071 - acc: 1.0000 - val_loss: 1.8707 - val_acc: 0.3333\n",
      "Epoch 232/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1055 - acc: 1.0000 - val_loss: 1.8625 - val_acc: 0.3333\n",
      "Epoch 233/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1267 - acc: 1.0000 - val_loss: 1.8557 - val_acc: 0.3333\n",
      "Epoch 234/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0973 - acc: 1.0000 - val_loss: 1.8528 - val_acc: 0.3333\n",
      "Epoch 235/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1047 - acc: 1.0000 - val_loss: 1.8478 - val_acc: 0.3333\n",
      "Epoch 236/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0931 - acc: 1.0000 - val_loss: 1.8521 - val_acc: 0.3333\n",
      "Epoch 237/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0970 - acc: 1.0000 - val_loss: 1.8674 - val_acc: 0.3333\n",
      "Epoch 238/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0887 - acc: 1.0000 - val_loss: 1.8751 - val_acc: 0.3333\n",
      "Epoch 239/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0857 - acc: 1.0000 - val_loss: 1.8844 - val_acc: 0.3333\n",
      "Epoch 240/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1001 - acc: 1.0000 - val_loss: 1.8971 - val_acc: 0.3333\n",
      "Epoch 241/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0858 - acc: 1.0000 - val_loss: 1.9061 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0917 - acc: 1.0000 - val_loss: 1.9228 - val_acc: 0.3333\n",
      "Epoch 243/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0750 - acc: 1.0000 - val_loss: 1.9387 - val_acc: 0.3333\n",
      "Epoch 244/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0772 - acc: 1.0000 - val_loss: 1.9574 - val_acc: 0.3333\n",
      "Epoch 245/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0865 - acc: 1.0000 - val_loss: 1.9734 - val_acc: 0.3333\n",
      "Epoch 246/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.1064 - acc: 1.0000 - val_loss: 1.9803 - val_acc: 0.3333\n",
      "Epoch 247/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0856 - acc: 1.0000 - val_loss: 1.9750 - val_acc: 0.3333\n",
      "Epoch 248/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0844 - acc: 1.0000 - val_loss: 1.9772 - val_acc: 0.3333\n",
      "Epoch 249/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0832 - acc: 1.0000 - val_loss: 1.9880 - val_acc: 0.3333\n",
      "Epoch 250/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0724 - acc: 1.0000 - val_loss: 1.9905 - val_acc: 0.3333\n",
      "Epoch 251/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0747 - acc: 1.0000 - val_loss: 1.9914 - val_acc: 0.3333\n",
      "Epoch 252/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0832 - acc: 1.0000 - val_loss: 1.9998 - val_acc: 0.3333\n",
      "Epoch 253/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0739 - acc: 1.0000 - val_loss: 2.0107 - val_acc: 0.3333\n",
      "Epoch 254/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0666 - acc: 1.0000 - val_loss: 2.0210 - val_acc: 0.3333\n",
      "Epoch 255/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0763 - acc: 1.0000 - val_loss: 2.0179 - val_acc: 0.3333\n",
      "Epoch 256/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0865 - acc: 1.0000 - val_loss: 1.9997 - val_acc: 0.3333\n",
      "Epoch 257/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0680 - acc: 1.0000 - val_loss: 1.9935 - val_acc: 0.3333\n",
      "Epoch 258/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0579 - acc: 1.0000 - val_loss: 1.9887 - val_acc: 0.3333\n",
      "Epoch 259/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0776 - acc: 1.0000 - val_loss: 1.9841 - val_acc: 0.3333\n",
      "Epoch 260/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0753 - acc: 1.0000 - val_loss: 1.9829 - val_acc: 0.3333\n",
      "Epoch 261/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0692 - acc: 1.0000 - val_loss: 1.9811 - val_acc: 0.3333\n",
      "Epoch 262/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0757 - acc: 1.0000 - val_loss: 1.9840 - val_acc: 0.3333\n",
      "Epoch 263/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0692 - acc: 1.0000 - val_loss: 1.9965 - val_acc: 0.3333\n",
      "Epoch 264/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0680 - acc: 1.0000 - val_loss: 2.0085 - val_acc: 0.3333\n",
      "Epoch 265/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 2.0211 - val_acc: 0.3333\n",
      "Epoch 266/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0714 - acc: 1.0000 - val_loss: 2.0365 - val_acc: 0.3333\n",
      "Epoch 267/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0628 - acc: 1.0000 - val_loss: 2.0446 - val_acc: 0.3333\n",
      "Epoch 268/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0591 - acc: 1.0000 - val_loss: 2.0593 - val_acc: 0.3333\n",
      "Epoch 269/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0704 - acc: 1.0000 - val_loss: 2.0753 - val_acc: 0.3333\n",
      "Epoch 270/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0583 - acc: 1.0000 - val_loss: 2.0849 - val_acc: 0.3333\n",
      "Epoch 271/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0544 - acc: 1.0000 - val_loss: 2.0962 - val_acc: 0.3333\n",
      "Epoch 272/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0647 - acc: 1.0000 - val_loss: 2.1098 - val_acc: 0.3333\n",
      "Epoch 273/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0598 - acc: 1.0000 - val_loss: 2.1225 - val_acc: 0.3333\n",
      "Epoch 274/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0493 - acc: 1.0000 - val_loss: 2.1323 - val_acc: 0.3333\n",
      "Epoch 275/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0693 - acc: 1.0000 - val_loss: 2.1174 - val_acc: 0.3333\n",
      "Epoch 276/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0491 - acc: 1.0000 - val_loss: 2.1160 - val_acc: 0.3333\n",
      "Epoch 277/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0608 - acc: 1.0000 - val_loss: 2.1137 - val_acc: 0.3333\n",
      "Epoch 278/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0626 - acc: 1.0000 - val_loss: 2.1088 - val_acc: 0.3333\n",
      "Epoch 279/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0612 - acc: 1.0000 - val_loss: 2.1104 - val_acc: 0.3333\n",
      "Epoch 280/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0536 - acc: 1.0000 - val_loss: 2.1097 - val_acc: 0.3333\n",
      "Epoch 281/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0468 - acc: 1.0000 - val_loss: 2.1103 - val_acc: 0.3333\n",
      "Epoch 282/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0534 - acc: 1.0000 - val_loss: 2.1106 - val_acc: 0.3333\n",
      "Epoch 283/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0863 - acc: 1.0000 - val_loss: 2.0925 - val_acc: 0.3333\n",
      "Epoch 284/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0597 - acc: 1.0000 - val_loss: 2.0894 - val_acc: 0.3333\n",
      "Epoch 285/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0481 - acc: 1.0000 - val_loss: 2.0858 - val_acc: 0.3333\n",
      "Epoch 286/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0530 - acc: 1.0000 - val_loss: 2.0874 - val_acc: 0.3333\n",
      "Epoch 287/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0552 - acc: 1.0000 - val_loss: 2.0816 - val_acc: 0.3333\n",
      "Epoch 288/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0539 - acc: 1.0000 - val_loss: 2.0809 - val_acc: 0.3333\n",
      "Epoch 289/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0446 - acc: 1.0000 - val_loss: 2.0785 - val_acc: 0.3333\n",
      "Epoch 290/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0488 - acc: 1.0000 - val_loss: 2.0800 - val_acc: 0.3333\n",
      "Epoch 291/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0461 - acc: 1.0000 - val_loss: 2.0783 - val_acc: 0.3333\n",
      "Epoch 292/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0525 - acc: 1.0000 - val_loss: 2.0716 - val_acc: 0.3333\n",
      "Epoch 293/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0529 - acc: 1.0000 - val_loss: 2.0805 - val_acc: 0.3333\n",
      "Epoch 294/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0434 - acc: 1.0000 - val_loss: 2.0818 - val_acc: 0.3333\n",
      "Epoch 295/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0591 - acc: 1.0000 - val_loss: 2.0940 - val_acc: 0.3333\n",
      "Epoch 296/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0465 - acc: 1.0000 - val_loss: 2.1050 - val_acc: 0.3333\n",
      "Epoch 297/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0504 - acc: 1.0000 - val_loss: 2.1244 - val_acc: 0.3333\n",
      "Epoch 298/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0464 - acc: 1.0000 - val_loss: 2.1383 - val_acc: 0.3333\n",
      "Epoch 299/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0413 - acc: 1.0000 - val_loss: 2.1496 - val_acc: 0.3333\n",
      "Epoch 300/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0469 - acc: 1.0000 - val_loss: 2.1539 - val_acc: 0.3333\n",
      "Epoch 301/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0503 - acc: 1.0000 - val_loss: 2.1512 - val_acc: 0.3333\n",
      "Epoch 302/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 2.1516 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 2.1521 - val_acc: 0.3333\n",
      "Epoch 304/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0551 - acc: 1.0000 - val_loss: 2.1570 - val_acc: 0.3333\n",
      "Epoch 305/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0424 - acc: 1.0000 - val_loss: 2.1585 - val_acc: 0.3333\n",
      "Epoch 306/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0487 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.3333\n",
      "Epoch 307/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0500 - acc: 1.0000 - val_loss: 2.1804 - val_acc: 0.3333\n",
      "Epoch 308/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 2.1870 - val_acc: 0.3333\n",
      "Epoch 309/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0436 - acc: 1.0000 - val_loss: 2.1957 - val_acc: 0.3333\n",
      "Epoch 310/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0625 - acc: 1.0000 - val_loss: 2.2038 - val_acc: 0.3333\n",
      "Epoch 311/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0401 - acc: 1.0000 - val_loss: 2.2100 - val_acc: 0.3333\n",
      "Epoch 312/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0384 - acc: 1.0000 - val_loss: 2.2169 - val_acc: 0.3333\n",
      "Epoch 313/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0470 - acc: 1.0000 - val_loss: 2.2215 - val_acc: 0.3333\n",
      "Epoch 314/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 2.2265 - val_acc: 0.3333\n",
      "Epoch 315/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0431 - acc: 1.0000 - val_loss: 2.2331 - val_acc: 0.3333\n",
      "Epoch 316/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0509 - acc: 1.0000 - val_loss: 2.2613 - val_acc: 0.3333\n",
      "Epoch 317/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0405 - acc: 1.0000 - val_loss: 2.2748 - val_acc: 0.3333\n",
      "Epoch 318/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 2.2855 - val_acc: 0.3333\n",
      "Epoch 319/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0500 - acc: 1.0000 - val_loss: 2.2844 - val_acc: 0.3333\n",
      "Epoch 320/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0491 - acc: 1.0000 - val_loss: 2.2715 - val_acc: 0.3333\n",
      "Epoch 321/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0382 - acc: 1.0000 - val_loss: 2.2732 - val_acc: 0.3333\n",
      "Epoch 322/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 2.2669 - val_acc: 0.3333\n",
      "Epoch 323/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0387 - acc: 1.0000 - val_loss: 2.2587 - val_acc: 0.3333\n",
      "Epoch 324/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0389 - acc: 1.0000 - val_loss: 2.2518 - val_acc: 0.3333\n",
      "Epoch 325/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0445 - acc: 1.0000 - val_loss: 2.2490 - val_acc: 0.3333\n",
      "Epoch 326/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0466 - acc: 1.0000 - val_loss: 2.2524 - val_acc: 0.3333\n",
      "Epoch 327/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 2.2586 - val_acc: 0.3333\n",
      "Epoch 328/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0391 - acc: 1.0000 - val_loss: 2.2587 - val_acc: 0.3333\n",
      "Epoch 329/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0377 - acc: 1.0000 - val_loss: 2.2689 - val_acc: 0.3333\n",
      "Epoch 330/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 2.2702 - val_acc: 0.3333\n",
      "Epoch 331/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0408 - acc: 1.0000 - val_loss: 2.2746 - val_acc: 0.3333\n",
      "Epoch 332/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.1136 - acc: 0.9444 - val_loss: 2.3032 - val_acc: 0.3333\n",
      "Epoch 333/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0386 - acc: 1.0000 - val_loss: 2.3152 - val_acc: 0.3333\n",
      "Epoch 334/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0372 - acc: 1.0000 - val_loss: 2.3283 - val_acc: 0.3333\n",
      "Epoch 335/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 2.3394 - val_acc: 0.3333\n",
      "Epoch 336/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0358 - acc: 1.0000 - val_loss: 2.3496 - val_acc: 0.3333\n",
      "Epoch 337/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0348 - acc: 1.0000 - val_loss: 2.3582 - val_acc: 0.3333\n",
      "Epoch 338/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0327 - acc: 1.0000 - val_loss: 2.3675 - val_acc: 0.3333\n",
      "Epoch 339/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 2.3700 - val_acc: 0.3333\n",
      "Epoch 340/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 2.3742 - val_acc: 0.3333\n",
      "Epoch 341/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0558 - acc: 1.0000 - val_loss: 2.3559 - val_acc: 0.3333\n",
      "Epoch 342/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0429 - acc: 1.0000 - val_loss: 2.3563 - val_acc: 0.3333\n",
      "Epoch 343/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 2.3514 - val_acc: 0.3333\n",
      "Epoch 344/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 2.3487 - val_acc: 0.3333\n",
      "Epoch 345/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0444 - acc: 1.0000 - val_loss: 2.3490 - val_acc: 0.3333\n",
      "Epoch 346/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 2.3453 - val_acc: 0.3333\n",
      "Epoch 347/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0331 - acc: 1.0000 - val_loss: 2.3380 - val_acc: 0.3333\n",
      "Epoch 348/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0558 - acc: 1.0000 - val_loss: 2.3798 - val_acc: 0.3333\n",
      "Epoch 349/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0384 - acc: 1.0000 - val_loss: 2.4029 - val_acc: 0.3333\n",
      "Epoch 350/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0321 - acc: 1.0000 - val_loss: 2.4160 - val_acc: 0.3333\n",
      "Epoch 351/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 2.4313 - val_acc: 0.3333\n",
      "Epoch 352/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 2.4472 - val_acc: 0.3333\n",
      "Epoch 353/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0318 - acc: 1.0000 - val_loss: 2.4607 - val_acc: 0.3333\n",
      "Epoch 354/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0444 - acc: 1.0000 - val_loss: 2.4729 - val_acc: 0.3333\n",
      "Epoch 355/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 2.4805 - val_acc: 0.3333\n",
      "Epoch 356/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 2.4889 - val_acc: 0.3333\n",
      "Epoch 357/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 2.4924 - val_acc: 0.3333\n",
      "Epoch 358/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0284 - acc: 1.0000 - val_loss: 2.4979 - val_acc: 0.3333\n",
      "Epoch 359/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0566 - acc: 1.0000 - val_loss: 2.5210 - val_acc: 0.3333\n",
      "Epoch 360/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 2.5291 - val_acc: 0.3333\n",
      "Epoch 361/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 2.5385 - val_acc: 0.3333\n",
      "Epoch 362/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0360 - acc: 1.0000 - val_loss: 2.5380 - val_acc: 0.3333\n",
      "Epoch 363/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 2.5424 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0295 - acc: 1.0000 - val_loss: 2.5428 - val_acc: 0.3333\n",
      "Epoch 365/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 2.5431 - val_acc: 0.3333\n",
      "Epoch 366/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 2.5393 - val_acc: 0.3333\n",
      "Epoch 367/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 2.5336 - val_acc: 0.3333\n",
      "Epoch 368/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 2.5354 - val_acc: 0.3333\n",
      "Epoch 369/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 2.5353 - val_acc: 0.3333\n",
      "Epoch 370/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0284 - acc: 1.0000 - val_loss: 2.5332 - val_acc: 0.3333\n",
      "Epoch 371/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0425 - acc: 1.0000 - val_loss: 2.5032 - val_acc: 0.3333\n",
      "Epoch 372/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0384 - acc: 1.0000 - val_loss: 2.4843 - val_acc: 0.3333\n",
      "Epoch 373/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 2.4675 - val_acc: 0.3333\n",
      "Epoch 374/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 2.4531 - val_acc: 0.3333\n",
      "Epoch 375/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 2.4356 - val_acc: 0.3333\n",
      "Epoch 376/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 2.4193 - val_acc: 0.3333\n",
      "Epoch 377/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 2.4031 - val_acc: 0.3333\n",
      "Epoch 378/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0262 - acc: 1.0000 - val_loss: 2.3904 - val_acc: 0.3333\n",
      "Epoch 379/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0272 - acc: 1.0000 - val_loss: 2.3813 - val_acc: 0.3333\n",
      "Epoch 380/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0322 - acc: 1.0000 - val_loss: 2.3713 - val_acc: 0.3333\n",
      "Epoch 381/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0257 - acc: 1.0000 - val_loss: 2.3619 - val_acc: 0.3333\n",
      "Epoch 382/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 2.3565 - val_acc: 0.3333\n",
      "Epoch 383/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 2.3482 - val_acc: 0.3333\n",
      "Epoch 384/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 2.3423 - val_acc: 0.3333\n",
      "Epoch 385/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0270 - acc: 1.0000 - val_loss: 2.3362 - val_acc: 0.3333\n",
      "Epoch 386/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0295 - acc: 1.0000 - val_loss: 2.3295 - val_acc: 0.3333\n",
      "Epoch 387/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 2.3144 - val_acc: 0.3333\n",
      "Epoch 388/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 2.3079 - val_acc: 0.3333\n",
      "Epoch 389/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0268 - acc: 1.0000 - val_loss: 2.3003 - val_acc: 0.3333\n",
      "Epoch 390/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 2.2957 - val_acc: 0.3333\n",
      "Epoch 391/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 2.2848 - val_acc: 0.3333\n",
      "Epoch 392/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 2.2789 - val_acc: 0.3333\n",
      "Epoch 393/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0291 - acc: 1.0000 - val_loss: 2.2791 - val_acc: 0.3333\n",
      "Epoch 394/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 2.2789 - val_acc: 0.3333\n",
      "Epoch 395/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 2.2797 - val_acc: 0.3333\n",
      "Epoch 396/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 2.2793 - val_acc: 0.3333\n",
      "Epoch 397/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 2.2794 - val_acc: 0.3333\n",
      "Epoch 398/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 2.2753 - val_acc: 0.3333\n",
      "Epoch 399/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0229 - acc: 1.0000 - val_loss: 2.2745 - val_acc: 0.3333\n",
      "Epoch 400/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 2.2735 - val_acc: 0.3333\n",
      "Epoch 401/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 2.2738 - val_acc: 0.3333\n",
      "Epoch 402/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0259 - acc: 1.0000 - val_loss: 2.2738 - val_acc: 0.3333\n",
      "Epoch 403/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 2.2818 - val_acc: 0.3333\n",
      "Epoch 404/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 2.2857 - val_acc: 0.3333\n",
      "Epoch 405/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 2.2898 - val_acc: 0.3333\n",
      "Epoch 406/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0360 - acc: 1.0000 - val_loss: 2.2631 - val_acc: 0.3333\n",
      "Epoch 407/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 2.2607 - val_acc: 0.3333\n",
      "Epoch 408/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 2.2559 - val_acc: 0.3333\n",
      "Epoch 409/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0225 - acc: 1.0000 - val_loss: 2.2518 - val_acc: 0.3333\n",
      "Epoch 410/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0304 - acc: 1.0000 - val_loss: 2.2504 - val_acc: 0.3333\n",
      "Epoch 411/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 2.2486 - val_acc: 0.3333\n",
      "Epoch 412/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 2.2543 - val_acc: 0.3333\n",
      "Epoch 413/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 2.2917 - val_acc: 0.3333\n",
      "Epoch 414/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 2.3095 - val_acc: 0.3333\n",
      "Epoch 415/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 2.3272 - val_acc: 0.3333\n",
      "Epoch 416/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 2.3458 - val_acc: 0.3333\n",
      "Epoch 417/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 2.3595 - val_acc: 0.3333\n",
      "Epoch 418/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 2.3714 - val_acc: 0.3333\n",
      "Epoch 419/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0231 - acc: 1.0000 - val_loss: 2.3840 - val_acc: 0.3333\n",
      "Epoch 420/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0344 - acc: 1.0000 - val_loss: 2.3831 - val_acc: 0.3333\n",
      "Epoch 421/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 2.3725 - val_acc: 0.3333\n",
      "Epoch 422/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0231 - acc: 1.0000 - val_loss: 2.3720 - val_acc: 0.3333\n",
      "Epoch 423/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 2.3701 - val_acc: 0.3333\n",
      "Epoch 424/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 2.3684 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 2.3650 - val_acc: 0.3333\n",
      "Epoch 426/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 2.3645 - val_acc: 0.3333\n",
      "Epoch 427/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0299 - acc: 1.0000 - val_loss: 2.3828 - val_acc: 0.3333\n",
      "Epoch 428/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0214 - acc: 1.0000 - val_loss: 2.3902 - val_acc: 0.3333\n",
      "Epoch 429/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 2.3987 - val_acc: 0.3333\n",
      "Epoch 430/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 2.4061 - val_acc: 0.3333\n",
      "Epoch 431/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 2.4131 - val_acc: 0.3333\n",
      "Epoch 432/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 2.4199 - val_acc: 0.3333\n",
      "Epoch 433/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 2.4295 - val_acc: 0.3333\n",
      "Epoch 434/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 2.4312 - val_acc: 0.3333\n",
      "Epoch 435/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 2.4360 - val_acc: 0.3333\n",
      "Epoch 436/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 2.4404 - val_acc: 0.3333\n",
      "Epoch 437/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 2.4602 - val_acc: 0.3333\n",
      "Epoch 438/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 2.4710 - val_acc: 0.3333\n",
      "Epoch 439/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0253 - acc: 1.0000 - val_loss: 2.4777 - val_acc: 0.3333\n",
      "Epoch 440/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 2.4854 - val_acc: 0.3333\n",
      "Epoch 441/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 2.4926 - val_acc: 0.3333\n",
      "Epoch 442/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 2.4991 - val_acc: 0.3333\n",
      "Epoch 443/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 2.5035 - val_acc: 0.3333\n",
      "Epoch 444/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 2.5011 - val_acc: 0.3333\n",
      "Epoch 445/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 2.5021 - val_acc: 0.3333\n",
      "Epoch 446/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0269 - acc: 1.0000 - val_loss: 2.5037 - val_acc: 0.3333\n",
      "Epoch 447/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 2.5056 - val_acc: 0.3333\n",
      "Epoch 448/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 2.5073 - val_acc: 0.3333\n",
      "Epoch 449/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 2.5075 - val_acc: 0.3333\n",
      "Epoch 450/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 2.5281 - val_acc: 0.3333\n",
      "Epoch 451/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 2.5379 - val_acc: 0.3333\n",
      "Epoch 452/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 2.5464 - val_acc: 0.3333\n",
      "Epoch 453/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 2.5455 - val_acc: 0.3333\n",
      "Epoch 454/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 2.5479 - val_acc: 0.3333\n",
      "Epoch 455/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 2.5583 - val_acc: 0.3333\n",
      "Epoch 456/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 2.5639 - val_acc: 0.3333\n",
      "Epoch 457/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 2.5685 - val_acc: 0.3333\n",
      "Epoch 458/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0229 - acc: 1.0000 - val_loss: 2.5716 - val_acc: 0.3333\n",
      "Epoch 459/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 2.5749 - val_acc: 0.3333\n",
      "Epoch 460/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 2.5801 - val_acc: 0.3333\n",
      "Epoch 461/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 2.5836 - val_acc: 0.3333\n",
      "Epoch 462/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 2.5868 - val_acc: 0.3333\n",
      "Epoch 463/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 2.5911 - val_acc: 0.3333\n",
      "Epoch 464/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 2.5933 - val_acc: 0.3333\n",
      "Epoch 465/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 2.5951 - val_acc: 0.3333\n",
      "Epoch 466/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 2.5873 - val_acc: 0.3333\n",
      "Epoch 467/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 2.5834 - val_acc: 0.3333\n",
      "Epoch 468/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 2.5833 - val_acc: 0.3333\n",
      "Epoch 469/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 2.5821 - val_acc: 0.3333\n",
      "Epoch 470/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 2.5854 - val_acc: 0.3333\n",
      "Epoch 471/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 2.5859 - val_acc: 0.3333\n",
      "Epoch 472/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 2.5846 - val_acc: 0.3333\n",
      "Epoch 473/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 2.5832 - val_acc: 0.3333\n",
      "Epoch 474/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 2.5789 - val_acc: 0.3333\n",
      "Epoch 475/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 2.5767 - val_acc: 0.3333\n",
      "Epoch 476/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 2.5724 - val_acc: 0.3333\n",
      "Epoch 477/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 2.5701 - val_acc: 0.3333\n",
      "Epoch 478/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 2.5669 - val_acc: 0.3333\n",
      "Epoch 479/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 2.5645 - val_acc: 0.3333\n",
      "Epoch 480/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 2.5599 - val_acc: 0.3333\n",
      "Epoch 481/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 2.5573 - val_acc: 0.3333\n",
      "Epoch 482/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 2.5550 - val_acc: 0.3333\n",
      "Epoch 483/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 2.5526 - val_acc: 0.3333\n",
      "Epoch 484/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 2.5495 - val_acc: 0.3333\n",
      "Epoch 485/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 2.5477 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 2.5446 - val_acc: 0.3333\n",
      "Epoch 487/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 2.5408 - val_acc: 0.3333\n",
      "Epoch 488/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 2.5392 - val_acc: 0.3333\n",
      "Epoch 489/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 2.5364 - val_acc: 0.3333\n",
      "Epoch 490/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 2.5360 - val_acc: 0.3333\n",
      "Epoch 491/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 2.5344 - val_acc: 0.3333\n",
      "Epoch 492/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 2.5322 - val_acc: 0.3333\n",
      "Epoch 493/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 2.5288 - val_acc: 0.3333\n",
      "Epoch 494/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 2.5254 - val_acc: 0.3333\n",
      "Epoch 495/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 2.5229 - val_acc: 0.3333\n",
      "Epoch 496/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 2.5156 - val_acc: 0.3333\n",
      "Epoch 497/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 2.5090 - val_acc: 0.3333\n",
      "Epoch 498/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 2.5065 - val_acc: 0.3333\n",
      "Epoch 499/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 2.5034 - val_acc: 0.3333\n",
      "Epoch 500/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 2.5012 - val_acc: 0.3333\n",
      "Epoch 501/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 2.4977 - val_acc: 0.3333\n",
      "Epoch 502/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 2.4957 - val_acc: 0.3333\n",
      "Epoch 503/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 2.4937 - val_acc: 0.3333\n",
      "Epoch 504/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 2.4926 - val_acc: 0.3333\n",
      "Epoch 505/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 2.4889 - val_acc: 0.3333\n",
      "Epoch 506/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 2.4898 - val_acc: 0.3333\n",
      "Epoch 507/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 2.4909 - val_acc: 0.3333\n",
      "Epoch 508/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 2.4862 - val_acc: 0.3333\n",
      "Epoch 509/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 2.4843 - val_acc: 0.3333\n",
      "Epoch 510/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 2.4821 - val_acc: 0.3333\n",
      "Epoch 511/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 2.4821 - val_acc: 0.3333\n",
      "Epoch 512/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 2.4826 - val_acc: 0.3333\n",
      "Epoch 513/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 2.4823 - val_acc: 0.3333\n",
      "Epoch 514/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 2.4818 - val_acc: 0.3333\n",
      "Epoch 515/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 2.4825 - val_acc: 0.3333\n",
      "Epoch 516/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 2.4837 - val_acc: 0.3333\n",
      "Epoch 517/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 2.4852 - val_acc: 0.3333\n",
      "Epoch 518/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 2.4821 - val_acc: 0.3333\n",
      "Epoch 519/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 2.4815 - val_acc: 0.3333\n",
      "Epoch 520/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 2.4831 - val_acc: 0.3333\n",
      "Epoch 521/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 2.4880 - val_acc: 0.3333\n",
      "Epoch 522/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 2.4902 - val_acc: 0.3333\n",
      "Epoch 523/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 2.4934 - val_acc: 0.3333\n",
      "Epoch 524/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 2.4954 - val_acc: 0.3333\n",
      "Epoch 525/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 2.4972 - val_acc: 0.3333\n",
      "Epoch 526/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 2.5004 - val_acc: 0.3333\n",
      "Epoch 527/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 2.5015 - val_acc: 0.3333\n",
      "Epoch 528/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.5037 - val_acc: 0.3333\n",
      "Epoch 529/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 2.5081 - val_acc: 0.3333\n",
      "Epoch 530/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 2.5105 - val_acc: 0.3333\n",
      "Epoch 531/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 2.5160 - val_acc: 0.3333\n",
      "Epoch 532/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 2.5195 - val_acc: 0.3333\n",
      "Epoch 533/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 2.5214 - val_acc: 0.3333\n",
      "Epoch 534/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 2.5243 - val_acc: 0.3333\n",
      "Epoch 535/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 2.5321 - val_acc: 0.3333\n",
      "Epoch 536/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 2.5398 - val_acc: 0.3333\n",
      "Epoch 537/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 2.5460 - val_acc: 0.3333\n",
      "Epoch 538/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 2.5468 - val_acc: 0.3333\n",
      "Epoch 539/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 2.5496 - val_acc: 0.3333\n",
      "Epoch 540/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 2.5530 - val_acc: 0.3333\n",
      "Epoch 541/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 2.5554 - val_acc: 0.3333\n",
      "Epoch 542/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 2.5565 - val_acc: 0.3333\n",
      "Epoch 543/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 2.5584 - val_acc: 0.3333\n",
      "Epoch 544/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.5608 - val_acc: 0.3333\n",
      "Epoch 545/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 2.5629 - val_acc: 0.3333\n",
      "Epoch 546/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 2.5662 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 547/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 2.5696 - val_acc: 0.3333\n",
      "Epoch 548/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 2.5719 - val_acc: 0.3333\n",
      "Epoch 549/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 2.5796 - val_acc: 0.3333\n",
      "Epoch 550/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 2.5848 - val_acc: 0.3333\n",
      "Epoch 551/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 2.5877 - val_acc: 0.3333\n",
      "Epoch 552/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.5918 - val_acc: 0.3333\n",
      "Epoch 553/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 2.5951 - val_acc: 0.3333\n",
      "Epoch 554/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 2.5970 - val_acc: 0.3333\n",
      "Epoch 555/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.5985 - val_acc: 0.3333\n",
      "Epoch 556/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 2.6063 - val_acc: 0.3333\n",
      "Epoch 557/2000\n",
      "18/18 [==============================] - 0s 167us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 2.6107 - val_acc: 0.3333\n",
      "Epoch 558/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 2.6133 - val_acc: 0.3333\n",
      "Epoch 559/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 2.6179 - val_acc: 0.3333\n",
      "Epoch 560/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 2.6223 - val_acc: 0.3333\n",
      "Epoch 561/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.6259 - val_acc: 0.3333\n",
      "Epoch 562/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.6298 - val_acc: 0.3333\n",
      "Epoch 563/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0256 - acc: 1.0000 - val_loss: 2.6572 - val_acc: 0.3333\n",
      "Epoch 564/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 2.6705 - val_acc: 0.3333\n",
      "Epoch 565/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 2.6831 - val_acc: 0.3333\n",
      "Epoch 566/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 2.6935 - val_acc: 0.3333\n",
      "Epoch 567/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 2.7041 - val_acc: 0.3333\n",
      "Epoch 568/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 2.7131 - val_acc: 0.3333\n",
      "Epoch 569/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 2.7226 - val_acc: 0.3333\n",
      "Epoch 570/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 2.7289 - val_acc: 0.3333\n",
      "Epoch 571/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.7357 - val_acc: 0.3333\n",
      "Epoch 572/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 2.7422 - val_acc: 0.3333\n",
      "Epoch 573/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 2.7489 - val_acc: 0.3333\n",
      "Epoch 574/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.7544 - val_acc: 0.3333\n",
      "Epoch 575/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0455 - acc: 1.0000 - val_loss: 2.8330 - val_acc: 0.3333\n",
      "Epoch 576/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 2.8696 - val_acc: 0.3333\n",
      "Epoch 577/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 2.8857 - val_acc: 0.3333\n",
      "Epoch 578/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 2.9052 - val_acc: 0.3333\n",
      "Epoch 579/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0266 - acc: 1.0000 - val_loss: 2.8969 - val_acc: 0.3333\n",
      "Epoch 580/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 2.9010 - val_acc: 0.3333\n",
      "Epoch 581/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 2.9046 - val_acc: 0.3333\n",
      "Epoch 582/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 2.9071 - val_acc: 0.3333\n",
      "Epoch 583/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 2.9104 - val_acc: 0.3333\n",
      "Epoch 584/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.9130 - val_acc: 0.3333\n",
      "Epoch 585/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 2.9115 - val_acc: 0.3333\n",
      "Epoch 586/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 2.9108 - val_acc: 0.3333\n",
      "Epoch 587/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 2.9111 - val_acc: 0.3333\n",
      "Epoch 588/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.9117 - val_acc: 0.3333\n",
      "Epoch 589/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 2.9125 - val_acc: 0.3333\n",
      "Epoch 590/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.9135 - val_acc: 0.3333\n",
      "Epoch 591/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 2.9149 - val_acc: 0.3333\n",
      "Epoch 592/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 2.9153 - val_acc: 0.3333\n",
      "Epoch 593/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.9166 - val_acc: 0.3333\n",
      "Epoch 594/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 2.9170 - val_acc: 0.3333\n",
      "Epoch 595/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 2.9170 - val_acc: 0.3333\n",
      "Epoch 596/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 2.9128 - val_acc: 0.3333\n",
      "Epoch 597/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 2.8967 - val_acc: 0.3333\n",
      "Epoch 598/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.8897 - val_acc: 0.3333\n",
      "Epoch 599/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.8814 - val_acc: 0.3333\n",
      "Epoch 600/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 2.8760 - val_acc: 0.3333\n",
      "Epoch 601/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0573 - acc: 0.9444 - val_loss: 2.7168 - val_acc: 0.3333\n",
      "Epoch 602/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 2.6447 - val_acc: 0.3333\n",
      "Epoch 603/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 2.5822 - val_acc: 0.3333\n",
      "Epoch 604/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 2.5272 - val_acc: 0.3333\n",
      "Epoch 605/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 2.4798 - val_acc: 0.3333\n",
      "Epoch 606/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.4390 - val_acc: 0.3333\n",
      "Epoch 607/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 2.4062 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 608/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 2.3796 - val_acc: 0.3333\n",
      "Epoch 609/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.3585 - val_acc: 0.3333\n",
      "Epoch 610/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 2.3413 - val_acc: 0.3333\n",
      "Epoch 611/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 2.3287 - val_acc: 0.3333\n",
      "Epoch 612/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 2.3187 - val_acc: 0.3333\n",
      "Epoch 613/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 2.3121 - val_acc: 0.3333\n",
      "Epoch 614/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.3067 - val_acc: 0.3333\n",
      "Epoch 615/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 2.3060 - val_acc: 0.3333\n",
      "Epoch 616/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 2.3037 - val_acc: 0.3333\n",
      "Epoch 617/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 2.3019 - val_acc: 0.3333\n",
      "Epoch 618/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 2.3006 - val_acc: 0.3333\n",
      "Epoch 619/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 2.3039 - val_acc: 0.3333\n",
      "Epoch 620/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 2.3048 - val_acc: 0.3333\n",
      "Epoch 621/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 2.3038 - val_acc: 0.3333\n",
      "Epoch 622/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 2.3050 - val_acc: 0.3333\n",
      "Epoch 623/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 2.3054 - val_acc: 0.3333\n",
      "Epoch 624/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 2.3058 - val_acc: 0.3333\n",
      "Epoch 625/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 2.3084 - val_acc: 0.3333\n",
      "Epoch 626/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 2.3156 - val_acc: 0.3333\n",
      "Epoch 627/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 2.3241 - val_acc: 0.3333\n",
      "Epoch 628/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 2.3402 - val_acc: 0.3333\n",
      "Epoch 629/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 2.3598 - val_acc: 0.3333\n",
      "Epoch 630/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 2.3749 - val_acc: 0.3333\n",
      "Epoch 631/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 2.3905 - val_acc: 0.3333\n",
      "Epoch 632/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 2.4044 - val_acc: 0.3333\n",
      "Epoch 633/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 2.4191 - val_acc: 0.3333\n",
      "Epoch 634/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 2.4340 - val_acc: 0.3333\n",
      "Epoch 635/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 2.4473 - val_acc: 0.3333\n",
      "Epoch 636/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 2.4599 - val_acc: 0.3333\n",
      "Epoch 637/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 2.4715 - val_acc: 0.3333\n",
      "Epoch 638/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.4842 - val_acc: 0.3333\n",
      "Epoch 639/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.4954 - val_acc: 0.3333\n",
      "Epoch 640/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 2.5119 - val_acc: 0.3333\n",
      "Epoch 641/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 2.5254 - val_acc: 0.3333\n",
      "Epoch 642/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 2.5385 - val_acc: 0.3333\n",
      "Epoch 643/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 2.5495 - val_acc: 0.3333\n",
      "Epoch 644/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 2.5365 - val_acc: 0.3333\n",
      "Epoch 645/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 2.5373 - val_acc: 0.3333\n",
      "Epoch 646/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 2.5448 - val_acc: 0.3333\n",
      "Epoch 647/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 2.5491 - val_acc: 0.3333\n",
      "Epoch 648/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 2.5540 - val_acc: 0.3333\n",
      "Epoch 649/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 2.5593 - val_acc: 0.3333\n",
      "Epoch 650/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 2.5715 - val_acc: 0.3333\n",
      "Epoch 651/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 2.5786 - val_acc: 0.3333\n",
      "Epoch 652/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 2.5887 - val_acc: 0.3333\n",
      "Epoch 653/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 2.5965 - val_acc: 0.3333\n",
      "Epoch 654/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.6048 - val_acc: 0.3333\n",
      "Epoch 655/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 2.6138 - val_acc: 0.3333\n",
      "Epoch 656/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 2.6218 - val_acc: 0.3333\n",
      "Epoch 657/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 2.6287 - val_acc: 0.3333\n",
      "Epoch 658/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 2.6354 - val_acc: 0.3333\n",
      "Epoch 659/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 2.6412 - val_acc: 0.3333\n",
      "Epoch 660/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 2.6468 - val_acc: 0.3333\n",
      "Epoch 661/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 2.6497 - val_acc: 0.3333\n",
      "Epoch 662/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 2.6538 - val_acc: 0.3333\n",
      "Epoch 663/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 2.6422 - val_acc: 0.3333\n",
      "Epoch 664/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0243 - acc: 1.0000 - val_loss: 2.6543 - val_acc: 0.3333\n",
      "Epoch 665/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 2.6581 - val_acc: 0.3333\n",
      "Epoch 666/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 2.6638 - val_acc: 0.3333\n",
      "Epoch 667/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 2.6681 - val_acc: 0.3333\n",
      "Epoch 668/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 2.6723 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 669/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 2.6797 - val_acc: 0.3333\n",
      "Epoch 670/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 2.6862 - val_acc: 0.3333\n",
      "Epoch 671/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 2.6896 - val_acc: 0.3333\n",
      "Epoch 672/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 2.6902 - val_acc: 0.3333\n",
      "Epoch 673/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 2.6929 - val_acc: 0.3333\n",
      "Epoch 674/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 2.6959 - val_acc: 0.3333\n",
      "Epoch 675/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 2.6990 - val_acc: 0.3333\n",
      "Epoch 676/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 2.7366 - val_acc: 0.3333\n",
      "Epoch 677/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 2.7820 - val_acc: 0.3333\n",
      "Epoch 678/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 2.8101 - val_acc: 0.3333\n",
      "Epoch 679/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 2.8394 - val_acc: 0.3333\n",
      "Epoch 680/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 2.8635 - val_acc: 0.3333\n",
      "Epoch 681/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 2.8865 - val_acc: 0.3333\n",
      "Epoch 682/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 2.9066 - val_acc: 0.3333\n",
      "Epoch 683/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 2.9244 - val_acc: 0.3333\n",
      "Epoch 684/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 2.9400 - val_acc: 0.3333\n",
      "Epoch 685/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 2.9538 - val_acc: 0.3333\n",
      "Epoch 686/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 2.9662 - val_acc: 0.3333\n",
      "Epoch 687/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 2.9784 - val_acc: 0.3333\n",
      "Epoch 688/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 2.9893 - val_acc: 0.3333\n",
      "Epoch 689/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 2.9990 - val_acc: 0.3333\n",
      "Epoch 690/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0079 - val_acc: 0.3333\n",
      "Epoch 691/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 3.0149 - val_acc: 0.3333\n",
      "Epoch 692/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 3.0212 - val_acc: 0.3333\n",
      "Epoch 693/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 3.0272 - val_acc: 0.3333\n",
      "Epoch 694/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0324 - val_acc: 0.3333\n",
      "Epoch 695/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 3.0629 - val_acc: 0.3333\n",
      "Epoch 696/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 3.0837 - val_acc: 0.3333\n",
      "Epoch 697/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 3.0936 - val_acc: 0.3333\n",
      "Epoch 698/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 3.1054 - val_acc: 0.3333\n",
      "Epoch 699/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 3.1146 - val_acc: 0.3333\n",
      "Epoch 700/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 3.1226 - val_acc: 0.3333\n",
      "Epoch 701/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.1297 - val_acc: 0.3333\n",
      "Epoch 702/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 3.1361 - val_acc: 0.3333\n",
      "Epoch 703/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 3.1419 - val_acc: 0.3333\n",
      "Epoch 704/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 3.1469 - val_acc: 0.3333\n",
      "Epoch 705/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 3.1512 - val_acc: 0.3333\n",
      "Epoch 706/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 3.1550 - val_acc: 0.3333\n",
      "Epoch 707/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 3.1582 - val_acc: 0.3333\n",
      "Epoch 708/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 3.1567 - val_acc: 0.3333\n",
      "Epoch 709/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 3.1576 - val_acc: 0.3333\n",
      "Epoch 710/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 3.1574 - val_acc: 0.3333\n",
      "Epoch 711/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 3.1566 - val_acc: 0.3333\n",
      "Epoch 712/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 3.1537 - val_acc: 0.3333\n",
      "Epoch 713/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 3.1519 - val_acc: 0.3333\n",
      "Epoch 714/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 3.1497 - val_acc: 0.3333\n",
      "Epoch 715/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 3.1461 - val_acc: 0.3333\n",
      "Epoch 716/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 3.1440 - val_acc: 0.3333\n",
      "Epoch 717/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 3.1420 - val_acc: 0.3333\n",
      "Epoch 718/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.1402 - val_acc: 0.3333\n",
      "Epoch 719/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 3.1391 - val_acc: 0.3333\n",
      "Epoch 720/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 3.1372 - val_acc: 0.3333\n",
      "Epoch 721/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 3.1339 - val_acc: 0.3333\n",
      "Epoch 722/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.1315 - val_acc: 0.3333\n",
      "Epoch 723/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.1290 - val_acc: 0.3333\n",
      "Epoch 724/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 3.1265 - val_acc: 0.3333\n",
      "Epoch 725/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.1232 - val_acc: 0.3333\n",
      "Epoch 726/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 3.1192 - val_acc: 0.3333\n",
      "Epoch 727/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 3.1159 - val_acc: 0.3333\n",
      "Epoch 728/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.1127 - val_acc: 0.3333\n",
      "Epoch 729/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.1107 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 730/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 3.1069 - val_acc: 0.3333\n",
      "Epoch 731/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.1037 - val_acc: 0.3333\n",
      "Epoch 732/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 3.1055 - val_acc: 0.3333\n",
      "Epoch 733/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 3.0998 - val_acc: 0.3333\n",
      "Epoch 734/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 3.0990 - val_acc: 0.3333\n",
      "Epoch 735/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0982 - val_acc: 0.3333\n",
      "Epoch 736/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 3.0964 - val_acc: 0.3333\n",
      "Epoch 737/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0951 - val_acc: 0.3333\n",
      "Epoch 738/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.0935 - val_acc: 0.3333\n",
      "Epoch 739/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0922 - val_acc: 0.3333\n",
      "Epoch 740/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 3.0914 - val_acc: 0.3333\n",
      "Epoch 741/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0903 - val_acc: 0.3333\n",
      "Epoch 742/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 3.0894 - val_acc: 0.3333\n",
      "Epoch 743/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0887 - val_acc: 0.3333\n",
      "Epoch 744/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0857 - val_acc: 0.3333\n",
      "Epoch 745/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 3.0841 - val_acc: 0.3333\n",
      "Epoch 746/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 3.0813 - val_acc: 0.3333\n",
      "Epoch 747/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0789 - val_acc: 0.3333\n",
      "Epoch 748/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.0762 - val_acc: 0.3333\n",
      "Epoch 749/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 3.0741 - val_acc: 0.3333\n",
      "Epoch 750/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 3.0718 - val_acc: 0.3333\n",
      "Epoch 751/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0699 - val_acc: 0.3333\n",
      "Epoch 752/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 3.0687 - val_acc: 0.3333\n",
      "Epoch 753/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.0678 - val_acc: 0.3333\n",
      "Epoch 754/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.0667 - val_acc: 0.3333\n",
      "Epoch 755/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0663 - val_acc: 0.3333\n",
      "Epoch 756/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 3.0661 - val_acc: 0.3333\n",
      "Epoch 757/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0643 - val_acc: 0.3333\n",
      "Epoch 758/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.0625 - val_acc: 0.3333\n",
      "Epoch 759/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.0589 - val_acc: 0.3333\n",
      "Epoch 760/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 3.0534 - val_acc: 0.3333\n",
      "Epoch 761/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 3.0502 - val_acc: 0.3333\n",
      "Epoch 762/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0474 - val_acc: 0.3333\n",
      "Epoch 763/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 3.0463 - val_acc: 0.3333\n",
      "Epoch 764/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0461 - val_acc: 0.3333\n",
      "Epoch 765/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.0462 - val_acc: 0.3333\n",
      "Epoch 766/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0436 - val_acc: 0.3333\n",
      "Epoch 767/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 3.0422 - val_acc: 0.3333\n",
      "Epoch 768/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 3.0495 - val_acc: 0.3333\n",
      "Epoch 769/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 3.0572 - val_acc: 0.3333\n",
      "Epoch 770/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0611 - val_acc: 0.3333\n",
      "Epoch 771/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 3.0645 - val_acc: 0.3333\n",
      "Epoch 772/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 3.0728 - val_acc: 0.3333\n",
      "Epoch 773/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 3.0768 - val_acc: 0.3333\n",
      "Epoch 774/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 3.0787 - val_acc: 0.3333\n",
      "Epoch 775/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 3.0787 - val_acc: 0.3333\n",
      "Epoch 776/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 3.0799 - val_acc: 0.3333\n",
      "Epoch 777/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 3.0802 - val_acc: 0.3333\n",
      "Epoch 778/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0796 - val_acc: 0.3333\n",
      "Epoch 779/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0795 - val_acc: 0.3333\n",
      "Epoch 780/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 3.0791 - val_acc: 0.3333\n",
      "Epoch 781/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 3.0748 - val_acc: 0.3333\n",
      "Epoch 782/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0727 - val_acc: 0.3333\n",
      "Epoch 783/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 3.0690 - val_acc: 0.3333\n",
      "Epoch 784/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0661 - val_acc: 0.3333\n",
      "Epoch 785/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 3.0633 - val_acc: 0.3333\n",
      "Epoch 786/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0611 - val_acc: 0.3333\n",
      "Epoch 787/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 3.0583 - val_acc: 0.3333\n",
      "Epoch 788/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0567 - val_acc: 0.3333\n",
      "Epoch 789/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0557 - val_acc: 0.3333\n",
      "Epoch 790/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0550 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.0543 - val_acc: 0.3333\n",
      "Epoch 792/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0532 - val_acc: 0.3333\n",
      "Epoch 793/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0523 - val_acc: 0.3333\n",
      "Epoch 794/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0500 - val_acc: 0.3333\n",
      "Epoch 795/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 3.0451 - val_acc: 0.3333\n",
      "Epoch 796/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0423 - val_acc: 0.3333\n",
      "Epoch 797/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 3.0358 - val_acc: 0.3333\n",
      "Epoch 798/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 3.0332 - val_acc: 0.3333\n",
      "Epoch 799/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0301 - val_acc: 0.3333\n",
      "Epoch 800/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.0275 - val_acc: 0.3333\n",
      "Epoch 801/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0251 - val_acc: 0.3333\n",
      "Epoch 802/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 3.0237 - val_acc: 0.3333\n",
      "Epoch 803/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0219 - val_acc: 0.3333\n",
      "Epoch 804/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0201 - val_acc: 0.3333\n",
      "Epoch 805/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0188 - val_acc: 0.3333\n",
      "Epoch 806/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.0177 - val_acc: 0.3333\n",
      "Epoch 807/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 3.0189 - val_acc: 0.3333\n",
      "Epoch 808/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.0188 - val_acc: 0.3333\n",
      "Epoch 809/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.0236 - val_acc: 0.3333\n",
      "Epoch 810/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0274 - val_acc: 0.3333\n",
      "Epoch 811/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.0302 - val_acc: 0.3333\n",
      "Epoch 812/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0317 - val_acc: 0.3333\n",
      "Epoch 813/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0339 - val_acc: 0.3333\n",
      "Epoch 814/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.0365 - val_acc: 0.3333\n",
      "Epoch 815/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0380 - val_acc: 0.3333\n",
      "Epoch 816/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 3.0414 - val_acc: 0.3333\n",
      "Epoch 817/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0447 - val_acc: 0.3333\n",
      "Epoch 818/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0467 - val_acc: 0.3333\n",
      "Epoch 819/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.0482 - val_acc: 0.3333\n",
      "Epoch 820/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 3.0499 - val_acc: 0.3333\n",
      "Epoch 821/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0510 - val_acc: 0.3333\n",
      "Epoch 822/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.0523 - val_acc: 0.3333\n",
      "Epoch 823/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0538 - val_acc: 0.3333\n",
      "Epoch 824/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.0518 - val_acc: 0.3333\n",
      "Epoch 825/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0518 - val_acc: 0.3333\n",
      "Epoch 826/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0528 - val_acc: 0.3333\n",
      "Epoch 827/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.0542 - val_acc: 0.3333\n",
      "Epoch 828/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.0565 - val_acc: 0.3333\n",
      "Epoch 829/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.0583 - val_acc: 0.3333\n",
      "Epoch 830/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.0602 - val_acc: 0.3333\n",
      "Epoch 831/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0615 - val_acc: 0.3333\n",
      "Epoch 832/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.0633 - val_acc: 0.3333\n",
      "Epoch 833/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.0649 - val_acc: 0.3333\n",
      "Epoch 834/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0643 - val_acc: 0.3333\n",
      "Epoch 835/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.0652 - val_acc: 0.3333\n",
      "Epoch 836/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0665 - val_acc: 0.3333\n",
      "Epoch 837/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 3.0682 - val_acc: 0.3333\n",
      "Epoch 838/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.0710 - val_acc: 0.3333\n",
      "Epoch 839/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 3.0743 - val_acc: 0.3333\n",
      "Epoch 840/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0771 - val_acc: 0.3333\n",
      "Epoch 841/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0795 - val_acc: 0.3333\n",
      "Epoch 842/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0813 - val_acc: 0.3333\n",
      "Epoch 843/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.0829 - val_acc: 0.3333\n",
      "Epoch 844/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 3.0857 - val_acc: 0.3333\n",
      "Epoch 845/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.0883 - val_acc: 0.3333\n",
      "Epoch 846/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0902 - val_acc: 0.3333\n",
      "Epoch 847/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0922 - val_acc: 0.3333\n",
      "Epoch 848/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.0928 - val_acc: 0.3333\n",
      "Epoch 849/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.0935 - val_acc: 0.3333\n",
      "Epoch 850/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 3.0866 - val_acc: 0.3333\n",
      "Epoch 851/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 3.0874 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 852/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.0850 - val_acc: 0.3333\n",
      "Epoch 853/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.0838 - val_acc: 0.3333\n",
      "Epoch 854/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.0825 - val_acc: 0.3333\n",
      "Epoch 855/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.0811 - val_acc: 0.3333\n",
      "Epoch 856/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.0806 - val_acc: 0.3333\n",
      "Epoch 857/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.0803 - val_acc: 0.3333\n",
      "Epoch 858/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.0806 - val_acc: 0.3333\n",
      "Epoch 859/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 3.1176 - val_acc: 0.3333\n",
      "Epoch 860/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1331 - val_acc: 0.3333\n",
      "Epoch 861/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 3.1472 - val_acc: 0.3333\n",
      "Epoch 862/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1602 - val_acc: 0.3333\n",
      "Epoch 863/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.1724 - val_acc: 0.3333\n",
      "Epoch 864/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.1830 - val_acc: 0.3333\n",
      "Epoch 865/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.1951 - val_acc: 0.3333\n",
      "Epoch 866/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.2020 - val_acc: 0.3333\n",
      "Epoch 867/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.2096 - val_acc: 0.3333\n",
      "Epoch 868/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.2154 - val_acc: 0.3333\n",
      "Epoch 869/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2209 - val_acc: 0.3333\n",
      "Epoch 870/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2258 - val_acc: 0.3333\n",
      "Epoch 871/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.2304 - val_acc: 0.3333\n",
      "Epoch 872/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.2349 - val_acc: 0.3333\n",
      "Epoch 873/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2387 - val_acc: 0.3333\n",
      "Epoch 874/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.2423 - val_acc: 0.3333\n",
      "Epoch 875/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2449 - val_acc: 0.3333\n",
      "Epoch 876/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.2475 - val_acc: 0.3333\n",
      "Epoch 877/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 3.2500 - val_acc: 0.3333\n",
      "Epoch 878/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.2524 - val_acc: 0.3333\n",
      "Epoch 879/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 3.2593 - val_acc: 0.3333\n",
      "Epoch 880/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 3.2631 - val_acc: 0.3333\n",
      "Epoch 881/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.2659 - val_acc: 0.3333\n",
      "Epoch 882/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.2690 - val_acc: 0.3333\n",
      "Epoch 883/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.2730 - val_acc: 0.3333\n",
      "Epoch 884/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 3.2703 - val_acc: 0.3333\n",
      "Epoch 885/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 3.2567 - val_acc: 0.3333\n",
      "Epoch 886/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2512 - val_acc: 0.3333\n",
      "Epoch 887/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.2464 - val_acc: 0.3333\n",
      "Epoch 888/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.2417 - val_acc: 0.3333\n",
      "Epoch 889/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2365 - val_acc: 0.3333\n",
      "Epoch 890/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2326 - val_acc: 0.3333\n",
      "Epoch 891/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.2293 - val_acc: 0.3333\n",
      "Epoch 892/2000\n",
      "18/18 [==============================] - 0s 611us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.2269 - val_acc: 0.3333\n",
      "Epoch 893/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.2243 - val_acc: 0.3333\n",
      "Epoch 894/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2220 - val_acc: 0.3333\n",
      "Epoch 895/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.2200 - val_acc: 0.3333\n",
      "Epoch 896/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.2182 - val_acc: 0.3333\n",
      "Epoch 897/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.2164 - val_acc: 0.3333\n",
      "Epoch 898/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 3.2150 - val_acc: 0.3333\n",
      "Epoch 899/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.2136 - val_acc: 0.3333\n",
      "Epoch 900/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.2118 - val_acc: 0.3333\n",
      "Epoch 901/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.2103 - val_acc: 0.3333\n",
      "Epoch 902/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.2077 - val_acc: 0.3333\n",
      "Epoch 903/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.2053 - val_acc: 0.3333\n",
      "Epoch 904/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.2056 - val_acc: 0.3333\n",
      "Epoch 905/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.2046 - val_acc: 0.3333\n",
      "Epoch 906/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 3.2346 - val_acc: 0.3333\n",
      "Epoch 907/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 3.2475 - val_acc: 0.3333\n",
      "Epoch 908/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 3.2587 - val_acc: 0.3333\n",
      "Epoch 909/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.2689 - val_acc: 0.3333\n",
      "Epoch 910/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2780 - val_acc: 0.3333\n",
      "Epoch 911/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 3.2860 - val_acc: 0.3333\n",
      "Epoch 912/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 3.2934 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.3000 - val_acc: 0.3333\n",
      "Epoch 914/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 3.2855 - val_acc: 0.3333\n",
      "Epoch 915/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2814 - val_acc: 0.3333\n",
      "Epoch 916/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 3.2792 - val_acc: 0.3333\n",
      "Epoch 917/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2750 - val_acc: 0.3333\n",
      "Epoch 918/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.2719 - val_acc: 0.3333\n",
      "Epoch 919/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.2714 - val_acc: 0.3333\n",
      "Epoch 920/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.2700 - val_acc: 0.3333\n",
      "Epoch 921/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.2685 - val_acc: 0.3333\n",
      "Epoch 922/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.2671 - val_acc: 0.3333\n",
      "Epoch 923/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.2656 - val_acc: 0.3333\n",
      "Epoch 924/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 3.2586 - val_acc: 0.3333\n",
      "Epoch 925/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.2550 - val_acc: 0.3333\n",
      "Epoch 926/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.2527 - val_acc: 0.3333\n",
      "Epoch 927/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.2501 - val_acc: 0.3333\n",
      "Epoch 928/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.2480 - val_acc: 0.3333\n",
      "Epoch 929/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 3.2351 - val_acc: 0.3333\n",
      "Epoch 930/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.2289 - val_acc: 0.3333\n",
      "Epoch 931/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.2233 - val_acc: 0.3333\n",
      "Epoch 932/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.2182 - val_acc: 0.3333\n",
      "Epoch 933/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.2136 - val_acc: 0.3333\n",
      "Epoch 934/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.2092 - val_acc: 0.3333\n",
      "Epoch 935/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.2067 - val_acc: 0.3333\n",
      "Epoch 936/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.2039 - val_acc: 0.3333\n",
      "Epoch 937/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.1995 - val_acc: 0.3333\n",
      "Epoch 938/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.1965 - val_acc: 0.3333\n",
      "Epoch 939/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1942 - val_acc: 0.3333\n",
      "Epoch 940/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.1920 - val_acc: 0.3333\n",
      "Epoch 941/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1902 - val_acc: 0.3333\n",
      "Epoch 942/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.1881 - val_acc: 0.3333\n",
      "Epoch 943/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1864 - val_acc: 0.3333\n",
      "Epoch 944/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.1855 - val_acc: 0.3333\n",
      "Epoch 945/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.1854 - val_acc: 0.3333\n",
      "Epoch 946/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1847 - val_acc: 0.3333\n",
      "Epoch 947/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1843 - val_acc: 0.3333\n",
      "Epoch 948/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1851 - val_acc: 0.3333\n",
      "Epoch 949/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.1853 - val_acc: 0.3333\n",
      "Epoch 950/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.1852 - val_acc: 0.3333\n",
      "Epoch 951/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.1860 - val_acc: 0.3333\n",
      "Epoch 952/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.1863 - val_acc: 0.3333\n",
      "Epoch 953/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1858 - val_acc: 0.3333\n",
      "Epoch 954/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.1860 - val_acc: 0.3333\n",
      "Epoch 955/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1858 - val_acc: 0.3333\n",
      "Epoch 956/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1851 - val_acc: 0.3333\n",
      "Epoch 957/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.1849 - val_acc: 0.3333\n",
      "Epoch 958/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.1848 - val_acc: 0.3333\n",
      "Epoch 959/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.1845 - val_acc: 0.3333\n",
      "Epoch 960/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.1848 - val_acc: 0.3333\n",
      "Epoch 961/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.1807 - val_acc: 0.3333\n",
      "Epoch 962/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1790 - val_acc: 0.3333\n",
      "Epoch 963/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1783 - val_acc: 0.3333\n",
      "Epoch 964/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.1775 - val_acc: 0.3333\n",
      "Epoch 965/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1767 - val_acc: 0.3333\n",
      "Epoch 966/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.1761 - val_acc: 0.3333\n",
      "Epoch 967/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1762 - val_acc: 0.3333\n",
      "Epoch 968/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.1767 - val_acc: 0.3333\n",
      "Epoch 969/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1765 - val_acc: 0.3333\n",
      "Epoch 970/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1765 - val_acc: 0.3333\n",
      "Epoch 971/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1772 - val_acc: 0.3333\n",
      "Epoch 972/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1771 - val_acc: 0.3333\n",
      "Epoch 973/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1779 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 974/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1785 - val_acc: 0.3333\n",
      "Epoch 975/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1788 - val_acc: 0.3333\n",
      "Epoch 976/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1792 - val_acc: 0.3333\n",
      "Epoch 977/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1795 - val_acc: 0.3333\n",
      "Epoch 978/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1792 - val_acc: 0.3333\n",
      "Epoch 979/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1799 - val_acc: 0.3333\n",
      "Epoch 980/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.1768 - val_acc: 0.3333\n",
      "Epoch 981/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1752 - val_acc: 0.3333\n",
      "Epoch 982/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1746 - val_acc: 0.3333\n",
      "Epoch 983/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.1735 - val_acc: 0.3333\n",
      "Epoch 984/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1724 - val_acc: 0.3333\n",
      "Epoch 985/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 3.1707 - val_acc: 0.3333\n",
      "Epoch 986/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1712 - val_acc: 0.3333\n",
      "Epoch 987/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1709 - val_acc: 0.3333\n",
      "Epoch 988/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1707 - val_acc: 0.3333\n",
      "Epoch 989/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1704 - val_acc: 0.3333\n",
      "Epoch 990/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1700 - val_acc: 0.3333\n",
      "Epoch 991/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1698 - val_acc: 0.3333\n",
      "Epoch 992/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1681 - val_acc: 0.3333\n",
      "Epoch 993/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1671 - val_acc: 0.3333\n",
      "Epoch 994/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1670 - val_acc: 0.3333\n",
      "Epoch 995/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1663 - val_acc: 0.3333\n",
      "Epoch 996/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1652 - val_acc: 0.3333\n",
      "Epoch 997/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.1636 - val_acc: 0.3333\n",
      "Epoch 998/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1630 - val_acc: 0.3333\n",
      "Epoch 999/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1627 - val_acc: 0.3333\n",
      "Epoch 1000/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1621 - val_acc: 0.3333\n",
      "Epoch 1001/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 3.1602 - val_acc: 0.3333\n",
      "Epoch 1002/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 3.1596 - val_acc: 0.3333\n",
      "Epoch 1003/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 3.1633 - val_acc: 0.3333\n",
      "Epoch 1004/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1648 - val_acc: 0.3333\n",
      "Epoch 1005/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1663 - val_acc: 0.3333\n",
      "Epoch 1006/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1656 - val_acc: 0.3333\n",
      "Epoch 1007/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1666 - val_acc: 0.3333\n",
      "Epoch 1008/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1672 - val_acc: 0.3333\n",
      "Epoch 1009/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1681 - val_acc: 0.3333\n",
      "Epoch 1010/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1695 - val_acc: 0.3333\n",
      "Epoch 1011/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1704 - val_acc: 0.3333\n",
      "Epoch 1012/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1713 - val_acc: 0.3333\n",
      "Epoch 1013/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1722 - val_acc: 0.3333\n",
      "Epoch 1014/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1729 - val_acc: 0.3333\n",
      "Epoch 1015/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 3.1570 - val_acc: 0.3333\n",
      "Epoch 1016/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1510 - val_acc: 0.3333\n",
      "Epoch 1017/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1451 - val_acc: 0.3333\n",
      "Epoch 1018/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1397 - val_acc: 0.3333\n",
      "Epoch 1019/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1347 - val_acc: 0.3333\n",
      "Epoch 1020/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1303 - val_acc: 0.3333\n",
      "Epoch 1021/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1267 - val_acc: 0.3333\n",
      "Epoch 1022/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1235 - val_acc: 0.3333\n",
      "Epoch 1023/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1207 - val_acc: 0.3333\n",
      "Epoch 1024/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1184 - val_acc: 0.3333\n",
      "Epoch 1025/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1165 - val_acc: 0.3333\n",
      "Epoch 1026/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1149 - val_acc: 0.3333\n",
      "Epoch 1027/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1134 - val_acc: 0.3333\n",
      "Epoch 1028/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1122 - val_acc: 0.3333\n",
      "Epoch 1029/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1111 - val_acc: 0.3333\n",
      "Epoch 1030/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1103 - val_acc: 0.3333\n",
      "Epoch 1031/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1098 - val_acc: 0.3333\n",
      "Epoch 1032/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1092 - val_acc: 0.3333\n",
      "Epoch 1033/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1091 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1034/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1078 - val_acc: 0.3333\n",
      "Epoch 1035/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1079 - val_acc: 0.3333\n",
      "Epoch 1036/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1078 - val_acc: 0.3333\n",
      "Epoch 1037/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1079 - val_acc: 0.3333\n",
      "Epoch 1038/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.1080 - val_acc: 0.3333\n",
      "Epoch 1039/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1084 - val_acc: 0.3333\n",
      "Epoch 1040/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 3.1091 - val_acc: 0.3333\n",
      "Epoch 1041/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1095 - val_acc: 0.3333\n",
      "Epoch 1042/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1109 - val_acc: 0.3333\n",
      "Epoch 1043/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1116 - val_acc: 0.3333\n",
      "Epoch 1044/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1123 - val_acc: 0.3333\n",
      "Epoch 1045/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 3.1128 - val_acc: 0.3333\n",
      "Epoch 1046/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1144 - val_acc: 0.3333\n",
      "Epoch 1047/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1164 - val_acc: 0.3333\n",
      "Epoch 1048/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1179 - val_acc: 0.3333\n",
      "Epoch 1049/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.1236 - val_acc: 0.3333\n",
      "Epoch 1050/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1268 - val_acc: 0.3333\n",
      "Epoch 1051/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1303 - val_acc: 0.3333\n",
      "Epoch 1052/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1332 - val_acc: 0.3333\n",
      "Epoch 1053/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1361 - val_acc: 0.3333\n",
      "Epoch 1054/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1382 - val_acc: 0.3333\n",
      "Epoch 1055/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1405 - val_acc: 0.3333\n",
      "Epoch 1056/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1431 - val_acc: 0.3333\n",
      "Epoch 1057/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1459 - val_acc: 0.3333\n",
      "Epoch 1058/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.1481 - val_acc: 0.3333\n",
      "Epoch 1059/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1496 - val_acc: 0.3333\n",
      "Epoch 1060/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1515 - val_acc: 0.3333\n",
      "Epoch 1061/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1532 - val_acc: 0.3333\n",
      "Epoch 1062/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1556 - val_acc: 0.3333\n",
      "Epoch 1063/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1574 - val_acc: 0.3333\n",
      "Epoch 1064/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1606 - val_acc: 0.3333\n",
      "Epoch 1065/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1631 - val_acc: 0.3333\n",
      "Epoch 1066/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1656 - val_acc: 0.3333\n",
      "Epoch 1067/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.1678 - val_acc: 0.3333\n",
      "Epoch 1068/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1700 - val_acc: 0.3333\n",
      "Epoch 1069/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.1713 - val_acc: 0.3333\n",
      "Epoch 1070/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.1729 - val_acc: 0.3333\n",
      "Epoch 1071/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1743 - val_acc: 0.3333\n",
      "Epoch 1072/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.1749 - val_acc: 0.3333\n",
      "Epoch 1073/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.1763 - val_acc: 0.3333\n",
      "Epoch 1074/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.1774 - val_acc: 0.3333\n",
      "Epoch 1075/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1794 - val_acc: 0.3333\n",
      "Epoch 1076/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1802 - val_acc: 0.3333\n",
      "Epoch 1077/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1808 - val_acc: 0.3333\n",
      "Epoch 1078/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.1816 - val_acc: 0.3333\n",
      "Epoch 1079/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1819 - val_acc: 0.3333\n",
      "Epoch 1080/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.1828 - val_acc: 0.3333\n",
      "Epoch 1081/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.1846 - val_acc: 0.3333\n",
      "Epoch 1082/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.1842 - val_acc: 0.3333\n",
      "Epoch 1083/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.1847 - val_acc: 0.3333\n",
      "Epoch 1084/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.1837 - val_acc: 0.3333\n",
      "Epoch 1085/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.1835 - val_acc: 0.3333\n",
      "Epoch 1086/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.1837 - val_acc: 0.3333\n",
      "Epoch 1087/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.1884 - val_acc: 0.3333\n",
      "Epoch 1088/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.1908 - val_acc: 0.3333\n",
      "Epoch 1089/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.1930 - val_acc: 0.3333\n",
      "Epoch 1090/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.1957 - val_acc: 0.3333\n",
      "Epoch 1091/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.1978 - val_acc: 0.3333\n",
      "Epoch 1092/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.1997 - val_acc: 0.3333\n",
      "Epoch 1093/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.2003 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1094/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.2026 - val_acc: 0.3333\n",
      "Epoch 1095/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.2048 - val_acc: 0.3333\n",
      "Epoch 1096/2000\n",
      "18/18 [==============================] - 0s 167us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.2069 - val_acc: 0.3333\n",
      "Epoch 1097/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2086 - val_acc: 0.3333\n",
      "Epoch 1098/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.2103 - val_acc: 0.3333\n",
      "Epoch 1099/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.2118 - val_acc: 0.3333\n",
      "Epoch 1100/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.2131 - val_acc: 0.3333\n",
      "Epoch 1101/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.2140 - val_acc: 0.3333\n",
      "Epoch 1102/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.2148 - val_acc: 0.3333\n",
      "Epoch 1103/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2158 - val_acc: 0.3333\n",
      "Epoch 1104/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.2156 - val_acc: 0.3333\n",
      "Epoch 1105/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.2160 - val_acc: 0.3333\n",
      "Epoch 1106/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2167 - val_acc: 0.3333\n",
      "Epoch 1107/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.2180 - val_acc: 0.3333\n",
      "Epoch 1108/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2186 - val_acc: 0.3333\n",
      "Epoch 1109/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.2193 - val_acc: 0.3333\n",
      "Epoch 1110/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.2208 - val_acc: 0.3333\n",
      "Epoch 1111/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.2232 - val_acc: 0.3333\n",
      "Epoch 1112/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2251 - val_acc: 0.3333\n",
      "Epoch 1113/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.2265 - val_acc: 0.3333\n",
      "Epoch 1114/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.2282 - val_acc: 0.3333\n",
      "Epoch 1115/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.2290 - val_acc: 0.3333\n",
      "Epoch 1116/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.2303 - val_acc: 0.3333\n",
      "Epoch 1117/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.2315 - val_acc: 0.3333\n",
      "Epoch 1118/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.2326 - val_acc: 0.3333\n",
      "Epoch 1119/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.2344 - val_acc: 0.3333\n",
      "Epoch 1120/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2363 - val_acc: 0.3333\n",
      "Epoch 1121/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2378 - val_acc: 0.3333\n",
      "Epoch 1122/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2389 - val_acc: 0.3333\n",
      "Epoch 1123/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.2405 - val_acc: 0.3333\n",
      "Epoch 1124/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2424 - val_acc: 0.3333\n",
      "Epoch 1125/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.2441 - val_acc: 0.3333\n",
      "Epoch 1126/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.2457 - val_acc: 0.3333\n",
      "Epoch 1127/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.2488 - val_acc: 0.3333\n",
      "Epoch 1128/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2506 - val_acc: 0.3333\n",
      "Epoch 1129/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.2522 - val_acc: 0.3333\n",
      "Epoch 1130/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.2537 - val_acc: 0.3333\n",
      "Epoch 1131/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.2546 - val_acc: 0.3333\n",
      "Epoch 1132/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2558 - val_acc: 0.3333\n",
      "Epoch 1133/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.2582 - val_acc: 0.3333\n",
      "Epoch 1134/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.2619 - val_acc: 0.3333\n",
      "Epoch 1135/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.2642 - val_acc: 0.3333\n",
      "Epoch 1136/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.2664 - val_acc: 0.3333\n",
      "Epoch 1137/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.2698 - val_acc: 0.3333\n",
      "Epoch 1138/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.2724 - val_acc: 0.3333\n",
      "Epoch 1139/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 3.3069 - val_acc: 0.3333\n",
      "Epoch 1140/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.3199 - val_acc: 0.3333\n",
      "Epoch 1141/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.3324 - val_acc: 0.3333\n",
      "Epoch 1142/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.3452 - val_acc: 0.3333\n",
      "Epoch 1143/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.3559 - val_acc: 0.3333\n",
      "Epoch 1144/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.3648 - val_acc: 0.3333\n",
      "Epoch 1145/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.3721 - val_acc: 0.3333\n",
      "Epoch 1146/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.3793 - val_acc: 0.3333\n",
      "Epoch 1147/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.3858 - val_acc: 0.3333\n",
      "Epoch 1148/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3916 - val_acc: 0.3333\n",
      "Epoch 1149/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.3969 - val_acc: 0.3333\n",
      "Epoch 1150/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.4016 - val_acc: 0.3333\n",
      "Epoch 1151/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.4059 - val_acc: 0.3333\n",
      "Epoch 1152/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.4099 - val_acc: 0.3333\n",
      "Epoch 1153/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.4132 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1154/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.4164 - val_acc: 0.3333\n",
      "Epoch 1155/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.4191 - val_acc: 0.3333\n",
      "Epoch 1156/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.4215 - val_acc: 0.3333\n",
      "Epoch 1157/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.4219 - val_acc: 0.3333\n",
      "Epoch 1158/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.4210 - val_acc: 0.3333\n",
      "Epoch 1159/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 3.4120 - val_acc: 0.3333\n",
      "Epoch 1160/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.4082 - val_acc: 0.3333\n",
      "Epoch 1161/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.3990 - val_acc: 0.3333\n",
      "Epoch 1162/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.3935 - val_acc: 0.3333\n",
      "Epoch 1163/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.3883 - val_acc: 0.3333\n",
      "Epoch 1164/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.3835 - val_acc: 0.3333\n",
      "Epoch 1165/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3796 - val_acc: 0.3333\n",
      "Epoch 1166/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.3760 - val_acc: 0.3333\n",
      "Epoch 1167/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 3.3691 - val_acc: 0.3333\n",
      "Epoch 1168/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.3645 - val_acc: 0.3333\n",
      "Epoch 1169/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.3602 - val_acc: 0.3333\n",
      "Epoch 1170/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.3566 - val_acc: 0.3333\n",
      "Epoch 1171/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.3529 - val_acc: 0.3333\n",
      "Epoch 1172/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3500 - val_acc: 0.3333\n",
      "Epoch 1173/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 3.3496 - val_acc: 0.3333\n",
      "Epoch 1174/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.3480 - val_acc: 0.3333\n",
      "Epoch 1175/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.3464 - val_acc: 0.3333\n",
      "Epoch 1176/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3451 - val_acc: 0.3333\n",
      "Epoch 1177/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.3437 - val_acc: 0.3333\n",
      "Epoch 1178/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 3.3930 - val_acc: 0.3333\n",
      "Epoch 1179/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.4126 - val_acc: 0.3333\n",
      "Epoch 1180/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.4302 - val_acc: 0.3333\n",
      "Epoch 1181/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.4461 - val_acc: 0.3333\n",
      "Epoch 1182/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.4603 - val_acc: 0.3333\n",
      "Epoch 1183/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.4731 - val_acc: 0.3333\n",
      "Epoch 1184/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.4852 - val_acc: 0.3333\n",
      "Epoch 1185/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.4945 - val_acc: 0.3333\n",
      "Epoch 1186/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5034 - val_acc: 0.3333\n",
      "Epoch 1187/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5116 - val_acc: 0.3333\n",
      "Epoch 1188/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5187 - val_acc: 0.3333\n",
      "Epoch 1189/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.5249 - val_acc: 0.3333\n",
      "Epoch 1190/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.5310 - val_acc: 0.3333\n",
      "Epoch 1191/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5362 - val_acc: 0.3333\n",
      "Epoch 1192/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5404 - val_acc: 0.3333\n",
      "Epoch 1193/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5436 - val_acc: 0.3333\n",
      "Epoch 1194/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5468 - val_acc: 0.3333\n",
      "Epoch 1195/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.5502 - val_acc: 0.3333\n",
      "Epoch 1196/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5528 - val_acc: 0.3333\n",
      "Epoch 1197/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.5548 - val_acc: 0.3333\n",
      "Epoch 1198/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5569 - val_acc: 0.3333\n",
      "Epoch 1199/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5586 - val_acc: 0.3333\n",
      "Epoch 1200/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5599 - val_acc: 0.3333\n",
      "Epoch 1201/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5605 - val_acc: 0.3333\n",
      "Epoch 1202/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5616 - val_acc: 0.3333\n",
      "Epoch 1203/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5625 - val_acc: 0.3333\n",
      "Epoch 1204/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5631 - val_acc: 0.3333\n",
      "Epoch 1205/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.5643 - val_acc: 0.3333\n",
      "Epoch 1206/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.5648 - val_acc: 0.3333\n",
      "Epoch 1207/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5655 - val_acc: 0.3333\n",
      "Epoch 1208/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5666 - val_acc: 0.3333\n",
      "Epoch 1209/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.5657 - val_acc: 0.3333\n",
      "Epoch 1210/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5659 - val_acc: 0.3333\n",
      "Epoch 1211/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5652 - val_acc: 0.3333\n",
      "Epoch 1212/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5632 - val_acc: 0.3333\n",
      "Epoch 1213/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5621 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1214/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5609 - val_acc: 0.3333\n",
      "Epoch 1215/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5598 - val_acc: 0.3333\n",
      "Epoch 1216/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5588 - val_acc: 0.3333\n",
      "Epoch 1217/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.5561 - val_acc: 0.3333\n",
      "Epoch 1218/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5544 - val_acc: 0.3333\n",
      "Epoch 1219/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5529 - val_acc: 0.3333\n",
      "Epoch 1220/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5510 - val_acc: 0.3333\n",
      "Epoch 1221/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5486 - val_acc: 0.3333\n",
      "Epoch 1222/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.5525 - val_acc: 0.3333\n",
      "Epoch 1223/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5534 - val_acc: 0.3333\n",
      "Epoch 1224/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5541 - val_acc: 0.3333\n",
      "Epoch 1225/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5547 - val_acc: 0.3333\n",
      "Epoch 1226/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5554 - val_acc: 0.3333\n",
      "Epoch 1227/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.5537 - val_acc: 0.3333\n",
      "Epoch 1228/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5536 - val_acc: 0.3333\n",
      "Epoch 1229/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5534 - val_acc: 0.3333\n",
      "Epoch 1230/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5530 - val_acc: 0.3333\n",
      "Epoch 1231/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5529 - val_acc: 0.3333\n",
      "Epoch 1232/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5530 - val_acc: 0.3333\n",
      "Epoch 1233/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5526 - val_acc: 0.3333\n",
      "Epoch 1234/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.5513 - val_acc: 0.3333\n",
      "Epoch 1235/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5512 - val_acc: 0.3333\n",
      "Epoch 1236/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5509 - val_acc: 0.3333\n",
      "Epoch 1237/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5507 - val_acc: 0.3333\n",
      "Epoch 1238/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5518 - val_acc: 0.3333\n",
      "Epoch 1239/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5523 - val_acc: 0.3333\n",
      "Epoch 1240/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5526 - val_acc: 0.3333\n",
      "Epoch 1241/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5529 - val_acc: 0.3333\n",
      "Epoch 1242/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5515 - val_acc: 0.3333\n",
      "Epoch 1243/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5512 - val_acc: 0.3333\n",
      "Epoch 1244/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5507 - val_acc: 0.3333\n",
      "Epoch 1245/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5505 - val_acc: 0.3333\n",
      "Epoch 1246/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 3.5641 - val_acc: 0.3333\n",
      "Epoch 1247/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5694 - val_acc: 0.3333\n",
      "Epoch 1248/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5744 - val_acc: 0.3333\n",
      "Epoch 1249/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.5767 - val_acc: 0.3333\n",
      "Epoch 1250/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 3.5796 - val_acc: 0.3333\n",
      "Epoch 1251/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5826 - val_acc: 0.3333\n",
      "Epoch 1252/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5847 - val_acc: 0.3333\n",
      "Epoch 1253/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5867 - val_acc: 0.3333\n",
      "Epoch 1254/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5884 - val_acc: 0.3333\n",
      "Epoch 1255/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5888 - val_acc: 0.3333\n",
      "Epoch 1256/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5898 - val_acc: 0.3333\n",
      "Epoch 1257/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.5890 - val_acc: 0.3333\n",
      "Epoch 1258/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5891 - val_acc: 0.3333\n",
      "Epoch 1259/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5894 - val_acc: 0.3333\n",
      "Epoch 1260/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5888 - val_acc: 0.3333\n",
      "Epoch 1261/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5885 - val_acc: 0.3333\n",
      "Epoch 1262/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5884 - val_acc: 0.3333\n",
      "Epoch 1263/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5882 - val_acc: 0.3333\n",
      "Epoch 1264/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.5882 - val_acc: 0.3333\n",
      "Epoch 1265/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5884 - val_acc: 0.3333\n",
      "Epoch 1266/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5881 - val_acc: 0.3333\n",
      "Epoch 1267/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5879 - val_acc: 0.3333\n",
      "Epoch 1268/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5880 - val_acc: 0.3333\n",
      "Epoch 1269/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 3.5885 - val_acc: 0.3333\n",
      "Epoch 1270/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5890 - val_acc: 0.3333\n",
      "Epoch 1271/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5892 - val_acc: 0.3333\n",
      "Epoch 1272/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5896 - val_acc: 0.3333\n",
      "Epoch 1273/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.5877 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1274/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5870 - val_acc: 0.3333\n",
      "Epoch 1275/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5867 - val_acc: 0.3333\n",
      "Epoch 1276/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5854 - val_acc: 0.3333\n",
      "Epoch 1277/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5847 - val_acc: 0.3333\n",
      "Epoch 1278/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5837 - val_acc: 0.3333\n",
      "Epoch 1279/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.5828 - val_acc: 0.3333\n",
      "Epoch 1280/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.5780 - val_acc: 0.3333\n",
      "Epoch 1281/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5755 - val_acc: 0.3333\n",
      "Epoch 1282/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5730 - val_acc: 0.3333\n",
      "Epoch 1283/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5708 - val_acc: 0.3333\n",
      "Epoch 1284/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5691 - val_acc: 0.3333\n",
      "Epoch 1285/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5669 - val_acc: 0.3333\n",
      "Epoch 1286/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5648 - val_acc: 0.3333\n",
      "Epoch 1287/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5632 - val_acc: 0.3333\n",
      "Epoch 1288/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5615 - val_acc: 0.3333\n",
      "Epoch 1289/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5606 - val_acc: 0.3333\n",
      "Epoch 1290/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5596 - val_acc: 0.3333\n",
      "Epoch 1291/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5590 - val_acc: 0.3333\n",
      "Epoch 1292/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 3.5546 - val_acc: 0.3333\n",
      "Epoch 1293/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5524 - val_acc: 0.3333\n",
      "Epoch 1294/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5503 - val_acc: 0.3333\n",
      "Epoch 1295/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5484 - val_acc: 0.3333\n",
      "Epoch 1296/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5472 - val_acc: 0.3333\n",
      "Epoch 1297/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5460 - val_acc: 0.3333\n",
      "Epoch 1298/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5449 - val_acc: 0.3333\n",
      "Epoch 1299/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5439 - val_acc: 0.3333\n",
      "Epoch 1300/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5425 - val_acc: 0.3333\n",
      "Epoch 1301/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5416 - val_acc: 0.3333\n",
      "Epoch 1302/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5402 - val_acc: 0.3333\n",
      "Epoch 1303/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5395 - val_acc: 0.3333\n",
      "Epoch 1304/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5388 - val_acc: 0.3333\n",
      "Epoch 1305/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5384 - val_acc: 0.3333\n",
      "Epoch 1306/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5379 - val_acc: 0.3333\n",
      "Epoch 1307/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5375 - val_acc: 0.3333\n",
      "Epoch 1308/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5369 - val_acc: 0.3333\n",
      "Epoch 1309/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5367 - val_acc: 0.3333\n",
      "Epoch 1310/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5367 - val_acc: 0.3333\n",
      "Epoch 1311/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5365 - val_acc: 0.3333\n",
      "Epoch 1312/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5363 - val_acc: 0.3333\n",
      "Epoch 1313/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5362 - val_acc: 0.3333\n",
      "Epoch 1314/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5359 - val_acc: 0.3333\n",
      "Epoch 1315/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5356 - val_acc: 0.3333\n",
      "Epoch 1316/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5357 - val_acc: 0.3333\n",
      "Epoch 1317/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5349 - val_acc: 0.3333\n",
      "Epoch 1318/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5343 - val_acc: 0.3333\n",
      "Epoch 1319/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5337 - val_acc: 0.3333\n",
      "Epoch 1320/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5338 - val_acc: 0.3333\n",
      "Epoch 1321/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5335 - val_acc: 0.3333\n",
      "Epoch 1322/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5340 - val_acc: 0.3333\n",
      "Epoch 1323/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.5344 - val_acc: 0.3333\n",
      "Epoch 1324/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5347 - val_acc: 0.3333\n",
      "Epoch 1325/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5350 - val_acc: 0.3333\n",
      "Epoch 1326/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5354 - val_acc: 0.3333\n",
      "Epoch 1327/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5331 - val_acc: 0.3333\n",
      "Epoch 1328/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.3333\n",
      "Epoch 1329/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5312 - val_acc: 0.3333\n",
      "Epoch 1330/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5301 - val_acc: 0.3333\n",
      "Epoch 1331/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5294 - val_acc: 0.3333\n",
      "Epoch 1332/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.5288 - val_acc: 0.3333\n",
      "Epoch 1333/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5281 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1334/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5268 - val_acc: 0.3333\n",
      "Epoch 1335/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5263 - val_acc: 0.3333\n",
      "Epoch 1336/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5258 - val_acc: 0.3333\n",
      "Epoch 1337/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5252 - val_acc: 0.3333\n",
      "Epoch 1338/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5247 - val_acc: 0.3333\n",
      "Epoch 1339/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5243 - val_acc: 0.3333\n",
      "Epoch 1340/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5256 - val_acc: 0.3333\n",
      "Epoch 1341/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5266 - val_acc: 0.3333\n",
      "Epoch 1342/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5276 - val_acc: 0.3333\n",
      "Epoch 1343/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5276 - val_acc: 0.3333\n",
      "Epoch 1344/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5275 - val_acc: 0.3333\n",
      "Epoch 1345/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5275 - val_acc: 0.3333\n",
      "Epoch 1346/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5277 - val_acc: 0.3333\n",
      "Epoch 1347/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5278 - val_acc: 0.3333\n",
      "Epoch 1348/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5279 - val_acc: 0.3333\n",
      "Epoch 1349/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 3.5288 - val_acc: 0.3333\n",
      "Epoch 1350/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5293 - val_acc: 0.3333\n",
      "Epoch 1351/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5299 - val_acc: 0.3333\n",
      "Epoch 1352/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5304 - val_acc: 0.3333\n",
      "Epoch 1353/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5308 - val_acc: 0.3333\n",
      "Epoch 1354/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5314 - val_acc: 0.3333\n",
      "Epoch 1355/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5312 - val_acc: 0.3333\n",
      "Epoch 1356/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5314 - val_acc: 0.3333\n",
      "Epoch 1357/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.3333\n",
      "Epoch 1358/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5320 - val_acc: 0.3333\n",
      "Epoch 1359/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5325 - val_acc: 0.3333\n",
      "Epoch 1360/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5330 - val_acc: 0.3333\n",
      "Epoch 1361/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5335 - val_acc: 0.3333\n",
      "Epoch 1362/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5340 - val_acc: 0.3333\n",
      "Epoch 1363/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5343 - val_acc: 0.3333\n",
      "Epoch 1364/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5344 - val_acc: 0.3333\n",
      "Epoch 1365/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5347 - val_acc: 0.3333\n",
      "Epoch 1366/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5349 - val_acc: 0.3333\n",
      "Epoch 1367/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5352 - val_acc: 0.3333\n",
      "Epoch 1368/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 3.5357 - val_acc: 0.3333\n",
      "Epoch 1369/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5362 - val_acc: 0.3333\n",
      "Epoch 1370/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5368 - val_acc: 0.3333\n",
      "Epoch 1371/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5381 - val_acc: 0.3333\n",
      "Epoch 1372/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5386 - val_acc: 0.3333\n",
      "Epoch 1373/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5393 - val_acc: 0.3333\n",
      "Epoch 1374/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5397 - val_acc: 0.3333\n",
      "Epoch 1375/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5396 - val_acc: 0.3333\n",
      "Epoch 1376/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5399 - val_acc: 0.3333\n",
      "Epoch 1377/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5398 - val_acc: 0.3333\n",
      "Epoch 1378/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5385 - val_acc: 0.3333\n",
      "Epoch 1379/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5379 - val_acc: 0.3333\n",
      "Epoch 1380/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5368 - val_acc: 0.3333\n",
      "Epoch 1381/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5363 - val_acc: 0.3333\n",
      "Epoch 1382/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5357 - val_acc: 0.3333\n",
      "Epoch 1383/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5352 - val_acc: 0.3333\n",
      "Epoch 1384/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5344 - val_acc: 0.3333\n",
      "Epoch 1385/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5332 - val_acc: 0.3333\n",
      "Epoch 1386/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5324 - val_acc: 0.3333\n",
      "Epoch 1387/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.5311 - val_acc: 0.3333\n",
      "Epoch 1388/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5301 - val_acc: 0.3333\n",
      "Epoch 1389/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5289 - val_acc: 0.3333\n",
      "Epoch 1390/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5277 - val_acc: 0.3333\n",
      "Epoch 1391/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5274 - val_acc: 0.3333\n",
      "Epoch 1392/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5264 - val_acc: 0.3333\n",
      "Epoch 1393/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5257 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1394/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5241 - val_acc: 0.3333\n",
      "Epoch 1395/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5230 - val_acc: 0.3333\n",
      "Epoch 1396/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5222 - val_acc: 0.3333\n",
      "Epoch 1397/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5218 - val_acc: 0.3333\n",
      "Epoch 1398/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5209 - val_acc: 0.3333\n",
      "Epoch 1399/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5206 - val_acc: 0.3333\n",
      "Epoch 1400/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5190 - val_acc: 0.3333\n",
      "Epoch 1401/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5184 - val_acc: 0.3333\n",
      "Epoch 1402/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5178 - val_acc: 0.3333\n",
      "Epoch 1403/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5166 - val_acc: 0.3333\n",
      "Epoch 1404/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5160 - val_acc: 0.3333\n",
      "Epoch 1405/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5154 - val_acc: 0.3333\n",
      "Epoch 1406/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5151 - val_acc: 0.3333\n",
      "Epoch 1407/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.5109 - val_acc: 0.3333\n",
      "Epoch 1408/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5087 - val_acc: 0.3333\n",
      "Epoch 1409/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5056 - val_acc: 0.3333\n",
      "Epoch 1410/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5045 - val_acc: 0.3333\n",
      "Epoch 1411/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5030 - val_acc: 0.3333\n",
      "Epoch 1412/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5015 - val_acc: 0.3333\n",
      "Epoch 1413/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.5009 - val_acc: 0.3333\n",
      "Epoch 1414/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5000 - val_acc: 0.3333\n",
      "Epoch 1415/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4983 - val_acc: 0.3333\n",
      "Epoch 1416/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4973 - val_acc: 0.3333\n",
      "Epoch 1417/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.4971 - val_acc: 0.3333\n",
      "Epoch 1418/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4965 - val_acc: 0.3333\n",
      "Epoch 1419/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4960 - val_acc: 0.3333\n",
      "Epoch 1420/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4958 - val_acc: 0.3333\n",
      "Epoch 1421/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4946 - val_acc: 0.3333\n",
      "Epoch 1422/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4943 - val_acc: 0.3333\n",
      "Epoch 1423/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.4876 - val_acc: 0.3333\n",
      "Epoch 1424/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4842 - val_acc: 0.3333\n",
      "Epoch 1425/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4814 - val_acc: 0.3333\n",
      "Epoch 1426/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4788 - val_acc: 0.3333\n",
      "Epoch 1427/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4766 - val_acc: 0.3333\n",
      "Epoch 1428/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4747 - val_acc: 0.3333\n",
      "Epoch 1429/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4733 - val_acc: 0.3333\n",
      "Epoch 1430/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4717 - val_acc: 0.3333\n",
      "Epoch 1431/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4704 - val_acc: 0.3333\n",
      "Epoch 1432/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4692 - val_acc: 0.3333\n",
      "Epoch 1433/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.4681 - val_acc: 0.3333\n",
      "Epoch 1434/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.4663 - val_acc: 0.3333\n",
      "Epoch 1435/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4656 - val_acc: 0.3333\n",
      "Epoch 1436/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4643 - val_acc: 0.3333\n",
      "Epoch 1437/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4636 - val_acc: 0.3333\n",
      "Epoch 1438/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4630 - val_acc: 0.3333\n",
      "Epoch 1439/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4625 - val_acc: 0.3333\n",
      "Epoch 1440/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4623 - val_acc: 0.3333\n",
      "Epoch 1441/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4615 - val_acc: 0.3333\n",
      "Epoch 1442/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4611 - val_acc: 0.3333\n",
      "Epoch 1443/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4601 - val_acc: 0.3333\n",
      "Epoch 1444/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4599 - val_acc: 0.3333\n",
      "Epoch 1445/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4595 - val_acc: 0.3333\n",
      "Epoch 1446/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4594 - val_acc: 0.3333\n",
      "Epoch 1447/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4598 - val_acc: 0.3333\n",
      "Epoch 1448/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4599 - val_acc: 0.3333\n",
      "Epoch 1449/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4603 - val_acc: 0.3333\n",
      "Epoch 1450/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4606 - val_acc: 0.3333\n",
      "Epoch 1451/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.4631 - val_acc: 0.3333\n",
      "Epoch 1452/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4646 - val_acc: 0.3333\n",
      "Epoch 1453/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4660 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1454/2000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.4638 - val_acc: 0.3333\n",
      "Epoch 1455/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 3.4668 - val_acc: 0.3333\n",
      "Epoch 1456/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4679 - val_acc: 0.3333\n",
      "Epoch 1457/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4706 - val_acc: 0.3333\n",
      "Epoch 1458/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4721 - val_acc: 0.3333\n",
      "Epoch 1459/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4717 - val_acc: 0.3333\n",
      "Epoch 1460/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.4717 - val_acc: 0.3333\n",
      "Epoch 1461/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4725 - val_acc: 0.3333\n",
      "Epoch 1462/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4732 - val_acc: 0.3333\n",
      "Epoch 1463/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4738 - val_acc: 0.3333\n",
      "Epoch 1464/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4744 - val_acc: 0.3333\n",
      "Epoch 1465/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4756 - val_acc: 0.3333\n",
      "Epoch 1466/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4764 - val_acc: 0.3333\n",
      "Epoch 1467/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4768 - val_acc: 0.3333\n",
      "Epoch 1468/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4775 - val_acc: 0.3333\n",
      "Epoch 1469/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4783 - val_acc: 0.3333\n",
      "Epoch 1470/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4792 - val_acc: 0.3333\n",
      "Epoch 1471/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4800 - val_acc: 0.3333\n",
      "Epoch 1472/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4804 - val_acc: 0.3333\n",
      "Epoch 1473/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4810 - val_acc: 0.3333\n",
      "Epoch 1474/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4810 - val_acc: 0.3333\n",
      "Epoch 1475/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.4813 - val_acc: 0.3333\n",
      "Epoch 1476/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4817 - val_acc: 0.3333\n",
      "Epoch 1477/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.4800 - val_acc: 0.3333\n",
      "Epoch 1478/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4789 - val_acc: 0.3333\n",
      "Epoch 1479/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4777 - val_acc: 0.3333\n",
      "Epoch 1480/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 3.4713 - val_acc: 0.3333\n",
      "Epoch 1481/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4684 - val_acc: 0.3333\n",
      "Epoch 1482/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4659 - val_acc: 0.3333\n",
      "Epoch 1483/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4634 - val_acc: 0.3333\n",
      "Epoch 1484/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.4615 - val_acc: 0.3333\n",
      "Epoch 1485/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4591 - val_acc: 0.3333\n",
      "Epoch 1486/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4574 - val_acc: 0.3333\n",
      "Epoch 1487/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4557 - val_acc: 0.3333\n",
      "Epoch 1488/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4543 - val_acc: 0.3333\n",
      "Epoch 1489/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4535 - val_acc: 0.3333\n",
      "Epoch 1490/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4528 - val_acc: 0.3333\n",
      "Epoch 1491/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4526 - val_acc: 0.3333\n",
      "Epoch 1492/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.4535 - val_acc: 0.3333\n",
      "Epoch 1493/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4537 - val_acc: 0.3333\n",
      "Epoch 1494/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4540 - val_acc: 0.3333\n",
      "Epoch 1495/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.4652 - val_acc: 0.3333\n",
      "Epoch 1496/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4701 - val_acc: 0.3333\n",
      "Epoch 1497/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 3.4710 - val_acc: 0.3333\n",
      "Epoch 1498/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4737 - val_acc: 0.3333\n",
      "Epoch 1499/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4759 - val_acc: 0.3333\n",
      "Epoch 1500/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.4787 - val_acc: 0.3333\n",
      "Epoch 1501/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4807 - val_acc: 0.3333\n",
      "Epoch 1502/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4830 - val_acc: 0.3333\n",
      "Epoch 1503/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4853 - val_acc: 0.3333\n",
      "Epoch 1504/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4874 - val_acc: 0.3333\n",
      "Epoch 1505/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4892 - val_acc: 0.3333\n",
      "Epoch 1506/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4908 - val_acc: 0.3333\n",
      "Epoch 1507/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.4926 - val_acc: 0.3333\n",
      "Epoch 1508/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.4940 - val_acc: 0.3333\n",
      "Epoch 1509/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.4948 - val_acc: 0.3333\n",
      "Epoch 1510/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.4968 - val_acc: 0.3333\n",
      "Epoch 1511/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4981 - val_acc: 0.3333\n",
      "Epoch 1512/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4995 - val_acc: 0.3333\n",
      "Epoch 1513/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5007 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1514/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5018 - val_acc: 0.3333\n",
      "Epoch 1515/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5035 - val_acc: 0.3333\n",
      "Epoch 1516/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5048 - val_acc: 0.3333\n",
      "Epoch 1517/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5058 - val_acc: 0.3333\n",
      "Epoch 1518/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5066 - val_acc: 0.3333\n",
      "Epoch 1519/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5068 - val_acc: 0.3333\n",
      "Epoch 1520/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5064 - val_acc: 0.3333\n",
      "Epoch 1521/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5065 - val_acc: 0.3333\n",
      "Epoch 1522/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5054 - val_acc: 0.3333\n",
      "Epoch 1523/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5053 - val_acc: 0.3333\n",
      "Epoch 1524/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5055 - val_acc: 0.3333\n",
      "Epoch 1525/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5055 - val_acc: 0.3333\n",
      "Epoch 1526/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5059 - val_acc: 0.3333\n",
      "Epoch 1527/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5051 - val_acc: 0.3333\n",
      "Epoch 1528/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5049 - val_acc: 0.3333\n",
      "Epoch 1529/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5044 - val_acc: 0.3333\n",
      "Epoch 1530/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5042 - val_acc: 0.3333\n",
      "Epoch 1531/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5040 - val_acc: 0.3333\n",
      "Epoch 1532/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5039 - val_acc: 0.3333\n",
      "Epoch 1533/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5034 - val_acc: 0.3333\n",
      "Epoch 1534/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5027 - val_acc: 0.3333\n",
      "Epoch 1535/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5029 - val_acc: 0.3333\n",
      "Epoch 1536/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5027 - val_acc: 0.3333\n",
      "Epoch 1537/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5027 - val_acc: 0.3333\n",
      "Epoch 1538/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5054 - val_acc: 0.3333\n",
      "Epoch 1539/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5067 - val_acc: 0.3333\n",
      "Epoch 1540/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5062 - val_acc: 0.3333\n",
      "Epoch 1541/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5073 - val_acc: 0.3333\n",
      "Epoch 1542/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5078 - val_acc: 0.3333\n",
      "Epoch 1543/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5081 - val_acc: 0.3333\n",
      "Epoch 1544/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5083 - val_acc: 0.3333\n",
      "Epoch 1545/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5088 - val_acc: 0.3333\n",
      "Epoch 1546/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5100 - val_acc: 0.3333\n",
      "Epoch 1547/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5108 - val_acc: 0.3333\n",
      "Epoch 1548/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5113 - val_acc: 0.3333\n",
      "Epoch 1549/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5117 - val_acc: 0.3333\n",
      "Epoch 1550/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5120 - val_acc: 0.3333\n",
      "Epoch 1551/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.5150 - val_acc: 0.3333\n",
      "Epoch 1552/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5167 - val_acc: 0.3333\n",
      "Epoch 1553/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5181 - val_acc: 0.3333\n",
      "Epoch 1554/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5188 - val_acc: 0.3333\n",
      "Epoch 1555/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5196 - val_acc: 0.3333\n",
      "Epoch 1556/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5204 - val_acc: 0.3333\n",
      "Epoch 1557/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5211 - val_acc: 0.3333\n",
      "Epoch 1558/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5219 - val_acc: 0.3333\n",
      "Epoch 1559/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5230 - val_acc: 0.3333\n",
      "Epoch 1560/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5239 - val_acc: 0.3333\n",
      "Epoch 1561/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 3.5216 - val_acc: 0.3333\n",
      "Epoch 1562/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5215 - val_acc: 0.3333\n",
      "Epoch 1563/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5212 - val_acc: 0.3333\n",
      "Epoch 1564/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5211 - val_acc: 0.3333\n",
      "Epoch 1565/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5212 - val_acc: 0.3333\n",
      "Epoch 1566/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5218 - val_acc: 0.3333\n",
      "Epoch 1567/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5223 - val_acc: 0.3333\n",
      "Epoch 1568/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5256 - val_acc: 0.3333\n",
      "Epoch 1569/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5273 - val_acc: 0.3333\n",
      "Epoch 1570/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5288 - val_acc: 0.3333\n",
      "Epoch 1571/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5301 - val_acc: 0.3333\n",
      "Epoch 1572/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5312 - val_acc: 0.3333\n",
      "Epoch 1573/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.5337 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1574/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5349 - val_acc: 0.3333\n",
      "Epoch 1575/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5356 - val_acc: 0.3333\n",
      "Epoch 1576/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5367 - val_acc: 0.3333\n",
      "Epoch 1577/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5365 - val_acc: 0.3333\n",
      "Epoch 1578/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5369 - val_acc: 0.3333\n",
      "Epoch 1579/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5373 - val_acc: 0.3333\n",
      "Epoch 1580/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5379 - val_acc: 0.3333\n",
      "Epoch 1581/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5384 - val_acc: 0.3333\n",
      "Epoch 1582/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5389 - val_acc: 0.3333\n",
      "Epoch 1583/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5397 - val_acc: 0.3333\n",
      "Epoch 1584/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5388 - val_acc: 0.3333\n",
      "Epoch 1585/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5388 - val_acc: 0.3333\n",
      "Epoch 1586/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5387 - val_acc: 0.3333\n",
      "Epoch 1587/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5385 - val_acc: 0.3333\n",
      "Epoch 1588/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5385 - val_acc: 0.3333\n",
      "Epoch 1589/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5388 - val_acc: 0.3333\n",
      "Epoch 1590/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5388 - val_acc: 0.3333\n",
      "Epoch 1591/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5389 - val_acc: 0.3333\n",
      "Epoch 1592/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5392 - val_acc: 0.3333\n",
      "Epoch 1593/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5393 - val_acc: 0.3333\n",
      "Epoch 1594/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5394 - val_acc: 0.3333\n",
      "Epoch 1595/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5397 - val_acc: 0.3333\n",
      "Epoch 1596/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5399 - val_acc: 0.3333\n",
      "Epoch 1597/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5416 - val_acc: 0.3333\n",
      "Epoch 1598/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5425 - val_acc: 0.3333\n",
      "Epoch 1599/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5416 - val_acc: 0.3333\n",
      "Epoch 1600/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5418 - val_acc: 0.3333\n",
      "Epoch 1601/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5420 - val_acc: 0.3333\n",
      "Epoch 1602/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5422 - val_acc: 0.3333\n",
      "Epoch 1603/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5426 - val_acc: 0.3333\n",
      "Epoch 1604/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5428 - val_acc: 0.3333\n",
      "Epoch 1605/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.5457 - val_acc: 0.3333\n",
      "Epoch 1606/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.5477 - val_acc: 0.3333\n",
      "Epoch 1607/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5491 - val_acc: 0.3333\n",
      "Epoch 1608/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5506 - val_acc: 0.3333\n",
      "Epoch 1609/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5516 - val_acc: 0.3333\n",
      "Epoch 1610/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5528 - val_acc: 0.3333\n",
      "Epoch 1611/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5537 - val_acc: 0.3333\n",
      "Epoch 1612/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5537 - val_acc: 0.3333\n",
      "Epoch 1613/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5540 - val_acc: 0.3333\n",
      "Epoch 1614/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5546 - val_acc: 0.3333\n",
      "Epoch 1615/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5556 - val_acc: 0.3333\n",
      "Epoch 1616/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.5515 - val_acc: 0.3333\n",
      "Epoch 1617/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5500 - val_acc: 0.3333\n",
      "Epoch 1618/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5485 - val_acc: 0.3333\n",
      "Epoch 1619/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5472 - val_acc: 0.3333\n",
      "Epoch 1620/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5461 - val_acc: 0.3333\n",
      "Epoch 1621/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5448 - val_acc: 0.3333\n",
      "Epoch 1622/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5410 - val_acc: 0.3333\n",
      "Epoch 1623/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5391 - val_acc: 0.3333\n",
      "Epoch 1624/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5369 - val_acc: 0.3333\n",
      "Epoch 1625/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5367 - val_acc: 0.3333\n",
      "Epoch 1626/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5354 - val_acc: 0.3333\n",
      "Epoch 1627/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5345 - val_acc: 0.3333\n",
      "Epoch 1628/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5346 - val_acc: 0.3333\n",
      "Epoch 1629/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5346 - val_acc: 0.3333\n",
      "Epoch 1630/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.5317 - val_acc: 0.3333\n",
      "Epoch 1631/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5319 - val_acc: 0.3333\n",
      "Epoch 1632/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5314 - val_acc: 0.3333\n",
      "Epoch 1633/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5309 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1634/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5314 - val_acc: 0.3333\n",
      "Epoch 1635/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5315 - val_acc: 0.3333\n",
      "Epoch 1636/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.3333\n",
      "Epoch 1637/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5321 - val_acc: 0.3333\n",
      "Epoch 1638/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5323 - val_acc: 0.3333\n",
      "Epoch 1639/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5327 - val_acc: 0.3333\n",
      "Epoch 1640/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5323 - val_acc: 0.3333\n",
      "Epoch 1641/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.3333\n",
      "Epoch 1642/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5315 - val_acc: 0.3333\n",
      "Epoch 1643/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5318 - val_acc: 0.3333\n",
      "Epoch 1644/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5292 - val_acc: 0.3333\n",
      "Epoch 1645/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.5283 - val_acc: 0.3333\n",
      "Epoch 1646/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5275 - val_acc: 0.3333\n",
      "Epoch 1647/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5272 - val_acc: 0.3333\n",
      "Epoch 1648/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5274 - val_acc: 0.3333\n",
      "Epoch 1649/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5268 - val_acc: 0.3333\n",
      "Epoch 1650/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5260 - val_acc: 0.3333\n",
      "Epoch 1651/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5258 - val_acc: 0.3333\n",
      "Epoch 1652/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5249 - val_acc: 0.3333\n",
      "Epoch 1653/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.5245 - val_acc: 0.3333\n",
      "Epoch 1654/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5246 - val_acc: 0.3333\n",
      "Epoch 1655/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5245 - val_acc: 0.3333\n",
      "Epoch 1656/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5244 - val_acc: 0.3333\n",
      "Epoch 1657/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5246 - val_acc: 0.3333\n",
      "Epoch 1658/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5247 - val_acc: 0.3333\n",
      "Epoch 1659/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5248 - val_acc: 0.3333\n",
      "Epoch 1660/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.5228 - val_acc: 0.3333\n",
      "Epoch 1661/2000\n",
      "18/18 [==============================] - 0s 500us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5223 - val_acc: 0.3333\n",
      "Epoch 1662/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5221 - val_acc: 0.3333\n",
      "Epoch 1663/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5218 - val_acc: 0.3333\n",
      "Epoch 1664/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5216 - val_acc: 0.3333\n",
      "Epoch 1665/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5213 - val_acc: 0.3333\n",
      "Epoch 1666/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5213 - val_acc: 0.3333\n",
      "Epoch 1667/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5214 - val_acc: 0.3333\n",
      "Epoch 1668/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5262 - val_acc: 0.3333\n",
      "Epoch 1669/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5280 - val_acc: 0.3333\n",
      "Epoch 1670/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5299 - val_acc: 0.3333\n",
      "Epoch 1671/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5317 - val_acc: 0.3333\n",
      "Epoch 1672/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5369 - val_acc: 0.3333\n",
      "Epoch 1673/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5401 - val_acc: 0.3333\n",
      "Epoch 1674/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5427 - val_acc: 0.3333\n",
      "Epoch 1675/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5451 - val_acc: 0.3333\n",
      "Epoch 1676/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5469 - val_acc: 0.3333\n",
      "Epoch 1677/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5495 - val_acc: 0.3333\n",
      "Epoch 1678/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5508 - val_acc: 0.3333\n",
      "Epoch 1679/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5525 - val_acc: 0.3333\n",
      "Epoch 1680/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5550 - val_acc: 0.3333\n",
      "Epoch 1681/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.5569 - val_acc: 0.3333\n",
      "Epoch 1682/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5594 - val_acc: 0.3333\n",
      "Epoch 1683/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.5684 - val_acc: 0.3333\n",
      "Epoch 1684/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5736 - val_acc: 0.3333\n",
      "Epoch 1685/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5768 - val_acc: 0.3333\n",
      "Epoch 1686/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5800 - val_acc: 0.3333\n",
      "Epoch 1687/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5831 - val_acc: 0.3333\n",
      "Epoch 1688/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5855 - val_acc: 0.3333\n",
      "Epoch 1689/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5886 - val_acc: 0.3333\n",
      "Epoch 1690/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5910 - val_acc: 0.3333\n",
      "Epoch 1691/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5934 - val_acc: 0.3333\n",
      "Epoch 1692/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5956 - val_acc: 0.3333\n",
      "Epoch 1693/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5976 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1694/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.5987 - val_acc: 0.3333\n",
      "Epoch 1695/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6002 - val_acc: 0.3333\n",
      "Epoch 1696/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.6013 - val_acc: 0.3333\n",
      "Epoch 1697/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.6002 - val_acc: 0.3333\n",
      "Epoch 1698/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.6012 - val_acc: 0.3333\n",
      "Epoch 1699/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6019 - val_acc: 0.3333\n",
      "Epoch 1700/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.6026 - val_acc: 0.3333\n",
      "Epoch 1701/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6032 - val_acc: 0.3333\n",
      "Epoch 1702/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6039 - val_acc: 0.3333\n",
      "Epoch 1703/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6049 - val_acc: 0.3333\n",
      "Epoch 1704/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.6053 - val_acc: 0.3333\n",
      "Epoch 1705/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6061 - val_acc: 0.3333\n",
      "Epoch 1706/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6066 - val_acc: 0.3333\n",
      "Epoch 1707/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.6071 - val_acc: 0.3333\n",
      "Epoch 1708/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.6073 - val_acc: 0.3333\n",
      "Epoch 1709/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.6063 - val_acc: 0.3333\n",
      "Epoch 1710/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6059 - val_acc: 0.3333\n",
      "Epoch 1711/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.6053 - val_acc: 0.3333\n",
      "Epoch 1712/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6050 - val_acc: 0.3333\n",
      "Epoch 1713/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.6052 - val_acc: 0.3333\n",
      "Epoch 1714/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6051 - val_acc: 0.3333\n",
      "Epoch 1715/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6050 - val_acc: 0.3333\n",
      "Epoch 1716/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6047 - val_acc: 0.3333\n",
      "Epoch 1717/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6046 - val_acc: 0.3333\n",
      "Epoch 1718/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.6032 - val_acc: 0.3333\n",
      "Epoch 1719/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.6033 - val_acc: 0.3333\n",
      "Epoch 1720/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6029 - val_acc: 0.3333\n",
      "Epoch 1721/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6031 - val_acc: 0.3333\n",
      "Epoch 1722/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6031 - val_acc: 0.3333\n",
      "Epoch 1723/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6031 - val_acc: 0.3333\n",
      "Epoch 1724/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.6024 - val_acc: 0.3333\n",
      "Epoch 1725/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6022 - val_acc: 0.3333\n",
      "Epoch 1726/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.6045 - val_acc: 0.3333\n",
      "Epoch 1727/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6055 - val_acc: 0.3333\n",
      "Epoch 1728/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6059 - val_acc: 0.3333\n",
      "Epoch 1729/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6065 - val_acc: 0.3333\n",
      "Epoch 1730/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.6378 - val_acc: 0.3333\n",
      "Epoch 1731/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6512 - val_acc: 0.3333\n",
      "Epoch 1732/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6630 - val_acc: 0.3333\n",
      "Epoch 1733/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.6722 - val_acc: 0.3333\n",
      "Epoch 1734/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6812 - val_acc: 0.3333\n",
      "Epoch 1735/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6894 - val_acc: 0.3333\n",
      "Epoch 1736/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6968 - val_acc: 0.3333\n",
      "Epoch 1737/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.7038 - val_acc: 0.3333\n",
      "Epoch 1738/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7097 - val_acc: 0.3333\n",
      "Epoch 1739/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.7155 - val_acc: 0.3333\n",
      "Epoch 1740/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.7208 - val_acc: 0.3333\n",
      "Epoch 1741/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7254 - val_acc: 0.3333\n",
      "Epoch 1742/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7294 - val_acc: 0.3333\n",
      "Epoch 1743/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7330 - val_acc: 0.3333\n",
      "Epoch 1744/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7363 - val_acc: 0.3333\n",
      "Epoch 1745/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7385 - val_acc: 0.3333\n",
      "Epoch 1746/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7408 - val_acc: 0.3333\n",
      "Epoch 1747/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7426 - val_acc: 0.3333\n",
      "Epoch 1748/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7444 - val_acc: 0.3333\n",
      "Epoch 1749/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7459 - val_acc: 0.3333\n",
      "Epoch 1750/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7471 - val_acc: 0.3333\n",
      "Epoch 1751/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7477 - val_acc: 0.3333\n",
      "Epoch 1752/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7494 - val_acc: 0.3333\n",
      "Epoch 1753/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7511 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1754/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7524 - val_acc: 0.3333\n",
      "Epoch 1755/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7536 - val_acc: 0.3333\n",
      "Epoch 1756/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7549 - val_acc: 0.3333\n",
      "Epoch 1757/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7554 - val_acc: 0.3333\n",
      "Epoch 1758/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7561 - val_acc: 0.3333\n",
      "Epoch 1759/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7568 - val_acc: 0.3333\n",
      "Epoch 1760/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7569 - val_acc: 0.3333\n",
      "Epoch 1761/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7572 - val_acc: 0.3333\n",
      "Epoch 1762/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7574 - val_acc: 0.3333\n",
      "Epoch 1763/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7580 - val_acc: 0.3333\n",
      "Epoch 1764/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7584 - val_acc: 0.3333\n",
      "Epoch 1765/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7588 - val_acc: 0.3333\n",
      "Epoch 1766/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7591 - val_acc: 0.3333\n",
      "Epoch 1767/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7596 - val_acc: 0.3333\n",
      "Epoch 1768/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7600 - val_acc: 0.3333\n",
      "Epoch 1769/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7603 - val_acc: 0.3333\n",
      "Epoch 1770/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7607 - val_acc: 0.3333\n",
      "Epoch 1771/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.7611 - val_acc: 0.3333\n",
      "Epoch 1772/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7614 - val_acc: 0.3333\n",
      "Epoch 1773/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7619 - val_acc: 0.3333\n",
      "Epoch 1774/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.7615 - val_acc: 0.3333\n",
      "Epoch 1775/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7617 - val_acc: 0.3333\n",
      "Epoch 1776/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7616 - val_acc: 0.3333\n",
      "Epoch 1777/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 3.7618 - val_acc: 0.3333\n",
      "Epoch 1778/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.7622 - val_acc: 0.3333\n",
      "Epoch 1779/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7626 - val_acc: 0.3333\n",
      "Epoch 1780/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.7634 - val_acc: 0.3333\n",
      "Epoch 1781/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7641 - val_acc: 0.3333\n",
      "Epoch 1782/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7645 - val_acc: 0.3333\n",
      "Epoch 1783/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7648 - val_acc: 0.3333\n",
      "Epoch 1784/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7652 - val_acc: 0.3333\n",
      "Epoch 1785/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7653 - val_acc: 0.3333\n",
      "Epoch 1786/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.7658 - val_acc: 0.3333\n",
      "Epoch 1787/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7662 - val_acc: 0.3333\n",
      "Epoch 1788/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7669 - val_acc: 0.3333\n",
      "Epoch 1789/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.7666 - val_acc: 0.3333\n",
      "Epoch 1790/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7660 - val_acc: 0.3333\n",
      "Epoch 1791/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7657 - val_acc: 0.3333\n",
      "Epoch 1792/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7655 - val_acc: 0.3333\n",
      "Epoch 1793/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7646 - val_acc: 0.3333\n",
      "Epoch 1794/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7641 - val_acc: 0.3333\n",
      "Epoch 1795/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7640 - val_acc: 0.3333\n",
      "Epoch 1796/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.7654 - val_acc: 0.3333\n",
      "Epoch 1797/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7653 - val_acc: 0.3333\n",
      "Epoch 1798/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7651 - val_acc: 0.3333\n",
      "Epoch 1799/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.7637 - val_acc: 0.3333\n",
      "Epoch 1800/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7631 - val_acc: 0.3333\n",
      "Epoch 1801/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7628 - val_acc: 0.3333\n",
      "Epoch 1802/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7623 - val_acc: 0.3333\n",
      "Epoch 1803/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7619 - val_acc: 0.3333\n",
      "Epoch 1804/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7616 - val_acc: 0.3333\n",
      "Epoch 1805/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7614 - val_acc: 0.3333\n",
      "Epoch 1806/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7612 - val_acc: 0.3333\n",
      "Epoch 1807/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7610 - val_acc: 0.3333\n",
      "Epoch 1808/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7605 - val_acc: 0.3333\n",
      "Epoch 1809/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7608 - val_acc: 0.3333\n",
      "Epoch 1810/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7607 - val_acc: 0.3333\n",
      "Epoch 1811/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7607 - val_acc: 0.3333\n",
      "Epoch 1812/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7603 - val_acc: 0.3333\n",
      "Epoch 1813/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7605 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1814/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7604 - val_acc: 0.3333\n",
      "Epoch 1815/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7604 - val_acc: 0.3333\n",
      "Epoch 1816/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.7689 - val_acc: 0.3333\n",
      "Epoch 1817/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7723 - val_acc: 0.3333\n",
      "Epoch 1818/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7754 - val_acc: 0.3333\n",
      "Epoch 1819/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7783 - val_acc: 0.3333\n",
      "Epoch 1820/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7810 - val_acc: 0.3333\n",
      "Epoch 1821/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7831 - val_acc: 0.3333\n",
      "Epoch 1822/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7854 - val_acc: 0.3333\n",
      "Epoch 1823/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7876 - val_acc: 0.3333\n",
      "Epoch 1824/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7896 - val_acc: 0.3333\n",
      "Epoch 1825/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7913 - val_acc: 0.3333\n",
      "Epoch 1826/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7930 - val_acc: 0.3333\n",
      "Epoch 1827/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7939 - val_acc: 0.3333\n",
      "Epoch 1828/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7949 - val_acc: 0.3333\n",
      "Epoch 1829/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7958 - val_acc: 0.3333\n",
      "Epoch 1830/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7966 - val_acc: 0.3333\n",
      "Epoch 1831/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7972 - val_acc: 0.3333\n",
      "Epoch 1832/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 3.7473 - val_acc: 0.3333\n",
      "Epoch 1833/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7257 - val_acc: 0.3333\n",
      "Epoch 1834/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7060 - val_acc: 0.3333\n",
      "Epoch 1835/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6884 - val_acc: 0.3333\n",
      "Epoch 1836/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6733 - val_acc: 0.3333\n",
      "Epoch 1837/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.6602 - val_acc: 0.3333\n",
      "Epoch 1838/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6480 - val_acc: 0.3333\n",
      "Epoch 1839/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6366 - val_acc: 0.3333\n",
      "Epoch 1840/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6265 - val_acc: 0.3333\n",
      "Epoch 1841/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6177 - val_acc: 0.3333\n",
      "Epoch 1842/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6098 - val_acc: 0.3333\n",
      "Epoch 1843/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0302 - acc: 1.0000 - val_loss: 3.6755 - val_acc: 0.3333\n",
      "Epoch 1844/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7000 - val_acc: 0.3333\n",
      "Epoch 1845/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7234 - val_acc: 0.3333\n",
      "Epoch 1846/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.7434 - val_acc: 0.3333\n",
      "Epoch 1847/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.7592 - val_acc: 0.3333\n",
      "Epoch 1848/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7737 - val_acc: 0.3333\n",
      "Epoch 1849/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7863 - val_acc: 0.3333\n",
      "Epoch 1850/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.7986 - val_acc: 0.3333\n",
      "Epoch 1851/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.8090 - val_acc: 0.3333\n",
      "Epoch 1852/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.8183 - val_acc: 0.3333\n",
      "Epoch 1853/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8264 - val_acc: 0.3333\n",
      "Epoch 1854/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8338 - val_acc: 0.3333\n",
      "Epoch 1855/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.8394 - val_acc: 0.3333\n",
      "Epoch 1856/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.8440 - val_acc: 0.3333\n",
      "Epoch 1857/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.8481 - val_acc: 0.3333\n",
      "Epoch 1858/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.8519 - val_acc: 0.3333\n",
      "Epoch 1859/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.8524 - val_acc: 0.3333\n",
      "Epoch 1860/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.8531 - val_acc: 0.3333\n",
      "Epoch 1861/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.8538 - val_acc: 0.3333\n",
      "Epoch 1862/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 3.8554 - val_acc: 0.3333\n",
      "Epoch 1863/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8566 - val_acc: 0.3333\n",
      "Epoch 1864/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.8522 - val_acc: 0.3333\n",
      "Epoch 1865/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.8461 - val_acc: 0.3333\n",
      "Epoch 1866/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8426 - val_acc: 0.3333\n",
      "Epoch 1867/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8394 - val_acc: 0.3333\n",
      "Epoch 1868/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8364 - val_acc: 0.3333\n",
      "Epoch 1869/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.8336 - val_acc: 0.3333\n",
      "Epoch 1870/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.8308 - val_acc: 0.3333\n",
      "Epoch 1871/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8284 - val_acc: 0.3333\n",
      "Epoch 1872/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.8262 - val_acc: 0.3333\n",
      "Epoch 1873/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.8251 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1874/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 3.8149 - val_acc: 0.3333\n",
      "Epoch 1875/2000\n",
      "18/18 [==============================] - 0s 445us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.8090 - val_acc: 0.3333\n",
      "Epoch 1876/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.8036 - val_acc: 0.3333\n",
      "Epoch 1877/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7988 - val_acc: 0.3333\n",
      "Epoch 1878/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.7942 - val_acc: 0.3333\n",
      "Epoch 1879/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7903 - val_acc: 0.3333\n",
      "Epoch 1880/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7863 - val_acc: 0.3333\n",
      "Epoch 1881/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7821 - val_acc: 0.3333\n",
      "Epoch 1882/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.7778 - val_acc: 0.3333\n",
      "Epoch 1883/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.7723 - val_acc: 0.3333\n",
      "Epoch 1884/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7679 - val_acc: 0.3333\n",
      "Epoch 1885/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7645 - val_acc: 0.3333\n",
      "Epoch 1886/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 3.7503 - val_acc: 0.3333\n",
      "Epoch 1887/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.7421 - val_acc: 0.3333\n",
      "Epoch 1888/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7340 - val_acc: 0.3333\n",
      "Epoch 1889/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7270 - val_acc: 0.3333\n",
      "Epoch 1890/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.7208 - val_acc: 0.3333\n",
      "Epoch 1891/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7155 - val_acc: 0.3333\n",
      "Epoch 1892/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.7105 - val_acc: 0.3333\n",
      "Epoch 1893/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.7023 - val_acc: 0.3333\n",
      "Epoch 1894/2000\n",
      "18/18 [==============================] - 0s 444us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6967 - val_acc: 0.3333\n",
      "Epoch 1895/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6916 - val_acc: 0.3333\n",
      "Epoch 1896/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6867 - val_acc: 0.3333\n",
      "Epoch 1897/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6823 - val_acc: 0.3333\n",
      "Epoch 1898/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.6783 - val_acc: 0.3333\n",
      "Epoch 1899/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.6888 - val_acc: 0.3333\n",
      "Epoch 1900/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6917 - val_acc: 0.3333\n",
      "Epoch 1901/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6943 - val_acc: 0.3333\n",
      "Epoch 1902/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6967 - val_acc: 0.3333\n",
      "Epoch 1903/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.6970 - val_acc: 0.3333\n",
      "Epoch 1904/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.6982 - val_acc: 0.3333\n",
      "Epoch 1905/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6999 - val_acc: 0.3333\n",
      "Epoch 1906/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.7012 - val_acc: 0.3333\n",
      "Epoch 1907/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.7027 - val_acc: 0.3333\n",
      "Epoch 1908/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 3.6992 - val_acc: 0.3333\n",
      "Epoch 1909/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6987 - val_acc: 0.3333\n",
      "Epoch 1910/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6976 - val_acc: 0.3333\n",
      "Epoch 1911/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.6971 - val_acc: 0.3333\n",
      "Epoch 1912/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.6968 - val_acc: 0.3333\n",
      "Epoch 1913/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.6959 - val_acc: 0.3333\n",
      "Epoch 1914/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.6957 - val_acc: 0.3333\n",
      "Epoch 1915/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.6952 - val_acc: 0.3333\n",
      "Epoch 1916/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.6949 - val_acc: 0.3333\n",
      "Epoch 1917/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.6935 - val_acc: 0.3333\n",
      "Epoch 1918/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6925 - val_acc: 0.3333\n",
      "Epoch 1919/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.6919 - val_acc: 0.3333\n",
      "Epoch 1920/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.6914 - val_acc: 0.3333\n",
      "Epoch 1921/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.6908 - val_acc: 0.3333\n",
      "Epoch 1922/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.7016 - val_acc: 0.3333\n",
      "Epoch 1923/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.7075 - val_acc: 0.3333\n",
      "Epoch 1924/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.7122 - val_acc: 0.3333\n",
      "Epoch 1925/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.7163 - val_acc: 0.3333\n",
      "Epoch 1926/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.7201 - val_acc: 0.3333\n",
      "Epoch 1927/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 3.7853 - val_acc: 0.3333\n",
      "Epoch 1928/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.8136 - val_acc: 0.3333\n",
      "Epoch 1929/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.8382 - val_acc: 0.3333\n",
      "Epoch 1930/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.8597 - val_acc: 0.3333\n",
      "Epoch 1931/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.8745 - val_acc: 0.3333\n",
      "Epoch 1932/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.8896 - val_acc: 0.3333\n",
      "Epoch 1933/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9026 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1934/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9142 - val_acc: 0.3333\n",
      "Epoch 1935/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.9249 - val_acc: 0.3333\n",
      "Epoch 1936/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.9343 - val_acc: 0.3333\n",
      "Epoch 1937/2000\n",
      "18/18 [==============================] - 0s 389us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9427 - val_acc: 0.3333\n",
      "Epoch 1938/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9502 - val_acc: 0.3333\n",
      "Epoch 1939/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.9585 - val_acc: 0.3333\n",
      "Epoch 1940/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.9650 - val_acc: 0.3333\n",
      "Epoch 1941/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9712 - val_acc: 0.3333\n",
      "Epoch 1942/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9765 - val_acc: 0.3333\n",
      "Epoch 1943/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.9814 - val_acc: 0.3333\n",
      "Epoch 1944/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.9855 - val_acc: 0.3333\n",
      "Epoch 1945/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.9894 - val_acc: 0.3333\n",
      "Epoch 1946/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.3333\n",
      "Epoch 1947/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.9939 - val_acc: 0.3333\n",
      "Epoch 1948/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9959 - val_acc: 0.3333\n",
      "Epoch 1949/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.9976 - val_acc: 0.3333\n",
      "Epoch 1950/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.9991 - val_acc: 0.3333\n",
      "Epoch 1951/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 4.0007 - val_acc: 0.3333\n",
      "Epoch 1952/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 4.0014 - val_acc: 0.3333\n",
      "Epoch 1953/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 4.0023 - val_acc: 0.3333\n",
      "Epoch 1954/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 4.0029 - val_acc: 0.3333\n",
      "Epoch 1955/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 4.0036 - val_acc: 0.3333\n",
      "Epoch 1956/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 4.0041 - val_acc: 0.3333\n",
      "Epoch 1957/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 4.0045 - val_acc: 0.3333\n",
      "Epoch 1958/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 4.0048 - val_acc: 0.3333\n",
      "Epoch 1959/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 4.0042 - val_acc: 0.3333\n",
      "Epoch 1960/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 4.0048 - val_acc: 0.3333\n",
      "Epoch 1961/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 4.0052 - val_acc: 0.3333\n",
      "Epoch 1962/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 4.0048 - val_acc: 0.3333\n",
      "Epoch 1963/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 4.0045 - val_acc: 0.3333\n",
      "Epoch 1964/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 4.0046 - val_acc: 0.3333\n",
      "Epoch 1965/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 4.0045 - val_acc: 0.3333\n",
      "Epoch 1966/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 4.0044 - val_acc: 0.3333\n",
      "Epoch 1967/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 4.0007 - val_acc: 0.3333\n",
      "Epoch 1968/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9990 - val_acc: 0.3333\n",
      "Epoch 1969/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.9976 - val_acc: 0.3333\n",
      "Epoch 1970/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.9962 - val_acc: 0.3333\n",
      "Epoch 1971/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.9957 - val_acc: 0.3333\n",
      "Epoch 1972/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.9942 - val_acc: 0.3333\n",
      "Epoch 1973/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.9910 - val_acc: 0.3333\n",
      "Epoch 1974/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.9885 - val_acc: 0.3333\n",
      "Epoch 1975/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.9875 - val_acc: 0.3333\n",
      "Epoch 1976/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.9863 - val_acc: 0.3333\n",
      "Epoch 1977/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.9846 - val_acc: 0.3333\n",
      "Epoch 1978/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.9815 - val_acc: 0.3333\n",
      "Epoch 1979/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9794 - val_acc: 0.3333\n",
      "Epoch 1980/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.9777 - val_acc: 0.3333\n",
      "Epoch 1981/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9762 - val_acc: 0.3333\n",
      "Epoch 1982/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.9768 - val_acc: 0.3333\n",
      "Epoch 1983/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.9754 - val_acc: 0.3333\n",
      "Epoch 1984/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.9742 - val_acc: 0.3333\n",
      "Epoch 1985/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9731 - val_acc: 0.3333\n",
      "Epoch 1986/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.9707 - val_acc: 0.3333\n",
      "Epoch 1987/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.9692 - val_acc: 0.3333\n",
      "Epoch 1988/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.9676 - val_acc: 0.3333\n",
      "Epoch 1989/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.9596 - val_acc: 0.3333\n",
      "Epoch 1990/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 3.9552 - val_acc: 0.3333\n",
      "Epoch 1991/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.9505 - val_acc: 0.3333\n",
      "Epoch 1992/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9463 - val_acc: 0.3333\n",
      "Epoch 1993/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.9386 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1994/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9334 - val_acc: 0.3333\n",
      "Epoch 1995/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.9305 - val_acc: 0.3333\n",
      "Epoch 1996/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.9241 - val_acc: 0.3333\n",
      "Epoch 1997/2000\n",
      "18/18 [==============================] - 0s 222us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9196 - val_acc: 0.3333\n",
      "Epoch 1998/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9156 - val_acc: 0.3333\n",
      "Epoch 1999/2000\n",
      "18/18 [==============================] - 0s 333us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 3.9120 - val_acc: 0.3333\n",
      "Epoch 2000/2000\n",
      "18/18 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.9088 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "#訓練開始 xx為feature Y為label  batch_size為每次放多少進去 epochs為處理幾輪 validation_split為抽多少樣本來驗證 verbose=1為每次顯示\n",
    "train_history=model.fit(xx_train,Y_trainO,batch_size=batch_size,epochs=epochs,validation_split=0.1,verbose=1)\n",
    "# train_history=model.fit(xx,Y,batch_size=batch_size,epochs=epochs,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4wp9f8BWcATLcLsPKsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32a21a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#劃出準確度歷程\n",
    "import matplotlib.pyplot as plt\n",
    "def show_tarin_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title(\"Train History\")\n",
    "    plt.ylabel(train)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(['train','validation'],loc=\"upper left\")\n",
    "    plt.show()\n",
    "show_tarin_history(train_history,'loss','val_loss')\n",
    "# show_tarin_history(train_history,'loss','loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e529320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tarin_history(train_history,'acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 143us/step\n",
      "Train accuracy: 0.9047619104385376\n",
      "9/9 [==============================] - 0s 222us/step\n",
      "Test accuracy: 0.4444444477558136\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(xx_train,Y_trainO,verbose=1)\n",
    "print('Train accuracy:',score[1])\n",
    "score=model.evaluate(xx_test,Y_testO,verbose=1)\n",
    "print('Test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "9/9 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.predict_classes(xx_test)==Y_test).count(True)/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "9/9 [==============================] - 0s 111us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 2, 2, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(xx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分群"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# kmeans分群 (xx>>正規化後的feature)\n",
    "\n",
    "如果直接用X(非正規化feature)則會變成按人口區分(因為人口的值特別高)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 14)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOWkJDkjhzbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e1a2940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2320054132639847, 0.18561702924343415, 0.14187899288327321, 0.15859544785099444, 0.13103977043133663, 0.19134846355240587, 0.18906768925860734, 0.20011745995955454, 0.19809988675585585, 0.201572036881026, 0.16929873353810257, 0.1344278519856217, 0.16252993959039244, 0.1528579040042141, 0.15972056454430333, 0.11419385780065087, 0.10107197794967643, 0.11775874683935249, 0.10501547728637546, 0.09083459462831417, 0.1028587477890448, 0.06927918116157136, 0.06102323898254659]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "# 印出效用最高的kmeans群\n",
    "silhouette_avgs = []\n",
    "ks = range(2, 25)\n",
    "for k in ks:\n",
    "    kmeans_fit = cluster.KMeans(n_clusters = k,max_iter=3000).fit(xx)\n",
    "    cluster_labels = kmeans_fit.labels_\n",
    "    silhouette_avg = metrics.silhouette_score(xx, cluster_labels) #組間變異\n",
    "    silhouette_avgs.append(silhouette_avg)\n",
    "\n",
    "# 作圖並印出 k = 2 到 30 的績效\n",
    "plt.bar(ks, silhouette_avgs)\n",
    "plt.show()\n",
    "print(silhouette_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=2\n",
    "km = KMeans(n_clusters=k,max_iter=3000)  #K=4群\n",
    "y_pred = km.fit_predict(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#利用PCA降維法、將feature印射在二維空間\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_xx = pca.fit_transform(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEyCAYAAABj+rxLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxScikaPiE5HIUfGJSOT8f+89OwMUWhLqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a662be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['costPower', 'human', 'simCostDien', 'busStation', 'conStore', 'star', 'mc', 'ken', 'wa', 'watson', 'pxmart', 'carrefour']\n",
      "第0群資料中心\n",
      "[-0.39, 0.36, 0.56, 0.64, 0.98, 0.92, 0.66, 0.68, 0.72, 0.93, -0.02, 0.01, 0.92]\n",
      "第1群資料中心\n",
      "[0.22, -0.21, -0.32, -0.37, -0.56, -0.53, -0.38, -0.4, -0.41, -0.54, 0.01, -0.01, -0.53]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAF3CAYAAAC/h9zqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a6c5d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "# plt.xlabel('costPower')\n",
    "# plt.ylabel('Nhuman')\n",
    "# plt.scatter(xx[:, 0], xx[:, 1], c=y_pred) #C是第三維度 已顏色做維度\n",
    "plt.scatter(reduced_xx.T[0], reduced_xx.T[1], c=y_pred)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print([i.replace(\"_Analyze\",\"\").replace(\"N\",\"\") for i in ['costPower_Analyze','Nhuman_Analyze',\"NsimCostDien\",\n",
    "        'NbusStation_Analyze','NconStore_Analyze','Nstar_Analyze',\n",
    "        'Nmc_Analyze', 'Nken_Analyze','Nwa_Analyze',\n",
    "        'Nwatson_Analyze','Npxmart_Analyze','Ncarrefour_Analyze']])\n",
    "x=0\n",
    "for i in np.around(km.cluster_centers_,2):\n",
    "    print(\"第{}群資料中心\".format(x),list(i),sep=\"\\n\")\n",
    "    x+=1\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('ADGC')\n",
    "plt.scatter(y_pred, Y, c=y_pred) #C是第三維度 已顏色做維度\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CY=df['costPower_Analyze'].values\n",
    "HY=df['Nhuman_Analyze'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEQCAYAAACk818iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmXZGFvZMJpNpZ/wfDX9lPPauFTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a738908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164, 225, 280, 289, 389, 238, 251, 219, 335, 385, 124]\n",
      "[59.0, 26.0, 73.0, 75.0, 61.0, 63.0, 41.0, 58.0, 69.0, 70.0, 86.0]\n",
      "[18637, 38524, 37878, 12352, 43925, 7974, 36372, 40403, 12000, 24395, 15744]\n",
      "[(164, 59.0, 18637), (225, 26.0, 38524), (280, 73.0, 37878), (289, 75.0, 12352), (389, 61.0, 43925), (238, 63.0, 7974), (251, 41.0, 36372), (219, 58.0, 40403), (335, 69.0, 12000), (385, 70.0, 24395), (124, 86.0, 15744)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEQCAYAAACk818iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2DOZTKbEyII9k8lkSoz/B/uUJOYN6PWeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a7a4550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162, 180, 146, 201, 189, 269, 204, 232, 298, 159, 307, 253, 252, 246, 143, 212, 199, 152, 112]\n",
      "[86.0, 84.0, 96.0, 90.0, 60.0, 89.0, 71.0, 54.0, 66.0, 44.0, 52.0, 95.0, 90.0, 76.0, 41.0, 59.0, 56.0, 89.0, 84.0]\n",
      "[12346, 18019, 19420, 8109, 6496, 14771, 19529, 43388, 29217, 26741, 37422, 29884, 9021, 9606, 6767, 34640, 6956, 7533, 22658]\n",
      "[(162, 86.0, 12346), (180, 84.0, 18019), (146, 96.0, 19420), (201, 90.0, 8109), (189, 60.0, 6496), (269, 89.0, 14771), (204, 71.0, 19529), (232, 54.0, 43388), (298, 66.0, 29217), (159, 44.0, 26741), (307, 52.0, 37422), (253, 95.0, 29884), (252, 90.0, 9021), (246, 76.0, 9606), (143, 41.0, 6767), (212, 59.0, 34640), (199, 56.0, 6956), (152, 89.0, 7533), (112, 84.0, 22658)]\n"
     ]
    }
   ],
   "source": [
    "xlabels=[i.replace(\"_Analyze\",\"\").replace(\"N\",\"\") for i in ['costPower_Analyze','Nhuman_Analyze',\"NsimCostDien\",\n",
    "        'NbusStation_Analyze','NconStore_Analyze','Nstar_Analyze',\n",
    "        'Nmc_Analyze', 'Nken_Analyze','Nwa_Analyze',\n",
    "        'Nwatson_Analyze','Npxmart_Analyze','Ncarrefour_Analyze']]\n",
    "style = ['o-r', 'o-b', 'o-g','o-y','o-y','o-y','o-y','o-y']\n",
    "for i in range(k):\n",
    "    plt.figure()\n",
    "    aa=[]\n",
    "    bb=[]\n",
    "    cc=[]\n",
    "    abc=[]\n",
    "    for j,x,a,b,c in zip(y_pred,xx,Y,CY,HY):\n",
    "        if j==i:\n",
    "            plt.plot(range(1, len(xx[0])+1), x, style[i],)\n",
    "            plt.xticks(range(1, len(xx[0])+1), xlabels, rotation = 20,fontproperties='SimHei') #坐标标签\n",
    "            plt.title(u'石二鍋類%s' %(i),fontproperties='SimHei') #我们计数习惯从1开始\n",
    "            plt.subplots_adjust(bottom=0.15) #调整底部\n",
    "            aa.append(a)\n",
    "            bb.append(b)\n",
    "            cc.append(c)\n",
    "            abc.append((a,b,c))\n",
    "    plt.show()\n",
    "    print(aa)\n",
    "    print(bb)\n",
    "    print(cc)\n",
    "    print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([304, 362, 299, 377, 329, 349, 289, 302, 362, 251, 292])\n",
    "len([91621, 286210, 34601, 164362, 179901, 116244, 100319, 154917, 111510, 83174, 67165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAECCAYAAADuGCyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEgroIiKRUEAXEYmEArqISCQU0EVEIvH/BzFRitrtdPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a986160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "#這裡使用scipy的層次聚類函數\n",
    "Z = linkage(xx, method = 'ward', metric = 'euclidean') #譜系聚類圖\n",
    "P = dendrogram(Z, 0) #畫譜系聚類圖\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='ward', memory=None, n_clusters=2,\n",
       "            pooling_func=<function mean at 0x0000000005920AE8>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering #導入sklearn的層次聚類函數\n",
    "model = AgglomerativeClustering(n_clusters = k, linkage = 'ward')\n",
    "model.fit(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAEyCAYAAABj+rxLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFZ+IRI6KT0Qi5/8AjZxGnCv2k1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aa5fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAF3CAYAAAC/h9zqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYHGW5/vHv0z3Ts2QhBAYICRCW\nECFBkjCEfUsQQmRVUEBW0ZyfAgcBPYLKAdSoiIoHQQQOyCIKCIo5iCxCWAXCBMKWEDKQAIEAgYQs\ns/Qy/fz+6AoMmcpkElPdNan7c119Tfdb1fU+ZXDuqbeq3jJ3R0REZGWpShcgIiLxpIAQEZFQCggR\nEQmlgBARkVAKCBERCaWAEBGRUAoIEREJpYAQEZFQCggREQkVeUCYWdrMnjOzu4PPW5vZ02Y2x8xu\nM7NM0F4TfG4Olg+NujYREVm1qjL0cRYwC+gffL4EuMzdbzWz3wGnAVcFPxe7+3Zmdmyw3pe72/DG\nG2/sQ4cOjaxwEZH10fTp0z9w94bVrWdRzsVkZkOAG4HJwDnAYcBCYDN3L5jZHsBF7n6wmd0XvH/S\nzKqAd4EG76bAxsZGb2pqiqx+EZH1kZlNd/fG1a0X9RDTr4H/AorB542Aj9y9EHyeDwwO3g8G3gII\nli8J1hcRkQqILCDM7FDgfXef3rk5ZFXvwbLO251kZk1m1rRw4cJ1UKmIiISJ8ghiL+BwM5sH3AqM\no3REMSAYQgIYArwTvJ8PbAEQLN8AWLTyRt39GndvdPfGhobVDqGJiMhaiiwg3P18dx/i7kOBY4GH\n3P0rwFTg6GC1k4G/Be+nBJ8Jlj/U3fkHERGJViXug/gucI6ZNVM6x3Bd0H4dsFHQfg5wXgVqExGR\nQDkuc8XdHwYeDt6/DowNWacdOKYc9YiIyOrpTmoREQmlgBARkVCJDwh3p1gsrn5FEZGESWxALFu8\nnJ+ecDkT645nQuZYvjP+IubPWVDpskREYiORAeHufGf8xTx6x5MUcgW86Dz/8Ez+c4/vsWzx8kqX\nJyISC4kMiJcef4W3m9+lkCt83Obu5NpzPHDTIxWsTEQkPhIZEG/NfgeKXe/By7bmeP35eeUvSEQk\nhspyH0TcDB25RejMTzX1NQzbZZvyFyQisgruDtn78dY/gbdD7aFY/TGY1UTedyIDYofdhrH1TlvS\n/Nw88tk8AKmUUde3lgNP3K/C1YmIfMKX/gja7wRvKzXkZ+Htf4OBf8SsOtK+EznEZGZccv8FHHLa\nOOr71ZGprWb3wxq5ctpP6dO/vtLliYgA4IU3oe3Pn4QDAG1QmAPZByPvP5FHEAB1fes484qvceYV\nX6t0KSIi4XJNYOmuDz7wVjz7CFY7IdLuE3kEISLSK6QGEP6onCpIRf+4AwWEiEhc1ewNhJ2MrsLq\njg5pX7cUECIiMWWWwQbeBKnNwerB+oL1wwZchlVtGXn/iT0HISLSG1j19tAwFQqzwLNQPQKzTFn6\nVkCIiMScmUH1jmXvV0NMIiISSgEhIiKhFBAiIhJKASEiIqEUECIiEkoBISIioRQQIiISSgEhIiKh\nFBAiIhJKASEiIqEUECIiEkoBISIioRQQIiISSgEhIiKhFBAiIhJKASEiIqEUECIiEkoBISIioRQQ\nIiISSgEhIiKhFBAiIhJKASEiIqEUECIiEkoBISIioRQQIiISSgEhIiKhFBAiIhJKASEiIqEiCwgz\nqzWzaWb2vJm9bGYXB+03mNlcM5sRvEYF7WZml5tZs5m9YGZjoqpNRERWryrCbWeBce6+3MyqgcfN\n7B/Bsu+4+x0rrX8IMCx47QZcFfwUEZEKiOwIwkuWBx+rg5d385UjgJuC7z0FDDCzQVHVJyIi3Yv0\nHISZpc1sBvA+8IC7Px0smhwMI11mZjVB22DgrU5fnx+0iYhIBUQaEO7e4e6jgCHAWDMbCZwPfAbY\nFRgIfDdY3cI2sXKDmU0ysyYza1q4cGFElYuISFmuYnL3j4CHgQnuviAYRsoCvwfGBqvNB7bo9LUh\nwDsh27rG3RvdvbGhoSHiykVEkivKq5gazGxA8L4OOBB4ZcV5BTMz4EjgpeArU4CTgquZdgeWuPuC\nqOoTEZHuRXkV0yDgRjNLUwqi2939bjN7yMwaKA0pzQD+X7D+PcBEoBloBU6NsDYREVmNyALC3V8A\nRoe0j1vF+g6cHlU9IiKyZnQntYiIhFJAiIhIKAWEiIiEUkCIiEgoBYSIiIRSQIiISCgFhIiIhIry\nRjkREVkH3B06XgPPQdX2mJXnV7cCQkQkxrzwGr74G9DxHlgKqIYBv8Bq9o28bw0xiYjElHseX3Qi\ndLwBtIG3gH+ELz4DL8yPvH8FhIhIXGUfB2+j65MPOvC2lR/Kue4pIERE4qq4CLwYsiAPxXcj714B\nISISV5ldgLCAqMcye0fevQJCRCSmrGoo1B0O1HVqrYWqoVB7UOT96yomEZEYs/4/hszueOsfwduh\n9lCsz/GYZSLvWwEhIhJjZgZ1h2F1h5W9bw0xiYhIKAWEiIiEUkCIiEgoBYSIiIRSQIiISCgFhIiI\nhFJAiIhIKAWEiIiEUkCIiEgoBYSIiIRSQIiISCgFhIiIhFJAiIhIKAWEiIiEUkCIiEgoBYSIiIRS\nQIiISCgFhIiIhFJAiIhIKAWEiIiEUkCIiEioRAeEu9P83FxmPjmbfC5f6XJERGKlqtIFVMrcF9/g\nB4f9jGWLlmMpw8z47k1nssdhjZUuTUQkFhJ5BJHP5fn2+It5/80PaFveTuvSNlqWtDL5uMtYMPe9\nSpcnIhILiQyIZ+6dQSFb6NLeUejg3usfqkBFIiLxk8iAWPrBMorFYpf2Qq6DRe9+VIGKRETiJ5EB\n8dn9dqTY0TUg6vrWMnbC6ApUJCISP4kMiM233YyDTz2A2j41H7fV1GcYOnJL9jxi1wpWJiISH4m9\niunMK77G6HE78X9X30+2Nce44/fmkNPGk65KV7o0EZFYiCwgzKwWeBSoCfq5w90vNLOtgVuBgcCz\nwInunjOzGuAmYBfgQ+DL7j4vwvrY54u7s88Xd4+qCxGRXi3KIaYsMM7ddwZGARPMbHfgEuAydx8G\nLAZOC9Y/DVjs7tsBlwXriYhIhUQWEF6yPPhYHbwcGAfcEbTfCBwZvD8i+EywfLyZWVT1Acx+ppnL\nT7+WS0+9kqf/Pj30yiYRkaSK9ByEmaWB6cB2wJXAa8BH7r7iJoT5wODg/WDgLQB3L5jZEmAj4IMo\narvt53dx88V/JpfN40Xn0TueZNcJo7ng9nOIOJdERHqFSK9icvcOdx8FDAHGAjuErRb8DPut7Cs3\nmNkkM2sys6aFCxeuVV0fvLOImy66nWxbDi+WumhvyfLMvc8x/YEX1mqbIiLrm7Jc5uruHwEPA7sD\nA8xsxZHLEOCd4P18YAuAYPkGwKKQbV3j7o3u3tjQ0LBW9Uy//3nSVV13vb0ly+N/eWqttikisr6J\nLCDMrMHMBgTv64ADgVnAVODoYLWTgb8F76cEnwmWP+TuXY4g1oXaPrUQMoyUSqeo61cXRZciIr1O\nlOcgBgE3BuchUsDt7n63mc0EbjWzHwPPAdcF618H3GxmzZSOHI6NqrCxE0eHDF5BdaaKg07eP6pu\nRUR6lcgCwt1fALrMW+Hur1M6H7FyeztwTFT1dFbXp5YfTTmPC464BAMcp5DvYNIvTmLrkVuWowQR\nkdhL7J3UO+8/gj+/ey1N9z1Prj3PmAN3YoON+1e6LBGR2EhsQADU1NWw15FdDmZERISETtYnIiKr\np4AQEYkxLy6nuOQCiu/tTPHdERQXTcILb5WlbwWEiEhMuTu++FRo+yt4G5CH3KP4h0fjxWWR96+A\nEBGJq/zzUJgD5Do1FsHb8ba/RN69AkJEJK4KzRB6v3AbFGZG3r0CQkQkrqq2CZ31AWqhKmxqu3VL\nASEiElfVoyG9DZDp1JgCq8Xqjoq8ewWEiEhMmRk28EaoO5RSSKQgswe20R1YaoPI+0/0jXIiInFn\nqX7YBj/D+/+09LmMz6tRQIiI9AKVeJBZogMil83z3D9fINeeZ9S4kfTbsG+lSxIRiY3EBsSLj83i\ngsN/xopHThRyBU6//KtM/NqBFa5MRCQeEnmSur01y/cP/SktS1ppXdpG69I2cu15fnvW73ljZnlu\nYRcRibtEBsS0e54NfQB2IV/gvhumlr0eEZE4SmRAtC1vp1jsendiR6FI69K2ClQkIhI/iQyIMQd+\nlmJHR5f22j56PoSIyAqJDIiGIRtx/Pe+QE19zceXjtX2qWH0+J3Y5aCdK1ydiEg8JPYqpq/84GhG\njduJe3//EO0tWfb/0p7scXgjqVQiM1NEpIvEBgTAiD2HM2LP4ZUuQ0QklvTnsoiIhFJAiIhIKAWE\niIiESvQ5CBGR3sDzc/D2u6DYhtUeDJmxZZm8TwEhIhJjxZY/wLKfA3mgiLffCTUHwQY/jzwkNMQk\nIhJT3vEhLLsEaAc6AAdvg+z9kPtX5P0rIERE4ir3GJDu2u5tePu9kXevgBARia0MhA4jlZ5LHTUF\nhIhIXNXsCxRDFmSwuiMj714BISISU5bqiw24AqwOrA9YPVADff8Tqx4Ref+6iklEJMasZh9oeAKy\nU8HboWYfLL1ZWfpWQIiIxJyl+kLdYWXvV0NMIiISSgEhIiKhVhkQZnaCmZ0Y0v51Mzs+2rJERKTS\nujuCOBe4K6T91mCZiIisx7oLiLS7L1u5MWirjq4kERGJg+4CotrM+qzcaGb9gEx0JYmISBx0FxDX\nAXeY2dAVDcH7W4NlIiKyHlvlfRDu/gszWw48YmZ9g+blwM/c/aqyVCciIhXT7Y1y7v474HdBQFjY\nOYnebMkHS3nirmfItefYbeIYBm2zaaVLEhGJjVUGhJkNAYa6++PuvtzMzul0JPFHd28uT4nReOKu\nafzk+F/j7hSLzjXfuYnjzv8CJ/73MZUuTUQkFro7B3EpMKDT5/8AWgAHLo6yqKi1LGlh8nGXkWvP\nk88W6Mh3kM8WuGXyHcxueq3S5YmIxEJ3ATHc3e/u9LnV3X/p7j8Ctlzdhs1sCzObamazzOxlMzsr\naL/IzN42sxnBa2Kn75xvZs1mNtvMDl7rvVqNx/4yjXy20KW9I1/kL7++O+QbIiLJ0905iJWfRjG+\n0/uNerDtAnCuuz8bXBo73cweCJZd5u6/6Lyyme0IHAuMADYH/mlm27t7Rw/6WiOvPP3qKpe9+uzc\ndd2diEiv1N0RxDIz237FB3dfBGBmn6F0NVO33H2Buz8bvF8GzAIGd/OVI4Bb3T3r7nOBZmDs6ndh\nzXV0rDpzivmuRxYiIknUXUBcCNxtZieb2U7B6xRgSrCsx4L7J0YDTwdNZ5jZC2Z2vZltGLQNBt7q\n9LX5dB8oa23oiFWPkA0eNiiKLkVEep1VBoS73wt8gdLQ0g3BaxzwBXf/R087CK58uhP4lrsvBa4C\ntgVGAQuAX65YNayMkO1NMrMmM2tauHBhT8v4lL2P2o10VdddT1WlOOwbkZ36EBHpVbqd7tvdX3L3\nk9x9l+B1kru/1NONm1k1pXC4xd3/EmzzPXfvcPcicC2fDCPNB7bo9PUhwDshNV3j7o3u3tjQ0NDT\nUj5l060aOOqsz5NKf7L7ljJ23GM4YyeOXqttioisb7oNiGB4abqZtQSvJjM7qScbNjOjNCXHLHf/\nVaf2zmM4RwErAmcKcKyZ1ZjZ1sAwYNqa7ExPuTuznnoVS3U6aHGYP/ttsq25KLoUEel1urtR7iTg\nW8A5wLOUhoDGAJeaGe5+02q2vRdwIvCimc0I2r4HHGdmoygNH82jdH8F7v6ymd0OzKR0BdTpUVzB\nBPDKtGZemzGPjvwnm3d32luy/PPmRzn8mxpmEhHp7jLXbwJHufu8Tm0PmdkXKU3Y121AuPvjhJ9X\nuKeb70wGJne33XVh7otvhra3t2R5takZUECIiHQ3xNR/pXAAIGjrH1VB5TBk+0GURsA+raY+w9Y7\nrfYeQBGRROguINrWclns7bTPDgzaZlOqMp8cQJkZ1TXVHHTKARWsTEQkProLiB2CexVWfr0IfKZc\nBUbBzLj0oQvZ+6ixpKvTpNIpRu7zGS7/12T6bdh39RsQEUmA7s5B7BDSZpQuP/1eNOWUT/+B/fj+\nn86mWCziRSddla50SSIisdLdA4PeWPE+uOroeOBLwFxK9zasF1Kp1Gou9hURqSwvtkD2ESALmb2x\n9NrdA7amurvMdXtKk+cdB3wI3EbpoUEapBcRKRPPPoF/dDqlARwHL+D9ziXV59TI++7ub+dXKE2z\ncZi77+3uvwEiuS9BRES68mJLKRy8Fbyl9JMcLLsMz8+MvP/uAuKLwLvAVDO71szGE35fg4iIRCH3\nKOG/dnN4212Rd9/dZH1/dfcvU7pi6WHgbGBTM7vKzA6KvDIRkaTzLCFzlgJF8OjvNljt6Vl3b3H3\nW9z9UEpXMM0Azou8MhGRpMvsDaEzDtVjtRMi736Nrt9x90XufrW7j4uqIBERKbH0xtDvXEoP+Fzx\n67oOag+AzJ6R99/dfRDrtdZlbVz7Xzfz4B8fo5DroPHgnTn9f77KpluV5/IxEZGeSPU5Bc/sVjrn\n4G1Y7cGQ2TN0uqB1LZEB4e6cd/CPaH5uLvls6RGjT989nVlPzeGGVy+nT//6ClcoIvIJq94Bqw67\ndzlaibxFbNZTrzL3xTc/DgeAYtFpX97OP29+pIKViYjERyIDYt7L80MvDGhvzTJn+uvlL0hEJIYS\nGRBbDN/800+TC9TUZ9hm56HlL0hEJIYSGRAj9/4Mg4cN6jLdd6Yuw0En71+5wkREYiSRAWFmXPrg\nhez3pT2pylRhKWPnA0Zw+b9+Qt8BfSpdnohILCTyKiaAvgP68PVLTmD4rtuSb8uz22G7MGTYoEqX\nJSISG4kNiEdu/xc/P/VKAIodRW68+Ha+cNZETvvJVypcmYhIPCRyiGnZ4uVceuqV5Npy5NpyFHIF\ncm05/nr5P5j19JxKlyciEguJDIhp9zxHKt1113PtOR685dEKVCQiEj+JDAj3sNkRKT2LY1XLREQS\nJpEBMXbiaDoKXWdIzNRlGHfcPhWoSEQkfhIZEP0H9uPsa/6DTF2Gquo0ljJq6jIcOulARuw5vNLl\niYjEQiIDAqDx4FHstM8OFIsODoO23ZQJp42vdFkiIrGRyIBwd759wEXMmPoSxY4i7s4bL7/F2ftc\nwNIPl1W6PBGRWEhkQLzwyEzef/MDOvKfnIdwh3w2z72/n1rBykRE4iORAfF287uhVytl23K8OfOt\nClQkIhI/iQyIbXfeKrS9tk8Nn9lt+zJXIyIST4kMiOG7bsfwXbcjU1v9cVu6KkWfDeoZ/5W9K1iZ\niEh8JDIgACb//XyOPHMiGzT0p75/Pft9eS+ufOYS6vrWVbo0EZFYSOxkfTV1NRx08n6kq1Jk23Ls\ndeRYBm42oNJliYjERmIDYspv7+Xq79xMR75AsaPIPdf+k/2O2YNzr/smZl2fNicikjSJHGJa/P4S\nrv72TeTacnQUirhDe0uWR/78JM8//HKlyxMRiYVEBkTTvTNIV6W7tGdbSyEhIiIJDYiqTBWEjCKZ\n2aeubBIRSbJEBsTYiaMpdnS9Ua66pprPnbhfBSoSEYmfRAZEn/71XHD7OdTU11DXt5aa+gyZ2mpO\nvPAYthu9daXLExGJhcRexbTbxDHc9vbV/GtKE/n2PLseMpqGIRtVuiwRkdhIbEAA9NmgDweesC+A\nLm0VEVlJIoeYAFqWtHDpqVfy+fqvMCFzLOcd/CPeee3dSpclIhIbiQwId+c7B/6QqX96nHw2T7Gj\nyLMPvsiZe3yP5R+1VLo8EZFYSGRAvPyv2bw1+x3yucLHbV50sq1ZHrjp4coVJiISI4k8B/HmrLeh\nGPI8iNYcr82YV/6CRERWwb0IbX/FW/8I3g51n8fqT8ZSfSLvO7IjCDPbwsymmtksM3vZzM4K2gea\n2QNmNif4uWHQbmZ2uZk1m9kLZjYmqtq22nEIhJyUrq2vYbsx20TVrYjIGvMl5+NLfwiFF6FjDiy/\nCl90LO65yPuOcoipAJzr7jsAuwOnm9mOwHnAg+4+DHgw+AxwCDAseE0CroqqsB332J6tdhxCuvqT\n6TbMIFOf4XMn6UY5EYkHL7wO7fcAbZ1as1B4C9rvi7z/yALC3Re4+7PB+2XALGAwcARwY7DajcCR\nwfsjgJu85ClggJkNiqI2M2P0+JGlB1GvaEul2OazW1HXtzaKLkVE1lzuOcJ/TbfiuSci774sJ6nN\nbCgwGnga2NTdF0ApRIBNgtUGA50fCD0/aFvnFs7/kDt//Xc6CsWP24odRV6Z1kzTfc9H0aWIyJpL\nbwwW9ms6A6lI/n7+lMgDwsz6AncC33L3pd2tGtLW5UyymU0ysyYza1q4cOFa1TT9gRdIp7vuevvy\ndp646+m12qaIyDqX2Qusnq6/HlNY/TGRdx9pQJhZNaVwuMXd/xI0v7di6Cj4+X7QPh/YotPXhwDv\nrLxNd7/G3RvdvbGhoWGt6qrvV4uluu56uipF3wHRXxkgItITZlXYwFsgvR1QWwqL1MbYhr/D0ptH\n3n9kl7laae6K64BZ7v6rToumACcDPwt+/q1T+xlmdiuwG7BkxVDUujZ24piwi5hIV1dx0CkHRNGl\niMhasaqhWMPf8cKbpctcq7bDQoed1r0oe9kLOBEYZ2YzgtdESsHwOTObA3wu+AxwD/A60AxcC3wz\nqsJq62v4yT3fp++APtT3r6O+fx2ZugxnXnEaW+0wJKpuRUTWmlVtiVVvX7ZwADD3rjeM9RaNjY3e\n1NS01t/P5/LMmPoyubYcow4YQZ8NNLwkIus/M5vu7o2rWy+Rd1KvUJ2pZteDR1W6DBGRWErkXEwi\nIrJ6CggREQmV2IBob81y1dk3cNRGp3BYvxP48bGX8cHbH1a6LBGR2EjkOQh35/wJP2b2M6+Rz+YB\neOzOp3jx0Zn8fvbl1Perq3CFIiKVl8gjiFemNdP83NyPwwFKU220LmvjwT88WsHKRETiI5EBMffF\nN0Pb21uyzH6muczViIjEUyIDYvCwzbCQW6lr6jIMHbllBSoSEYmfRAbEZ/fdkc223oSqlZ4HUV1b\nzUGn7F+5wkREYiSRAWFm/GLqRex55FiqqtOk0ilG7PkZ/ueJyfQf2K/S5YmIxEIir2IC6D+wHxfc\ndg4dhQ6KxSLVmepKlyQiEiuJDYgV0lVp0qRXv6KISMIkOiA6Ch28+Ngssm05PrvvDtT11f0PIiIr\nJDYgZj09hx8c+lMK+QIAHYUi37p6Egd+Zd8KVyYiEg+JPEmda89x/oQfs/TDZbQubaN1aRvZ1iy/\nnnQ181/t8hA7EZFESmRAPH3PcxSLXZ+DUch3cO/1D1WgIhGR+ElkQLQsacWLxS7tHYUOli5aXoGK\nRETiJ5EBMWb8SIodXQOitm8tex6+awUqEhGJn0QGxCZbNvDFsw+ltk/Nx221fWoYsedwxk4cXcHK\nRETiI7FXMX118vGMGrcT//jff9LemuWAY/dmv2P2IJVKZGaKiHSR2IAAGDN+J8aM36nSZYiIxJL+\nXBYRkVAKCBERCaWAEBGRUAoIEREJpYAQEZFQCggREQmlgBARkVAKCBERCZX4gFgw9z3emDWfYsjk\nfSIiSZbYO6nfbl7ARV+4lHdee49UyqjvX8f5fziLUQeMrHRpIiKxkMgjiI5CB+fufyFvvDyfXFuO\n9pYsixZ8xAWH/4yF8z+sdHkiIrGQyIBouv95Wpe24f7phwYVch3ce/2DFapKRCReEhkQH76zmPaW\nbJf2Qr7A/FcXVKAiEZH4SWRAVGXSXY4eRETk0xIZEIVcB5ay0GXpdCL/JxER6SKRvw23+eyWhMaD\nwbBdtil3OSIisZTIgMjUZVjVAFNNXc0qloiIJEsiA2L2tGYytZmuCxxmPvVq+QsSEYmhRAbExkM2\nIpXuOshUXVPNoG02qUBFIiLxk8iAGHPgTvTbsC+plU5UV1WnmfDV8RWqSkQkXhIZEOl0ml898kOG\nNW5LdU01NXUZNh3awE/+8X02GrRhpcsTEYmFxM7FtOlWDVzx1E/5cMFi8tk8m27VgFn4pa8iIpXk\nnoPck+BZyOyOpfqXpd/EBsQKOmIQkTjz3HR88SRYce2l5/H+PyBV/+XI+45siMnMrjez983spU5t\nF5nZ22Y2I3hN7LTsfDNrNrPZZnZwVHWJiPQW7u344q+DLwNfXnqRhaWT8fycyPuP8hzEDcCEkPbL\n3H1U8LoHwMx2BI4FRgTf+a2ZpSOsTUQk/rKPQOhdW3m87S+Rdx9ZQLj7o8CiHq5+BHCru2fdfS7Q\nDIyNqjYRkV7BWwgPiA7wpZF3X4mrmM4wsxeCIagVJwAGA291Wmd+0CYiklyZPcE7urZbPVb7uci7\nL3dAXAVsC4wCFgC/DNrDLh8KnQ3DzCaZWZOZNS1cuDCaKkVEYsDSm0HfbwC1fPJrsh4yYyGzb+T9\nl/UqJnd/b8V7M7sWuDv4OB/YotOqQ4B3VrGNa4BrABobG9d6zu5sW5abL/4z993wMIVcgd0P24Wv\nX3ICAzfTVU0iEh+pvt/EM7vhbXeAt2G1h0DNgZhF//d9WQPCzAa5+4on8hwFrLjCaQrwRzP7FbA5\nMAyYFmUt3z/0p8x68lVy7XkApv7pCWZMfYnrZ/0PdX1qo+xaRGSNWGYXLLNL2fuN8jLXPwFPAsPN\nbL6ZnQb83MxeNLMXgAOAswHc/WXgdmAmcC9wunvYwNu6MbvpNWZPa/44HKD0nOrli1t48A+PRdWt\niEivEtkRhLsfF9J8XTfrTwYmR1VPZ6/NmBf6RLn2liyvTJvDof8R/ckfEZG4S+RcTJtvuymFfNcD\nFEsZQ4ZvXoGKRETiJ5EBscmWG9NR6BoQXnRNvSEiEkhkQLzwyMxVPjnuxUdnlrkaEZF4SmRA9BvY\nl3S6665XVacZsOmAClQkIhI/iQyIXQ8ZTbq661RP6ao0E049oAIViYjETyIDIlNTzc8f+G8GDhpA\nXb9a6vvXUdevlu/edCabb7tZpcsTEYmFxD4PYrvRW/Ont67mlWnN5Nvz7LD7MDK1mUqXJSISG4kN\nCIBUKsWOu29f6TJERGIpkUMkzVGzAAAI60lEQVRMIiKyegoIEREJpYAQEZFQCggREQmlgBARkVAK\nCBERCaWAEBGRUAoIEREJpYAQEZFQCggREQmlgBARkVCJnotJRKQ3cG+D7GPgOajZE0sNLEu/CggR\nkRjz7FP4R98IPgDk8X7fJdXnxMj71hCTiEhMebG1FA7eUnrRAuRg2aV4/pXI+1dAiIjEVe7RVS3A\n2/4aefcKCBGRuPI2gnGllRSDI4poKSBEROIqsxd4oWu71WO1B0XevQJCRCSmLL0J9D0bqOXjX9dW\nB5l9Sq+I6SomEZEYS/U9Da/ZDW+9E2jFag+BzL6YWeR9KyBERGLOqkdiG4wse78aYhIRkVAKCBER\nCaWAEBGRUAoIEREJleiT1Atef4+ptz1Bri3HHofvyvDGbStdkohIbCQ2IO79/UP85ozrKBY66Ogo\ncsev/o+DTzmAM35zWlkuHxMRibtEDjEt+WApvzn9f8m15SjkO/Cik23Ncf+ND/PS49FPgCUi0hsk\nMiCm/eM50lXpLu3Z1ixTb3uiAhWJiMRPIgMinU5B2CiSGVUhwSEikkSJDIixE8dQLBS7tGdqqxn/\nlejnNxER6Q0SGRB9B/ThvD/8J5m6DDX1NWRqq8nUVnPMt49g+K7bVbo8EZFYSOxVTHsftRu3zPst\nT/x1Gtm2HLsfugubb7tZpcsSEYmNxAYEwICGDfj8pM9VugwRkVhK5BCTiIisngJCRERCKSBERCSU\nAkJEREJFFhBmdr2ZvW9mL3VqG2hmD5jZnODnhkG7mdnlZtZsZi+Y2Zio6hIRkZ6J8gjiBmDCSm3n\nAQ+6+zDgweAzwCHAsOA1CbgqwrpERKQHIgsId38UWLRS8xHAjcH7G4EjO7Xf5CVPAQPMbFBUtYmI\nyOqV+xzEpu6+ACD4uUnQPhh4q9N684M2ERGpkLicpA6bOs9DVzSbZGZNZta0cOHCiMsSEUmucgfE\neyuGjoKf7wft84EtOq03BHgnbAPufo27N7p7Y0NDQ6TFiogkWbmn2pgCnAz8LPj5t07tZ5jZrcBu\nwJIVQ1HdmT59+gdm9sY6qGtj4IN1sJ3eQvu7fkvS/iZpX2Hd7e9WPVnJ3ENHcv5tZvYnYH9KO/Qe\ncCFwF3A7sCXwJnCMuy+y0jM+r6B01VMrcKq7N0VSWHitTe7eWK7+Kk37u35L0v4maV+h/Psb2RGE\nux+3ikXjQ9Z14PSoahERkTUXl5PUIiISMwqIkmsqXUCZaX/Xb0na3yTtK5R5fyM7ByEiIr2bjiBE\nRCRUogLCzCaY2exgUsDzQpbXmNltwfKnzWxo+atcd3qwv+eY2cxggsQHzaxHl77F0er2tdN6R5uZ\nm1mvvvKlJ/trZl8K/n1fNrM/lrvGdakH/y1vaWZTzey54L/niZWoc10Im+h0peXlm9zU3RPxAtLA\na8A2QAZ4HthxpXW+CfwueH8scFul6454fw8A6oP33+it+9uTfQ3W6wc8CjwFNFa67oj/bYcBzwEb\nBp83qXTdEe/vNcA3gvc7AvMqXfe/sb/7AmOAl1axfCLwD0ozUOwOPB1VLUk6ghgLNLv76+6eA26l\nNElgZ50nE7wDGB/co9EbrXZ/3X2qu7cGH5+idAd7b9STf1uAHwE/B9rLWVwEerK/XweudPfFAO7+\nPr1XT/bXgf7B+w1YxUwMvYGHT3TaWdkmN01SQPRkQsCP13H3ArAE2Kgs1a17azoB4mmU/irpjVa7\nr2Y2GtjC3e8uZ2ER6cm/7fbA9mb2hJk9ZWYrT73fm/Rkfy8CTjCz+cA9wJnlKa0iyja5abmn2qik\nnkwI2ONJA3uBNZkA8QSgEdgv0oqi0+2+mlkKuAw4pVwFRawn/7ZVlIaZ9qd0ZPiYmY10948iri0K\nPdnf44Ab3P2XZrYHcHOwv8Xoyyu7sv2eStIRRE8mBPx4HTOronSo2t2hXpz1aAJEMzsQ+D5wuLtn\ny1Tbura6fe0HjAQeNrN5lMZtp/TiE9U9/W/5b+6ed/e5wGxKgdEb9WR/T6M0jQ/u/iRQS2man/VR\njyc3/XclKSCeAYaZ2dZmlqF0EnrKSuusmEwQ4GjgIQ/OCvVCq93fYNjlakrh0JvHqLvdV3df4u4b\nu/tQdx9K6XzL4V7G+b7WsZ78t3wXpYsQMLONKQ05vV7WKtednuzvmwTT+JjZDpQCYn19HsAU4KTg\naqbd6eHkpmsjMUNM7l4wszOA+yhdFXG9u79sZj8Emtx9CnAdpUPTZkpHDsdWruJ/Tw/391KgL/Dn\n4Fz8m+5+eMWKXks93Nf1Rg/39z7gIDObCXQA33H3DytX9drr4f6eC1xrZmdTGm45pbf+cdd5otPg\nnMqFQDWAu/+O0jmWiUAzweSmkdXSS/83FBGRiCVpiElERNaAAkJEREIpIEREJJQCQkREQikgREQk\nlAJCJISZbWZmt5rZa8GMqPeY2farmmGzB9s7xcw2X9d1ikRJASGykmCCxr8CD7v7tu6+I/A9YNN/\nY7OnAGsUEMHd/CIVo4AQ6eoAIB/clASAu8+g0wRpwRHBFZ0+321m+5tZ2sxuMLOXzOxFMzvbzI6m\nNNfVLWY2w8zqzGwXM3vEzKab2X0rZuM0s4fN7Cdm9ghwVtn2WCSE/kIR6WokMH0tvzsKGOzuIwHM\nbIC7fxTcCfxtd28ys2rgN8AR7r7QzL4MTAa+GmxjgLv31okTZT2igBBZt14HtjGz3wB/B+4PWWc4\npRB6IJjiJA10nkvntqiLFOkJBYRIVy9TmqyxOwU+PURbC+Dui81sZ+Bg4HTgS3xyZLCCAS+7+x6r\n2HbLGlcsEgGdgxDp6iGgxsy+vqLBzHYFOj+zex4wysxSZrYFpaeerZg5NeXudwIXUHp0JMAyStOO\nQ2nq7YbguQWYWbWZjYhwf0TWio4gRFbi7m5mRwG/NrPzKD2idB7wrU6rPQHMBV4EXgKeDdoHA78P\nHlIEcH7w8wbgd2bWBuxB6QjlcjPbgNL/D39N6chFJDY0m6uIiITSEJOIiIRSQIiISCgFhIiIhFJA\niIhIKAWEiIiEUkCIiEgoBYSIiIRSQIiISKj/D2/OMdVBEe2rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aa143c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "# plt.xlabel('costPower')\n",
    "# plt.ylabel('Nhuman')\n",
    "# plt.scatter(xx[:, 0], xx[:, 1], c=y_pred) #C是第三維度 已顏色做維度\n",
    "plt.scatter(reduced_xx.T[0], reduced_xx.T[1], c=model.labels_)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('ADGC')\n",
    "plt.scatter(model.labels_, Y, c=model.labels_) #C是第三維度 已顏色做維度\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

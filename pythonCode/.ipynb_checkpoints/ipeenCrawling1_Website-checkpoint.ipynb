{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  第一版-待將set改成id 不抓取搬遷 抓取關店"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists('./data'):\n",
    "#     os.makedirs('./data')\n",
    "# URL=\"http://www.ipeen.com.tw/search/taiwan/000/1-100-0-0/?baragain=1\"\n",
    "\n",
    "# res=requests.get(URL+\"&p=1\")\n",
    "# soup=bs(res.text,\"lxml\")\n",
    "# res.close()\n",
    "# page=int(soup.select(\"#search > article > hgroup > h2 > b\")[0].text)//15\n",
    "# print(str(page)+\"pages(all)\")\n",
    "\n",
    "# #初始set、頁\n",
    "# WebsiteSet=set()\n",
    "# pagebegin=1\n",
    "\n",
    "# #如果已抓取，由中途開始繼續抓取 (抓取的資料set及頁面)\n",
    "# if os.path.exists('./data/WebsiteSet.json'):\n",
    "#     with open(\"./data/WebsiteSet.json\") as f:\n",
    "#         x=json.load(f)\n",
    "#         WebsiteSet=set(x[dien]+\"-\"+dien for dien in x)\n",
    "#         pagebegin=int(len(WebsiteSet)/15)\n",
    "\n",
    "# if os.path.exists('./data/beginpage.txt'):\n",
    "#     with open(\"./data/beginpage.txt\") as p:\n",
    "#         pagebegin=int(p.read())\n",
    "        \n",
    "# print(\"開始頁%s\"%pagebegin)\n",
    "        \n",
    "# # headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "\n",
    "# #會累積的變數>>i WebsiteSet\n",
    "# for i in range(pagebegin,page+1):\n",
    "#     res=requests.get(URL+\"&p=\"+str(i))\n",
    "#     if res.status_code==200:\n",
    "#         soup=bs(res.text,'lxml')\n",
    "#         dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")\n",
    "#         dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家\n",
    "#         dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"] for dien in dienlist \n",
    "#                   if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，並留下網址\n",
    "#         WebsiteSet.update(dienlist)\n",
    "#         if i%10==0:\n",
    "#             print(\"-----\\n已抓取第%s頁\"%i)\n",
    "#             print(len(WebsiteSet))\n",
    "#             websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in WebsiteSet}\n",
    "#             print(\"實際剩餘%s，店名重複%s\"%(len(websitedict),len(WebsiteSet)-len(websitedict)))\n",
    "#             with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#                 json.dump(websitedict,f)\n",
    "#             with open(\"./data/beginpage.txt\",\"w\") as p:\n",
    "#                 p.write(str(i))\n",
    "#     else:\n",
    "#         print(\"status_code\"+str(res.status_code))\n",
    "#         print(len(WebsiteSet))\n",
    "#         websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in WebsiteSet}\n",
    "#         with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#             json.dump(websitedict,f)\n",
    "#         with open(\"./data/beginpage.txt\",\"w\") as p:\n",
    "#             p.write(str(i))\n",
    "            \n",
    "# websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in WebsiteSet}\n",
    "# with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#     json.dump(websitedict,f)\n",
    "# with open(\"./data/beginpage.txt\",\"w\") as p:\n",
    "#     p.write(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將{店名：網址}格式的json改成{ID:網址}\n",
    "# if os.path.exists('./data/WebsiteSet.json'):\n",
    "#     with open(\"./data/WebsiteSet.json\") as f:\n",
    "#         x=json.load(f)\n",
    "# y={x[dien].split(\"/\")[-1]:x[dien] for dien in x}\n",
    "# with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#     json.dump(y,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二版 -存取資料改為 id:網址 (目前抓完約需5小)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "#確認檔案資料夾是否存在\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "    \n",
    "#抓取網址(愛評網全台餐飲店)\n",
    "URL=\"http://www.ipeen.com.tw/search/taiwan/000/0-0-0-0/%20/?bar=1\"\n",
    "\n",
    "#抓取總頁數\n",
    "res=requests.get(URL+\"&p=1\")\n",
    "soup=bs(res.text,\"lxml\")\n",
    "res.close()\n",
    "page=int(soup.select(\"#search > article > hgroup > h2 > b\")[0].text)//15\n",
    "print(str(page)+\"pages(all)\")\n",
    "\n",
    "#初始網址set(不會重複)、抓取起始頁\n",
    "WebsiteSet=set()\n",
    "pagebegin=1\n",
    "\n",
    "#如果曾抓取，由中途開始繼續抓取 (抓取的資料set讀入)\n",
    "if os.path.exists('../data/WebsiteSet.json'):\n",
    "    with open(\"../data/WebsiteSet.json\") as f:\n",
    "        x=json.load(f)\n",
    "        WebsiteSet=set(x[dien] for dien in x)\n",
    "        \n",
    "#從過去抓取到的頁面開始，繼續抓取\n",
    "if os.path.exists('../data/beginpage.txt'):\n",
    "    with open(\"../data/beginpage.txt\") as p:\n",
    "        pagebegin=int(p.read())\n",
    "        \n",
    "print(\"開始頁%s\"%pagebegin)\n",
    "        \n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "\n",
    "\n",
    "#抓取各店家的id:網址\n",
    "#會累積的變數>>i WebsiteSet\n",
    "for i in range(pagebegin,page+1):\n",
    "    res=requests.get(URL+\"&p=\"+str(i))\n",
    "    if res.status_code==200:\n",
    "        soup=bs(res.text,'lxml')\n",
    "        dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")\n",
    "        \n",
    "#         dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家>改全抓\n",
    "        dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"].split(\"-\")[0] for dien in dienlist \n",
    "                  if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，並留下網址>改不留店名\n",
    "        WebsiteSet.update(dienlist)\n",
    "        \n",
    "        #每抓取30頁的網址，睡5秒以免被ban，並將資料已json格式儲存一次\n",
    "        if i%30==0:\n",
    "            print(\"-----\\n抓取第%s頁\"%i)\n",
    "            print(len(WebsiteSet))\n",
    "            time.sleep(5)\n",
    "\n",
    "            websitedict={Website.split(\"/\")[-1]:Website for Website in WebsiteSet}#改存{id:網址}格式\n",
    "            print(\"實際剩餘%s，id重複%s\"%(len(websitedict),len(WebsiteSet)-len(websitedict)))\n",
    "            with open(\"../data/WebsiteSet.json\",\"w\") as f:\n",
    "                json.dump(websitedict,f)\n",
    "            with open(\"../data/beginpage.txt\",\"w\") as p:\n",
    "                p.write(str(i))\n",
    "    else:\n",
    "        #無法進入頁面則顯示目前網址個數\n",
    "        print(\"status_code\"+str(res.status_code))\n",
    "        print(len(WebsiteSet))\n",
    "#         websitedict={Website.split(\"/\")[-1]:Website for Website in WebsiteSet}#改存{id:網址}格式\n",
    "#         with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#             json.dump(websitedict,f)\n",
    "#         with open(\"./data/beginpage.txt\",\"w\") as p:\n",
    "#             p.write(str(i))\n",
    "\n",
    "\n",
    "#將所有資料抓取完畢後，在儲存最後一次\n",
    "websitedict={Website.split(\"/\")[-1]:Website for Website in WebsiteSet}#改存{id:網址}格式\n",
    "with open(\"../data/WebsiteSet.json\",\"w\") as f:\n",
    "    json.dump(websitedict,f)\n",
    "with open(\"../data/beginpage.txt\",\"w\") as p:\n",
    "    p.write(str(i))\n",
    "print(\"已抓取完畢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "# from pymongo import UpdateOne#0125\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "b=time.time()\n",
    "\n",
    "msg = EmailMessage()\n",
    "msg.set_content(str(time.strftime(\"%Y/%m/%d %H:%M\")))\n",
    "msg['Subject'] = 'The IpeenWebsite Crawler has started to run'\n",
    "msg['From'] = \"ServerNet\"\n",
    "msg['To'] = 'andy.yuan@wowprime.com'\n",
    "with smtplib.SMTP('192.168.2.1',25) as s:\n",
    "    s.send_message(msg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#確認檔案資料夾是否存在\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "    \n",
    "#抓取網址(愛評網全台餐飲店)\n",
    "URL=\"http://www.ipeen.com.tw/search/taiwan/000/0-0-0-0/%20/?bar=1\"\n",
    "\n",
    "\n",
    "#抓取總頁數\n",
    "res=requests.get(URL+\"&p=1\")\n",
    "soup=bs(res.text,\"lxml\")\n",
    "res.close()\n",
    "page=int(soup.select(\"#search > article > hgroup > h2 > b\")[0].text)//15\n",
    "print(str(page)+\"pages(all)\")\n",
    "\n",
    "#初始網址set(不會重複)、抓取起始頁\n",
    "WebsiteSet=set()\n",
    "pagebegin=1\n",
    "\n",
    "\n",
    "#從過去抓取到的頁面開始，繼續抓取\n",
    "if os.path.exists('../data/beginpage.txt'):\n",
    "    with open(\"../data/beginpage.txt\") as p:\n",
    "        pagebegin=int(p.read())\n",
    "\n",
    "print(\"開始頁%s\"%pagebegin)\n",
    "\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "\n",
    "\n",
    "#抓取各店家的id:網址\n",
    "#會累積的變數>>i WebsiteSet\n",
    "for i in range(pagebegin+1,page+1):\n",
    "#         try:\n",
    "#             res=requests.get(URL+\"&p=\"+str(i))\n",
    "#             if \"RedisException\" in res.text:\n",
    "#                 print(\"requests太頻繁被卡，睡兩分鐘\")\n",
    "#                 time.sleep(120)\n",
    "#                 res=requests.get(URL+\"&p=\"+str(i))\n",
    "#         except:\n",
    "#             print(\"requests逾時，睡兩分鐘\")\n",
    "#             time.sleep(120)\n",
    "#             res=requests.get(URL+\"&p=\"+str(i))\n",
    "    res = None\n",
    "    while res is None:\n",
    "        try:\n",
    "            res=requests.get(URL+\"&p=\"+str(i))\n",
    "            while \"RedisException\" in res.text or \"目前系統忙碌中\" in res.text:\n",
    "                print(\"requests太頻繁被卡，睡兩分鐘\")\n",
    "                time.sleep(120)\n",
    "                res=requests.get(URL+\"&p=\"+str(i))\n",
    "        except:\n",
    "            print(\"requests逾時，睡兩分鐘\")\n",
    "            time.sleep(120)\n",
    "\n",
    "\n",
    "    if res.status_code==200:\n",
    "        soup=bs(res.text,'lxml')\n",
    "        try:\n",
    "            dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")\n",
    "        except:\n",
    "            print(\"本頁已死\")\n",
    "            continue\n",
    "\n",
    "#         dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家>改全抓\n",
    "        dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"].split(\"-\")[0] for dien in dienlist \n",
    "                  if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，並留下網址>改不留店名\n",
    "        WebsiteSet.update(dienlist)\n",
    "\n",
    "        #每抓取15頁的網址，睡5秒以免被ban，並將資料已json格式儲存一次\n",
    "        if i%15==0:\n",
    "            print(\"-----\\n已抓取第%s頁\"%i)\n",
    "            print(len(WebsiteSet))\n",
    "\n",
    "            #-----------------------------------------------\n",
    "#                 with open(\"../data/WebsiteSet.json\",\"w\") as f:\n",
    "#                     json.dump(websitedict,f)\n",
    "            #連Mongodb\n",
    "            client = pymongo.MongoClient('localhost', 27017,username='j122085',password='850605')\n",
    "            #連DB名\n",
    "            db=client['rawData']\n",
    "            #秀DB內的collection list\n",
    "            collection = db.ipeenWebsite\n",
    "            try:\n",
    "#                     operations=[UpdateOne({\"_id\":Website.split(\"/\")[-1]},\n",
    "#                                           {'$set':{\"website\":Website}},\n",
    "#                                           upsert=True) \n",
    "#                                 Website in WebsiteSet]\n",
    "#                     collection.bulk_write(operations)\n",
    "                # 如果資料內有相同_id會報錯，但所有非相同_id的資料都會insert\n",
    "                collection.insert_many([{\"_id\":Website.split(\"/\")[-1],\"website\":Website}for Website in WebsiteSet], ordered=False)\n",
    "            except:\n",
    "                print(\"some data has same _id\")\n",
    "            client.close()\n",
    "            WebsiteSet=set()\n",
    "            #-----------------------------------------------    \n",
    "            with open(\"../data/beginpage.txt\",\"w\") as p:\n",
    "                p.write(str(i))\n",
    "            time.sleep(20)  \n",
    "    else:\n",
    "        #無法進入頁面則顯示目前網址個數\n",
    "        print(\"status_code\"+str(res.status_code))\n",
    "        print(len(WebsiteSet))\n",
    "        print(\"request出錯，睡30秒\")\n",
    "        time.sleep(30)\n",
    "#         websitedict={Website.split(\"/\")[-1]:Website for Website in WebsiteSet}#改存{id:網址}格式\n",
    "#         with open(\"./data/WebsiteSet.json\",\"w\") as f:\n",
    "#             json.dump(websitedict,f)\n",
    "#         with open(\"./data/beginpage.txt\",\"w\") as p:\n",
    "#             p.write(str(i))\n",
    "\n",
    "\n",
    "#將所有資料抓取完畢後，在儲存最後一次        \n",
    "#連Mongodb\n",
    "client = pymongo.MongoClient('localhost', 27017,username='j122085',password='850605')\n",
    "#連DB名\n",
    "db=client['rawData']\n",
    "#秀DB內的collection list\n",
    "collection = db.ipeenWebsite\n",
    "try:\n",
    "    collection.insert_many([{\"_id\":Website.split(\"/\")[-1],\"website\":Website}for Website in WebsiteSet], ordered=False)\n",
    "except:\n",
    "    print(\"some data has same _id\")\n",
    "NData=collection.count()\n",
    "client.close()\n",
    "with open(\"../data/beginpage.txt\",\"w\") as p:\n",
    "    p.write(str(0))\n",
    "print(\"已抓取完畢\")\n",
    "#兩週抓一次\n",
    "\n",
    "\n",
    "e=time.time()\n",
    "\n",
    "msg = EmailMessage()\n",
    "msg.set_content(\"\\n\".join([str(time.strftime(\"%Y/%m/%d %H:%M\")),\n",
    "                           \"use {} second\".format(round(e-b)),\n",
    "                           \"we crawled {} datas\".format(NData)]))\n",
    "msg['Subject'] = 'The IpeenWebsit Crawler is finished'\n",
    "msg['From'] = \"ServerNet\"\n",
    "msg['To'] = 'andy.yuan@wowprime.com'\n",
    "with smtplib.SMTP('192.168.2.1',25) as s:\n",
    "    s.send_message(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "#連Mongodb\n",
    "client = pymongo.MongoClient('172.20.26.39', 27017,username='j122085',password='850605')\n",
    "#連DB名\n",
    "db=client['rawData']\n",
    "#秀DB內的collection list\n",
    "print(db.collection_names())\n",
    "collection = db.ipeenWebsite\n",
    "print(collection.count())\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try to concurrent.futures 無法跑到一半存檔、會被BAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlink(page,linkset,):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"}\n",
    "    res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-100-0-0/?baragain=1&p=1&p=\"+str(page),headers=headers)\n",
    "    if res.status_code==200:\n",
    "        soup=bs(res.text,'lxml')\n",
    "        res.close()\n",
    "        dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")#找尋店家資訊\n",
    "        dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家\n",
    "        dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"] for dien in dienlist \n",
    "                  if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，留下網址\n",
    "        linkset.update(dienlist)#將該頁的店list加入set中(順便去除重複)\n",
    "#         if i%10==0:#每十頁存一次\n",
    "#             print(len(linkset))\n",
    "#             websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "#             with open(\"./data/linkset.json\",\"w\") as f:\n",
    "#                 json.dump(websitedict,f)\n",
    "    else:#沒連線成功也存\n",
    "        print(\"status_code=\"+str(res.status_code))\n",
    "        res.close()\n",
    "        print(len(linkset))\n",
    "#         websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "#         with open(\"./data/linkset.json\",\"w\") as f:\n",
    "#             json.dump(websitedict,f)\n",
    "#     websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "    return dienlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-100-0-0/?baragain=1&p=1&p=1\")\n",
    "soup=bs(res.text,\"lxml\")\n",
    "res.close()\n",
    "page=int(soup.select(\"#search > article > hgroup > h2 > b\")[0].text)//15\n",
    "print(str(page)+\"pages\")\n",
    "\n",
    "WebsiteSet=set()\n",
    "start = time.time()\n",
    "i=0\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    jobInfo = [executor.submit(getlink, page, WebsiteSet) for page in range(1,page+1)]\n",
    "    for future in concurrent.futures.as_completed(jobInfo):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            WebsiteSet.update(data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"zzz\")\n",
    "            pass\n",
    "        \n",
    "end = time.time()  \n",
    "print(\"-----------------\")\n",
    "print(end-start)\n",
    "\n",
    "websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in WebsiteSet}\n",
    "with open(\"../data/linkset.json\",\"w\") as f:\n",
    "    json.dump(websitedict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/linkSet.json\") as f:\n",
    "    x=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to change proxy 失敗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getlink(page,linkset,proxylist):\n",
    "    res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/?baragain=1&p=\"+str(page))\n",
    "    if res.status_code==200:\n",
    "        soup=bs(res.text,'lxml')\n",
    "        res.close()\n",
    "        dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")#找尋店家資訊\n",
    "        dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家\n",
    "        dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"] for dien in dienlist \n",
    "                  if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，留下網址\n",
    "#         linkset.update(dienlist)#將該頁的店list加入set中(順便去除重複)\n",
    "#         if i%10==0:#每十頁存一次\n",
    "#             print(len(linkset))\n",
    "#             websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "#             with open(\"./data/linkset.json\",\"w\") as f:\n",
    "#                 json.dump(websitedict,f)\n",
    "    else:#沒連線成功也存\n",
    "        res.close()\n",
    "        n=0\n",
    "        res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/?baragain=1&p=\"+str(page),proxies=proxylist[n])\n",
    "        while res.status_code!=200:\n",
    "            n+=1\n",
    "            res.close()\n",
    "            res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/?baragain=1&p=\"+str(page),proxies=proxylist[n])\n",
    "        dienlist=soup.select(\"article.serMain > div.result > section > article.serItem > div > h3\")#找尋店家資訊\n",
    "        dienlist=[dien for dien in dienlist  if not dien.select_one(\"span.status\")]#去除搬遷或關店狀態的店家\n",
    "        dienlist=[\"http://www.ipeen.com.tw\"+dien.select_one(\"a\")[\"href\"] for dien in dienlist \n",
    "                  if dien.select_one(\"a\")[\"data-action\"]!=\"ad_shop\"]#去除廣告店家，留下網址\n",
    "#         websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "#         with open(\"./data/linkset.json\",\"w\") as f:\n",
    "#             json.dump(websitedict,f)\n",
    "#     websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in linkset}\n",
    "    return dienlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url=\"https://free-proxy-list.net/\"\n",
    "res=requests.get(url)\n",
    "soup=bs(res.text,\"lxml\")\n",
    "proxylist=re.findall(\"([0-2]?[0-9]?[0-9]\\.[0-2]?[0-9]?[0-9]\\.[0-2]?[0-9]?[0-9]\\.[0-2]?[0-9]?[0-9])(80)\",soup.text)\n",
    "proxylist=[{'http':'http://'+proxy[0]+':'+proxy[1]} for proxy in proxylist]\n",
    "proxylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=requests.get(\"http://www.ipeen.com.tw/search/taiwan/000/1-0-0-0/?baragain=1&p=1\")\n",
    "soup=bs(res.text,\"lxml\")\n",
    "res.close()\n",
    "page=int(soup.select(\"#search > article > hgroup > h2 > b\")[0].text)//15\n",
    "print(str(page)+\"pages\")\n",
    "\n",
    "WebsiteSet=set()\n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    jobInfo = [executor.submit(getlink, page, WebsiteSet,proxylist) for page in range(1,page+1)]\n",
    "    for future in concurrent.futures.as_completed(jobInfo):\n",
    "        try:\n",
    "            data = future.result()\n",
    "            WebsiteSet.update(data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"zzz\")\n",
    "            pass\n",
    "        \n",
    "end = time.time()  \n",
    "print(\"-----------------\")\n",
    "print(end-start)\n",
    "\n",
    "websitedict={Website.split(\"-\",1)[1]:Website.split(\"-\",1)[0] for Website in WebsiteSet}\n",
    "with open(\"../data/linkset.json\",\"w\") as f:\n",
    "    json.dump(websitedict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/linkSet.json\") as f:\n",
    "    x=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 變更proxy的兩種方法\n",
    "import requests\n",
    "s = requests.Session()\n",
    "s.proxies = {\"http\": \"http://61.233.25.166:80\"}\n",
    "r = s.get(\"http://www.google.com\")\n",
    "print(r.text)\n",
    "\n",
    "import requests\n",
    "r = requests.get(\"http://www.google.com\", proxies={\"http\": \"http://61.233.25.166:80\"})\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 變更user agent的方法\n",
    "headers = {\n",
    "    'User-Agent': 'My User Agent 1.0',\n",
    "    'From': 'youremail@domain.com'  # This is another valid field\n",
    "}\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bs抓取某tag or tag群的方法\n",
    "soup.select(\"tag > tag > tag\")\n",
    "如:soup.select(\"#search > article > hgroup > h2 > b\")\n",
    "    \n",
    "soup.find_all('tag', attrs={'tag內的元素': '元素名稱'}) \n",
    "如:soup.find_all('a', attrs={'data-label': '店名'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
